{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Natural Language Processing (NLP) and Recurrent Neural Networks in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embeddings, or word vectors, provide a way of mapping words from a vocabulary into a low-dimensional space, where words with similar meanings are close together. Let's play around with a set of pre-trained word vectors, to get used to their properties. There exist many sets of pretrained word embeddings; here, we use ConceptNet Numberbatch, which provides a relatively small download in an easy-to-work-with format (h5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download word vectors\n",
    "from urllib.request import urlretrieve\n",
    "import os\n",
    "if not os.path.isfile('mini.h5'):\n",
    "    print(\"Downloading Conceptnet Numberbatch word embeddings...\")\n",
    "    conceptnet_url = 'http://conceptnet.s3.amazonaws.com/precomputed-data/2016/numberbatch/17.06/mini.h5'\n",
    "    urlretrieve(conceptnet_url, 'mini.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read an `h5` file, we'll need to use the `h5py` package. Below, we use the package to open the `mini.h5` file we just downloaded. We extract from the file a list of utf-8-encoded words, as well as their $300$-dimensional vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "with h5py.File('mini.h5', 'r') as f:\n",
    "    all_words = [word.decode('utf-8') for word in f['mat']['axis1'][:]]\n",
    "    all_embeddings = f['mat']['block0_values'][:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, `all_words` is a list of $V$ strings (what we call our *vocabulary*), and `all_embeddings` is a $V \\times 300$ matrix. The strings are of the form `/c/language_code/word`â€”for example, `/c/en/cat` and `/c/es/gato`.\n",
    "\n",
    "We are interested only in the English words. We use Python list comprehensions to pull out the indices of the English words, then extract just the English words (stripping the six-character `/c/en/` prefix) and their embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_words = [word[6:] for word in all_words if word.startswith('/c/en/')]\n",
    "english_word_indices = [i for i, word in enumerate(all_words) if word.startswith('/c/en/')]\n",
    "english_embedddings = all_embeddings[english_word_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The magnitude of a word vector is less important than its direction; the magnitude can be thought of as representing frequency of use, independent of the semantics of the word. \n",
    "Here, we will be interested in semantics, so we *normalize* our vectors, dividing each by its length. \n",
    "The result is that all of our word vectors are length 1, and as such, lie on a unit circle. \n",
    "The dot product of two vectors is proportional to the cosine of the angle between them, and provides a measure of similarity (the bigger the cosine, the smaller the angle).\n",
    "\n",
    "<img src=\"Figures/cosine_similarity.png\" alt=\"cosine\" style=\"width: 500px;\"/>\n",
    "<center>Figure adapted from *[Mastering Machine Learning with Spark 2.x](https://www.safaribooksonline.com/library/view/mastering-machine-learning/9781785283451/ba8bef27-953e-42a4-8180-cea152af8118.xhtml)*</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "norms = np.linalg.norm(english_embedddings, axis=1)\n",
    "normalized_embeddings = english_embedddings.astype('float32') / norms.astype('float32').reshape([-1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to look up words easily, so we create a dictionary that maps us from a word to its index in the word embeddings matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = {word: i for i, word in enumerate(english_words)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to measure the similarity between pairs of words. We use numpy to take dot products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_score(w1, w2):\n",
    "    score = np.dot(normalized_embeddings[index[w1], :], normalized_embeddings[index[w2], :])\n",
    "    return score\n",
    "\n",
    "def print_similarity(w1,w2):\n",
    "    try:\n",
    "        print('{0}\\t{1}\\t'.format(w1,w2), \\\n",
    "          similarity_score('{}'.format(w1), '{}'.format(w2)))\n",
    "    except:\n",
    "        print('One of the words is not in the dictionary.')\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\tcat\t 1.0\n",
      "cat\tfeline\t 0.8199548\n",
      "cat\tdog\t 0.590724\n",
      "cat\tmoo\t 0.0039538248\n",
      "cat\tfreeze\t -0.030225184\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# A word is as similar with itself as possible:\n",
    "print('cat\\tcat\\t', similarity_score('cat', 'cat'))\n",
    "# Closely related words still get high scores:\n",
    "print('cat\\tfeline\\t', similarity_score('cat', 'feline'))\n",
    "print('cat\\tdog\\t', similarity_score('cat', 'dog'))\n",
    "# Unrelated words, not so much\n",
    "print('cat\\tmoo\\t', similarity_score('cat', 'moo'))\n",
    "print('cat\\tfreeze\\t', similarity_score('cat', 'freeze'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "antonyms\topposites\t 0.3941065\n",
      "antonyms\tsynonyms\t 0.46883982\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Antonyms are still considered related, sometimes more so than synonyms\n",
    "print('antonyms\\topposites\\t', similarity_score('antonym', 'opposite'))\n",
    "print('antonyms\\tsynonyms\\t', similarity_score('antonym', 'synonym'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iguana\tchameleon\t 0.34494567\n",
      "iguana\tdog\t 0.20372051\n",
      "iguana\treptile\t 0.46178538\n",
      "encyclopedia\twikipedia\t 0.506083\n",
      "encyclopedia\tbook\t 0.28528008\n",
      "fake\tnews\t 0.043742567\n",
      "fake\treal\t 0.40731964\n"
     ]
    }
   ],
   "source": [
    "# whatever we want:\n",
    "print_similarity('iguana','chameleon')\n",
    "print_similarity('iguana','dog')\n",
    "print_similarity('iguana','reptile')\n",
    "print_similarity('encyclopedia','wikipedia')\n",
    "print_similarity('encyclopedia','book')\n",
    "print_similarity('fake','news')\n",
    "print_similarity('fake','real')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also find, for instance, the most similar words to a given word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closest_to_vector(v, n):\n",
    "    all_scores = np.dot(normalized_embeddings, v)\n",
    "    best_words = map(lambda i: english_words[i], reversed(np.argsort(all_scores)))\n",
    "    return [next(best_words) for _ in range(n)]\n",
    "\n",
    "def most_similar(w, n):\n",
    "    \"\"\"\n",
    "    Find the `n` most similar words to `w`.\n",
    "    \"\"\"\n",
    "    return closest_to_vector(normalized_embeddings[index[w], :], n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cat', 'humane_society', 'kitten', 'feline', 'colocolo', 'cats', 'kitty', 'maine_coon', 'housecat', 'sharp_teeth']\n",
      "['dog', 'dogs', 'wire_haired_dachshund', 'doggy_paddle', 'lhasa_apso', 'good_friend', 'puppy_dog', 'bichon_frise', 'woof_woof', 'golden_retrievers']\n",
      "['duke', 'dukes', 'duchess', 'duchesses', 'ducal', 'dukedom', 'duchy', 'voivode', 'princes', 'prince']\n",
      "['wikipedia', 'wikipedias', 'wikimedia', 'wikisource', 'wiki', 'wikiquote', 'mediawiki', 'wikipedians', 'wikipedian', 'wikimedia_commons']\n",
      "['deep', 'deeps', 'deepest', 'deeper', 'profound', 'unfathomed', 'depths', 'profoundest', 'depth', 'deepness']\n"
     ]
    }
   ],
   "source": [
    "print(most_similar('cat', 10))\n",
    "print(most_similar('dog', 10))\n",
    "print(most_similar('duke', 10))\n",
    "print(most_similar('wikipedia', 10))\n",
    "print(most_similar('deep', 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use `closest_to_vector` to find words \"nearby\" vectors that we create ourselves. This allows us to solve analogies. For example, in order to solve the analogy \"man : brother :: woman : ?\", we can compute a new vector `brother - man + woman`: the meaning of brother, minus the meaning of man, plus the meaning of woman. We can then ask which words are closest, in the embedding space, to that new vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_analogy(a1, b1, a2):\n",
    "    b2 = normalized_embeddings[index[b1], :] - normalized_embeddings[index[a1], :] + normalized_embeddings[index[a2], :]\n",
    "    return closest_to_vector(b2, 5)\n",
    "def print_analogy(a1, b1,a2):\n",
    "    closest_words=solve_analogy(a1,b1,a2)\n",
    "    print(\"{0}:{1} as {2}:?\".format(a1,b1,a2))\n",
    "    print(\"Best guesses are: {}\".format(closest_words))\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man:brother as woman:?\n",
      "Best guesses are: ['sister', 'brother', 'sisters', 'kid_sister', 'younger_brother']\n",
      "man:husband as woman:?\n",
      "Best guesses are: ['wife', 'husband', 'husbands', 'spouse', 'wifes']\n",
      "spain:madrid as france:?\n",
      "Best guesses are: ['paris', 'france', 'le_havre', 'in_france', 'montmartre']\n"
     ]
    }
   ],
   "source": [
    "print_analogy(\"man\", \"brother\", \"woman\")\n",
    "print_analogy(\"man\", \"husband\", \"woman\")\n",
    "print_analogy(\"spain\", \"madrid\", \"france\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog:golden_retriever as cat:?\n",
      "Best guesses are: ['cat', 'maine_coon', 'kitten', 'tabby', 'kitty']\n"
     ]
    }
   ],
   "source": [
    "print_analogy(\"dog\", \"golden_retriever\", \"cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog:bark as cat:?\n",
      "Best guesses are: ['bark', 'cat', 'sharp_teeth', 'barks', 'meow']\n"
     ]
    }
   ],
   "source": [
    "print_analogy(\"dog\", \"bark\", \"cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bark:meow as dog:?\n",
      "Best guesses are: ['meow', 'meows', 'cat', 'meowing', 'kitty']\n"
     ]
    }
   ],
   "source": [
    "print_analogy(\"bark\", \"meow\", \"dog\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These three results are quite good, but in general, the results of these analogies can be disappointing. Try experimenting with other analogies, and see if you can think of ways to get around the problems you notice (i.e., modifications to the solve_analogy algorithm)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using word embeddings in deep models\n",
    "Word embeddings are fun to play around with, but their primary use is that they allow us to think of words as existing in a continuous, Euclidean space; we can then use an existing arsenal of techniques for machine learning with continuous numerical data (like logistic regression or neural networks) to process text.\n",
    "\n",
    "Let's take a look at an especially simple version of this. We'll perform *sentiment analysis* on a set of movie reviews: in particular, we will attempt to classify a movie review as positive or negative based on its text.\n",
    "\n",
    "We will use a [Simple Word Embedding Model](http://people.ee.duke.edu/~lcarin/acl2018_swem.pdf) (SWEM, Shen et al. 2018) to do so. We will represent a review as the *mean* of the embeddings of the words in the review. Then we'll train a three-layer MLP (a neural network) to classify the review as positive or negative.\n",
    "\n",
    "###### A word of caution: these movie reviews are unfiltered real reviews online and contain inappropriate language.  For the purposes of this class, you don't have to read them, and can consider them just a matrix of numbers.\n",
    "\n",
    "Download the `movie-simple.txt` file from the repository into this directory. Each line of that file contains \n",
    "\n",
    "1. the numeral 0 (for negative) or the numeral 1 (for positive), followed by\n",
    "2. a tab (the whitespace character), and then\n",
    "3. the review itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "remove_punct=str.maketrans('','',string.punctuation)\n",
    "\n",
    "# This function converts a line of our data file into\n",
    "# a tuple (x, y), where x is 300-dimensional representation\n",
    "# of the words in a review, and y is its label.\n",
    "def convert_line_to_example(line):\n",
    "    # Pull out the first character: that's our label (0 or 1)\n",
    "    y = int(line[0])\n",
    "    # Split the line into words using Python's split() function\n",
    "    words = line[2:].translate(remove_punct).lower().split()\n",
    "    # Look up the embeddings of each word, ignoring words not\n",
    "    # in our pretrained vocabulary.\n",
    "    embeddings = [normalized_embeddings[index[w]] for w in words\n",
    "                  if w in index]\n",
    "    # Take the mean of the embeddings\n",
    "    x = np.mean(np.vstack(embeddings), axis=0)\n",
    "    return {'x': x, 'y': y, 'w':embeddings}\n",
    "\n",
    "# Apply the function to each line in the file.\n",
    "enc = 'utf-8' # This is necessary from within the singularity shell\n",
    "with open(\"Data/movie-simple.txt\", \"r\", encoding=enc) as f:\n",
    "    dataset = [convert_line_to_example(l) for l in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1411"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a dataset, let's shuffle it and do a train/test split. We use a quarter of the dataset for testing, 3/4 for training (but also ensure that we have a whole number of batches in our training set, to make the code nicer later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(dataset)\n",
    "\n",
    "batch_size = 100\n",
    "total_batches = len(dataset) // batch_size\n",
    "train_batches = 3 * total_batches // 4\n",
    "train, test = dataset[:train_batches*batch_size], dataset[train_batches*batch_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to build our MLP in Tensorflow. We'll use placeholders for `X` and `y` as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Placeholders for input\n",
    "X = tf.placeholder(tf.float32, [None, 300])\n",
    "y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "# Three-layer MLP\n",
    "h1 = tf.layers.dense(X, 100, tf.nn.relu)\n",
    "h2 = tf.layers.dense(h1, 20, tf.nn.relu)\n",
    "logits = tf.layers.dense(h2, 1)\n",
    "probabilities = tf.sigmoid(logits)\n",
    "\n",
    "# Loss and metrics\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.round(tf.sigmoid(logits)), y), tf.float32))\n",
    "\n",
    "# Training\n",
    "train_step = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "\n",
    "# Initialization of variables\n",
    "initialize_all = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now begin a session and train our model. We'll train for 250 epochs. When we're finished, we'll evaluate our accuracy on all the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss 0.69135916 Acc 0.61\n",
      "Epoch 10 Loss 0.66213167 Acc 0.65\n",
      "Epoch 20 Loss 0.65845925 Acc 0.68\n",
      "Epoch 30 Loss 0.66219825 Acc 0.55\n",
      "Epoch 40 Loss 0.6425139 Acc 0.64\n",
      "Epoch 50 Loss 0.59427387 Acc 0.8\n",
      "Epoch 60 Loss 0.5231353 Acc 0.81\n",
      "Epoch 70 Loss 0.4517753 Acc 0.89\n",
      "Epoch 80 Loss 0.4067031 Acc 0.86\n",
      "Epoch 90 Loss 0.3422463 Acc 0.89\n",
      "Epoch 100 Loss 0.26190525 Acc 0.98\n",
      "Epoch 110 Loss 0.27958417 Acc 0.9\n",
      "Epoch 120 Loss 0.2106394 Acc 0.92\n",
      "Epoch 130 Loss 0.2044314 Acc 0.92\n",
      "Epoch 140 Loss 0.17735112 Acc 0.96\n",
      "Epoch 150 Loss 0.26515996 Acc 0.85\n",
      "Epoch 160 Loss 0.1422936 Acc 0.97\n",
      "Epoch 170 Loss 0.10893148 Acc 0.97\n",
      "Epoch 180 Loss 0.08726247 Acc 0.99\n",
      "Epoch 190 Loss 0.12732412 Acc 0.96\n",
      "Epoch 200 Loss 0.0938323 Acc 0.99\n",
      "Epoch 210 Loss 0.11325945 Acc 0.97\n",
      "Epoch 220 Loss 0.14819159 Acc 0.95\n",
      "Epoch 230 Loss 0.08506051 Acc 0.95\n",
      "Epoch 240 Loss 0.12863559 Acc 0.95\n",
      "Final accuracy: 0.9440389\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(initialize_all)\n",
    "for epoch in range(250):\n",
    "    for batch in range(train_batches):\n",
    "        data = train[batch*batch_size:(batch+1)*batch_size]\n",
    "        reviews = [sample['x'] for sample in data]\n",
    "        labels  = [sample['y'] for sample in data]\n",
    "        labels = np.array(labels).reshape([-1, 1])\n",
    "        _, l, acc = sess.run([train_step, loss, accuracy], feed_dict={X: reviews, y: labels})\n",
    "    if epoch % 10 == 0:\n",
    "        print(\"Epoch\", epoch, \"Loss\", l, \"Acc\", acc)\n",
    "    random.shuffle(train)\n",
    "\n",
    "# Evaluate on test set\n",
    "test_reviews = [sample['x'] for sample in test]\n",
    "test_labels  = [sample['y'] for sample in test]\n",
    "test_labels = np.array(test_labels).reshape([-1, 1])\n",
    "acc = sess.run(accuracy, feed_dict={X: test_reviews, y: test_labels})\n",
    "print(\"Final accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now examine what our model has learned, seeing how it responds to word vectors for different words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exciting [[0.9999826]]\n",
      "hated [[3.0430762e-08]]\n",
      "boring [[5.766049e-07]]\n",
      "loved [[0.99999976]]\n"
     ]
    }
   ],
   "source": [
    "# Check some words\n",
    "words_to_test = ['exciting', 'hated', 'boring', 'loved']\n",
    "for word in words_to_test:\n",
    "    print(word, sess.run(probabilities, feed_dict={X: normalized_embeddings[index[word]].reshape(1, 300)}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turns out the internet hates \"Brokeback Mountain\" and loves \"Harry Potter\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brokeback [[0.97749025]]\n",
      "mountain [[0.00071368]]\n",
      "potter [[0.9919137]]\n"
     ]
    }
   ],
   "source": [
    "# Check some words\n",
    "words_to_test = ['brokeback','mountain','potter']\n",
    "for word in words_to_test:\n",
    "    print(word, sess.run(probabilities, feed_dict={X: normalized_embeddings[index[word]].reshape(1, 300)}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try some words of your own!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model works great for such a simple dataset, but does a little less well on something more complex. `movie-pang02.txt`, for instance, has 2000 longer, more complex movie reviews. It's in the same format as our simple dataset. On those longer reviews, this model achieves only 60-80% accuracy. (Increasing the number of epochs to, say, 1000, does help.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss 0.6930969 Acc 0.52\n",
      "Epoch 10 Loss 0.6924441 Acc 0.5\n",
      "Epoch 20 Loss 0.6919834 Acc 0.59\n",
      "Epoch 30 Loss 0.692334 Acc 0.52\n",
      "Epoch 40 Loss 0.69111776 Acc 0.56\n",
      "Epoch 50 Loss 0.6947514 Acc 0.4\n",
      "Epoch 60 Loss 0.690073 Acc 0.72\n",
      "Epoch 70 Loss 0.69025403 Acc 0.63\n",
      "Epoch 80 Loss 0.6894762 Acc 0.62\n",
      "Epoch 90 Loss 0.68978554 Acc 0.58\n",
      "Epoch 100 Loss 0.69094795 Acc 0.5\n",
      "Epoch 110 Loss 0.6878017 Acc 0.69\n",
      "Epoch 120 Loss 0.6897257 Acc 0.61\n",
      "Epoch 130 Loss 0.68736345 Acc 0.7\n",
      "Epoch 140 Loss 0.6898315 Acc 0.45\n",
      "Epoch 150 Loss 0.68583083 Acc 0.75\n",
      "Epoch 160 Loss 0.6862535 Acc 0.65\n",
      "Epoch 170 Loss 0.68725604 Acc 0.52\n",
      "Epoch 180 Loss 0.6836673 Acc 0.56\n",
      "Epoch 190 Loss 0.6855555 Acc 0.52\n",
      "Epoch 200 Loss 0.6824671 Acc 0.56\n",
      "Epoch 210 Loss 0.67649573 Acc 0.72\n",
      "Epoch 220 Loss 0.6792133 Acc 0.65\n",
      "Epoch 230 Loss 0.6842475 Acc 0.48\n",
      "Epoch 240 Loss 0.6696932 Acc 0.66\n",
      "Final accuracy: 0.662\n"
     ]
    }
   ],
   "source": [
    "# Apply the function to each line in the file.\n",
    "with open(\"Data/movie-pang02.txt\", \"r\",encoding=enc) as f:\n",
    "    dataset = [convert_line_to_example(l) for l in f.readlines()]\n",
    "import random\n",
    "random.shuffle(dataset)\n",
    "batch_size = 100\n",
    "total_batches = len(dataset) // batch_size\n",
    "train_batches = 3 * total_batches // 4\n",
    "train, test = dataset[:train_batches*batch_size], dataset[train_batches*batch_size:]\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(initialize_all)\n",
    "for epoch in range(250):\n",
    "    for batch in range(train_batches):\n",
    "        data = train[batch*batch_size:(batch+1)*batch_size]\n",
    "        reviews = [sample['x'] for sample in data]\n",
    "        labels  = [sample['y'] for sample in data]\n",
    "        labels = np.array(labels).reshape([-1, 1])\n",
    "        _, l, acc = sess.run([train_step, loss, accuracy], feed_dict={X: reviews, y: labels})\n",
    "    if epoch % 10 == 0:\n",
    "        print(\"Epoch\", epoch, \"Loss\", l, \"Acc\", acc)\n",
    "    random.shuffle(train)\n",
    "\n",
    "# Evaluate on test set\n",
    "test_reviews = [sample['x'] for sample in test]\n",
    "test_labels  = [sample['y'] for sample in test]\n",
    "test_labels = np.array(test_labels).reshape([-1, 1])\n",
    "acc = sess.run(accuracy, feed_dict={X: test_reviews, y: test_labels})\n",
    "print(\"Final accuracy:\", acc)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent Neural Networks (RNNs)\n",
    "\n",
    "In the context of deep learning, natural language is commonly modeled with Recurrent Neural Networks (RNNs).\n",
    "RNNs pass the output of a neuron back to the input of the next time step of the same neuron.\n",
    "These directed cycles in the RNN architecture gives them the ability to model temporal dynamics, making them particularly suited for modeling sequences (e.g. text).\n",
    "We can visualize an RNN layer as follows:\n",
    "\n",
    "<img src=\"Figures/basic_RNN.PNG\" alt=\"basic_RNN\" style=\"width: 80px;\"/>\n",
    "<center>Figure from *Understanding LSTMs*. https://colah.github.io/posts/2015-08-Understanding-LSTMs/</center>\n",
    "\n",
    "We can unroll an RNN through time, making the sequence aspect of them more obvious:\n",
    "\n",
    "<img src=\"Figures/unrolled_RNN.PNG\" alt=\"basic_RNN\" style=\"width: 400px;\"/>\n",
    "<center>Figure from *Understanding LSTMs*. https://colah.github.io/posts/2015-08-Understanding-LSTMs/</center>\n",
    "\n",
    "#### RNNs in TensorFlow\n",
    "How would we implement an RNN in TensorFlow? Given the different forms of RNNs, there are quite a few ways, but we'll stick to a simple one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As always, import TensorFlow first\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume we have our inputs in word embedding form already, say of dimensionality 100. We'll use a minibatch size of 16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb = 16\n",
    "x_dim = 100\n",
    "\n",
    "x1 = tf.placeholder(tf.float32, [mb, x_dim])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define weight matrices for projecting the input, the previous state, and the output. Rather arbitrarily, let's pick a hidden layer size of 64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_dim = 64\n",
    "\n",
    "# For projecting the input\n",
    "U = tf.Variable(tf.truncated_normal([x_dim, h_dim], stddev=0.1))\n",
    "\n",
    "# For projecting the previous state\n",
    "W = tf.Variable(tf.truncated_normal([h_dim, h_dim], stddev=0.1))\n",
    "\n",
    "# For projecting the output\n",
    "V = tf.Variable(tf.truncated_normal([h_dim, x_dim], stddev=0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, a function for one time step of the RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN_step(x, h):\n",
    "    h_next = tf.nn.tanh(tf.matmul(x, U) + tf.matmul(h, W))\n",
    "    output = tf.matmul(h_next, V)\n",
    "    return output, h_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output y1 dimensions: (16, 100)\n",
      "Hidden state h1 dimensions: (16, 64)\n"
     ]
    }
   ],
   "source": [
    "# Initialize hidden state to 0\n",
    "h0 = tf.zeros([mb, h_dim])\n",
    "\n",
    "# Forward pass of one RNN step for time step t=1\n",
    "y1, h1 = RNN_step(x1, h0)\n",
    "\n",
    "print(\"Output y1 dimensions: {0}\".format(y1.shape))\n",
    "print(\"Hidden state h1 dimensions: {0}\".format(h1.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can repeat using the `RNN_step` function to continue unrolling the RNN as far as we need to. For each step, we feed in the next input (a new placeholder) and get a new output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output y2 dimensions: (16, 100)\n",
      "Hidden state h2 dimensions: (16, 64)\n"
     ]
    }
   ],
   "source": [
    "x2 = tf.placeholder(tf.float32, [mb, x_dim])\n",
    "\n",
    "# Forward pass of one RNN step for time step t=2\n",
    "y2, h2 = RNN_step(x2, h1)\n",
    "\n",
    "print(\"Output y2 dimensions: {0}\".format(y2.shape))\n",
    "print(\"Hidden state h2 dimensions: {0}\".format(h2.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, in practice, you'd want to do this unrolling with a `for` loop, and the RNN functionality is more cleanly wrapped up in a class. \n",
    "We're not going to implement the class version here though, as TensorFlow already has these implemented: https://www.tensorflow.org/api_guides/python/contrib.rnn#Base_interface_for_all_RNN_Cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x dimensions:\n",
      "[TensorShape([Dimension(16), Dimension(100)]), TensorShape([Dimension(16), Dimension(100)]), TensorShape([Dimension(16), Dimension(100)]), TensorShape([Dimension(16), Dimension(100)]), TensorShape([Dimension(16), Dimension(100)]), TensorShape([Dimension(16), Dimension(100)]), TensorShape([Dimension(16), Dimension(100)]), TensorShape([Dimension(16), Dimension(100)]), TensorShape([Dimension(16), Dimension(100)]), TensorShape([Dimension(16), Dimension(100)])]\n",
      "\n",
      "h dimensions:\n",
      "[TensorShape([Dimension(16), Dimension(64)]), TensorShape([Dimension(16), Dimension(64)]), TensorShape([Dimension(16), Dimension(64)]), TensorShape([Dimension(16), Dimension(64)]), TensorShape([Dimension(16), Dimension(64)]), TensorShape([Dimension(16), Dimension(64)]), TensorShape([Dimension(16), Dimension(64)]), TensorShape([Dimension(16), Dimension(64)]), TensorShape([Dimension(16), Dimension(64)]), TensorShape([Dimension(16), Dimension(64)])]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "# Number of steps to unroll\n",
    "num_steps = 10\n",
    "\n",
    "# List of inputs and hidden states\n",
    "xs = []\n",
    "hs = []\n",
    "\n",
    "# Build RNN\n",
    "rnn = tf.contrib.rnn.BasicRNNCell(h_dim)\n",
    "\n",
    "# Initialize hidden state to zero\n",
    "h_t = tf.zeros([mb, h_dim])\n",
    "\n",
    "for t in range(num_steps):\n",
    "    x_t = tf.placeholder(tf.float32, [mb, x_dim])\n",
    "    h_t, _ = rnn(x_t, h_t)\n",
    "    \n",
    "    xs.append(x_t)\n",
    "    hs.append(h_t)\n",
    "    \n",
    "print(\"x dimensions:\")\n",
    "print([x_t.shape for x_t in xs])\n",
    "print(\"\\nh dimensions:\")\n",
    "print([h_t.shape for h_t in hs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using a static length, we can use a variable length output.  This can be done with tensorflow's built-in dynamic RNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by applying a RNN to MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "n_steps = 28\n",
    "n_inputs = 28\n",
    "n_neurons = 150\n",
    "n_outputs = 10\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "y = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\n",
    "outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32)\n",
    "\n",
    "logits = tf.layers.dense(states, n_outputs)\n",
    "cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
    "                                                          logits=logits)\n",
    "loss = tf.reduce_mean(cross_entropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-34-98a86d91dd61>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /opt/anaconda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /opt/anaconda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./tmp/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /opt/anaconda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./tmp/train-labels-idx1-ubyte.gz\n",
      "Extracting ./tmp/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./tmp/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /opt/anaconda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('./tmp')\n",
    "X_test = mnist.test.images.reshape((-1, n_steps, n_inputs))\n",
    "y_test = mnist.test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train accuracy: 0.9266667 Test accuracy: 0.9111\n",
      "1 Train accuracy: 0.94 Test accuracy: 0.9338\n",
      "2 Train accuracy: 0.9866667 Test accuracy: 0.9524\n",
      "3 Train accuracy: 0.96 Test accuracy: 0.9598\n",
      "4 Train accuracy: 0.96666664 Test accuracy: 0.9668\n",
      "5 Train accuracy: 0.9266667 Test accuracy: 0.9443\n",
      "6 Train accuracy: 0.97333336 Test accuracy: 0.9646\n",
      "7 Train accuracy: 0.97333336 Test accuracy: 0.9641\n",
      "8 Train accuracy: 0.99333334 Test accuracy: 0.9737\n",
      "9 Train accuracy: 0.98 Test accuracy: 0.966\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "batch_size = 150\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            X_batch = X_batch.reshape((-1, n_steps, n_inputs))\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={X: X_test, y: y_test})\n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying an RNN to the text reviews, starting with the easier data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "# sizes\n",
    "n_steps = None\n",
    "n_inputs = 300\n",
    "n_neurons = 50\n",
    "# Build RNN\n",
    "X= tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "y= tf.placeholder(tf.float32, [None, 1])\n",
    "basic_cell = tf.contrib.rnn.BasicRNNCell(n_neurons,activation=tf.nn.tanh)\n",
    "outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32)\n",
    "last_cell_output=outputs[:,-1,:]\n",
    "y_=tf.layers.dense(last_cell_output,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and metrics\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=y_, labels=y))\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.round(tf.sigmoid(y_)), y), tf.float32))\n",
    "\n",
    "# Training\n",
    "train_step = tf.train.AdamOptimizer(0.001).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's train this on the word embeddings for sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Data/movie-simple.txt\", \"r\",encoding=enc) as f:\n",
    "    dataset = [convert_line_to_example(l) for l in f.readlines()]\n",
    "import random\n",
    "random.shuffle(dataset)\n",
    "batch_size = 1\n",
    "total_batches = len(dataset) // batch_size\n",
    "train_batches = 3 * total_batches // 4\n",
    "train, test = dataset[:train_batches*batch_size], dataset[train_batches*batch_size:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 99 Loss 0.6807326930569666 Acc 0.5737172786808623\n",
      "batch 199 Loss 0.622806626555265 Acc 0.6618326846100836\n",
      "batch 299 Loss 0.5093100747250915 Acc 0.7340922617888246\n",
      "batch 399 Loss 0.43880586051760734 Acc 0.8155645786314655\n",
      "batch 499 Loss 0.37961545128341956 Acc 0.8569490547297536\n",
      "batch 599 Loss 0.4343319061892812 Acc 0.7955144696782365\n",
      "batch 699 Loss 0.36547634622865016 Acc 0.8326777645832846\n",
      "batch 799 Loss 0.3135014364280898 Acc 0.8613334747846224\n",
      "batch 899 Loss 0.33197937922695 Acc 0.8728015387520769\n",
      "batch 999 Loss 0.2827433999012625 Acc 0.8898436005865152\n",
      "Epoch 0 Loss 0.3120099963069911 Acc 0.8709885429134029\n",
      "batch 99 Loss 0.2657768022122512 Acc 0.8946576402311365\n",
      "batch 199 Loss 0.2366160425735716 Acc 0.9037072929649635\n",
      "batch 299 Loss 0.21485114587533588 Acc 0.9077573885726519\n",
      "batch 399 Loss 0.17474799786576492 Acc 0.9348693468622572\n",
      "batch 499 Loss 0.1618795363058343 Acc 0.9535601876409244\n",
      "batch 599 Loss 0.16044574291706593 Acc 0.9438806067709392\n",
      "batch 699 Loss 0.17988602010220836 Acc 0.9303672483839994\n",
      "batch 799 Loss 0.20592108195370387 Acc 0.9234055009727895\n",
      "batch 899 Loss 0.1996173407069803 Acc 0.9254327926157486\n",
      "batch 999 Loss 0.14495229294998108 Acc 0.9510454239064124\n",
      "Epoch 1 Loss 0.12109034053247703 Acc 0.9558447718898815\n"
     ]
    }
   ],
   "source": [
    "initialize_all = tf.global_variables_initializer()\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(initialize_all)\n",
    "l_ma=.74\n",
    "acc_ma=.5\n",
    "for epoch in range(2):\n",
    "    for batch in range(train_batches):\n",
    "        data = train[batch*batch_size:(batch+1)*batch_size]\n",
    "        reviews = np.array([sample['w'] for sample in data]).reshape([1,-1,300])\n",
    "        labels  = np.array([sample['y'] for sample in data]).reshape([1,1])\n",
    "        labels = np.array(labels).reshape([-1, 1])\n",
    "        _, l, acc = sess.run([train_step, loss, accuracy], feed_dict={X: reviews, y: labels})\n",
    "        l_ma=.99*l_ma+(.01)*l\n",
    "        acc_ma=.99*acc_ma+(.01)*acc\n",
    "        if (batch+1) % 100 == 0:\n",
    "            print(\"batch\", batch, \"Loss\", l_ma, \"Acc\", acc_ma)\n",
    "    if epoch % 1 == 0:\n",
    "        print(\"Epoch\", epoch, \"Loss\", l_ma, \"Acc\", acc_ma)\n",
    "    random.shuffle(train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 7, 300)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(reviews).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy: 0.9206798866855525\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set\n",
    "test_acc=0\n",
    "n=0\n",
    "for sample in test:\n",
    "    test_reviews = np.array([sample['w'] ]).reshape([1,-1,300])\n",
    "    test_labels  = np.array([sample['y']]).reshape([1,1])\n",
    "    test_labels = np.array(test_labels).reshape([-1, 1])\n",
    "    test_acc += sess.run(accuracy, feed_dict={X: test_reviews, y: test_labels})\n",
    "    n+=1\n",
    "acc=test_acc/n \n",
    "print(\"Final accuracy:\", acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Long Short-Term Memory (LSTM)\n",
    "One popular type of RNNs are Long Short-Term Memory (LSTM) networks, which we went into detail during the class.\n",
    "\n",
    "Trying out LSTMs is fairly easy in code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "# sizes\n",
    "n_steps = None\n",
    "n_inputs = 300\n",
    "n_neurons = 300\n",
    "# Build RNN\n",
    "X= tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "y= tf.placeholder(tf.float32, [None, 1])\n",
    "basic_cell = tf.contrib.rnn.LSTMCell(n_neurons,activation=tf.nn.tanh)\n",
    "outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32)\n",
    "last_cell_output=outputs[:,-1,:]\n",
    "y_=tf.layers.dense(last_cell_output,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and metrics\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=y_, labels=y))\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.round(tf.sigmoid(y_)), y), tf.float32))\n",
    "\n",
    "# Training\n",
    "train_step = tf.train.AdamOptimizer(0.0005).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 99 Loss 0.6953412945933292 Acc 0.5593095263642633\n",
      "batch 199 Loss 0.6473887512329114 Acc 0.6738199371685197\n",
      "batch 299 Loss 0.564437529583084 Acc 0.7915440309288584\n",
      "batch 399 Loss 0.5458760783299365 Acc 0.8075724021252919\n",
      "batch 499 Loss 0.4906432281056557 Acc 0.8252360221980967\n",
      "batch 599 Loss 0.4713405573776868 Acc 0.8270145546202745\n",
      "batch 699 Loss 0.39924134959420376 Acc 0.8576339785394074\n",
      "batch 799 Loss 0.40895472638877217 Acc 0.8463385178050217\n",
      "batch 899 Loss 0.4557517276340091 Acc 0.877230618810834\n",
      "batch 999 Loss 0.34532441152826093 Acc 0.9011750182308512\n",
      "Epoch 0 Loss 0.3375718806407419 Acc 0.8914188728019924\n",
      "batch 99 Loss 0.2754086046505621 Acc 0.9132404911303504\n",
      "batch 199 Loss 0.27054470837469446 Acc 0.9097989992333892\n",
      "batch 299 Loss 0.2936102623103599 Acc 0.911023526005473\n",
      "batch 399 Loss 0.22185479086925006 Acc 0.9285512738763193\n",
      "batch 499 Loss 0.17598988915449443 Acc 0.9408731495579647\n",
      "batch 599 Loss 0.19581428715270224 Acc 0.9504568039849209\n",
      "batch 699 Loss 0.1892672506326283 Acc 0.9304586638753365\n",
      "batch 799 Loss 0.17268633647341664 Acc 0.9352527341065648\n",
      "batch 899 Loss 0.19406867753176008 Acc 0.9382499021703805\n",
      "batch 999 Loss 0.21473378950890076 Acc 0.9198688204366896\n",
      "Epoch 1 Loss 0.1590572937043119 Acc 0.9408383066877845\n",
      "batch 99 Loss 0.1978376562232275 Acc 0.9491453097324124\n",
      "batch 199 Loss 0.21198337560596484 Acc 0.9402336598010543\n",
      "batch 299 Loss 0.18428794907254484 Acc 0.9383898730589659\n",
      "batch 399 Loss 0.25033741347930677 Acc 0.9041054850232156\n",
      "batch 499 Loss 0.21117542212383794 Acc 0.9091191665310749\n",
      "batch 599 Loss 0.12991171420928582 Acc 0.9383685498855006\n",
      "batch 699 Loss 0.16668684499968744 Acc 0.9200140716546521\n",
      "batch 799 Loss 0.15112470786989135 Acc 0.9443614133670645\n",
      "batch 899 Loss 0.16221753111028178 Acc 0.9647986403742884\n",
      "batch 999 Loss 0.1511903726115548 Acc 0.9699506529952472\n",
      "Epoch 2 Loss 0.13222797860906269 Acc 0.9674540992623971\n",
      "batch 99 Loss 0.16262947661773983 Acc 0.9657910161580159\n",
      "batch 199 Loss 0.14076553206980819 Acc 0.9621881352901234\n",
      "batch 299 Loss 0.09528080675301633 Acc 0.9694846565955765\n",
      "batch 399 Loss 0.0737055127356787 Acc 0.9694390453538533\n",
      "batch 499 Loss 0.056428087177267626 Acc 0.9810354886253342\n",
      "batch 599 Loss 0.13635656391271786 Acc 0.9560038406053353\n",
      "batch 699 Loss 0.11407181580647767 Acc 0.9752003885809629\n",
      "batch 799 Loss 0.14001847222242284 Acc 0.9545731238333646\n",
      "batch 899 Loss 0.07925040777338936 Acc 0.9833722941599974\n",
      "batch 999 Loss 0.06927250248979454 Acc 0.9766569255858748\n",
      "Epoch 3 Loss 0.06557687651182652 Acc 0.9869683519849912\n",
      "batch 99 Loss 0.044521400903734194 Acc 0.9868005634325786\n",
      "batch 199 Loss 0.05409756523233477 Acc 0.986431273052968\n",
      "batch 299 Loss 0.028690791762923867 Acc 0.9950334071074803\n",
      "batch 399 Loss 0.04615020902697561 Acc 0.9825154252154727\n",
      "batch 499 Loss 0.05063849156032516 Acc 0.9881285137315452\n",
      "batch 599 Loss 0.08314132179831517 Acc 0.9741564112485223\n",
      "batch 699 Loss 0.06444886641349144 Acc 0.9838506931166973\n",
      "batch 799 Loss 0.05479282081920439 Acc 0.9782771516288112\n",
      "batch 899 Loss 0.05523024549828108 Acc 0.9798730593265956\n",
      "batch 999 Loss 0.12359336992013482 Acc 0.9731983402951221\n",
      "Epoch 4 Loss 0.14127452347698283 Acc 0.9591620656606079\n"
     ]
    }
   ],
   "source": [
    "initialize_all = tf.global_variables_initializer()\n",
    "# Apply the function to each line in the file.\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(initialize_all)\n",
    "l_ma=.74\n",
    "acc_ma=.5\n",
    "for epoch in range(5):\n",
    "    for batch in range(train_batches):\n",
    "        data = train[batch*batch_size:(batch+1)*batch_size]\n",
    "        reviews = np.array([sample['w'] for sample in data]).reshape([1,-1,300])\n",
    "        labels  = np.array([sample['y'] for sample in data]).reshape([1,1])\n",
    "        labels = np.array(labels).reshape([-1, 1])\n",
    "        _, l, acc = sess.run([train_step, loss, accuracy], feed_dict={X: reviews, y: labels})\n",
    "        l_ma=.99*l_ma+(.01)*l\n",
    "        acc_ma=.99*acc_ma+(.01)*acc\n",
    "        if (batch+1) % 100 == 0:\n",
    "            print(\"batch\", batch, \"Loss\", l_ma, \"Acc\", acc_ma)\n",
    "    if epoch % 1 == 0:\n",
    "        print(\"Epoch\", epoch, \"Loss\", l_ma, \"Acc\", acc_ma)\n",
    "    random.shuffle(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy: 0.9631728045325779\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set\n",
    "test_acc=0\n",
    "n=0\n",
    "for sample in test:\n",
    "    test_reviews = np.array([sample['w'] ]).reshape([1,-1,300])\n",
    "    test_labels  = np.array([sample['y']]).reshape([1,1])\n",
    "    test_labels = np.array(test_labels).reshape([-1, 1])\n",
    "    test_acc += sess.run(accuracy, feed_dict={X: test_reviews, y: test_labels})\n",
    "    n+=1\n",
    "acc=test_acc/n \n",
    "print(\"Final accuracy:\", acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Swapping out the more complex dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Data/movie-pang02.txt\", \"r\",encoding=enc) as f:\n",
    "    dataset = [convert_line_to_example(l) for l in f.readlines()]\n",
    "import random\n",
    "random.shuffle(dataset)\n",
    "batch_size = 1\n",
    "total_batches = len(dataset) // batch_size\n",
    "train_batches = 3 * total_batches // 4\n",
    "train, test = dataset[:train_batches*batch_size], dataset[train_batches*batch_size:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 99 Loss 0.7127559205358465 Acc 0.5117723937439334\n",
      "batch 199 Loss 0.7041729767792424 Acc 0.476417691398714\n",
      "batch 299 Loss 0.7136889316928847 Acc 0.5247999034197425\n",
      "batch 399 Loss 0.700590607597621 Acc 0.5166465018942937\n",
      "batch 499 Loss 0.6973219907535622 Acc 0.4891128552295483\n",
      "batch 599 Loss 0.6879451475568742 Acc 0.5301195300376083\n",
      "batch 699 Loss 0.6852633044628296 Acc 0.5582710913652682\n",
      "batch 799 Loss 0.6824738077939042 Acc 0.5483043370396196\n",
      "batch 899 Loss 0.6814669891477889 Acc 0.5565460319777348\n",
      "batch 999 Loss 0.6855136773754444 Acc 0.5864027234411453\n",
      "batch 1099 Loss 0.6800206481604915 Acc 0.6105919246898751\n",
      "batch 1199 Loss 0.676026827809165 Acc 0.6062120632777231\n",
      "batch 1299 Loss 0.6929756899802499 Acc 0.5315496961651291\n",
      "batch 1399 Loss 0.6921897788119804 Acc 0.5085756584298227\n",
      "batch 1499 Loss 0.6866363371058599 Acc 0.5418367911951362\n",
      "Epoch 0 Loss 0.6866363371058599 Acc 0.5418367911951362\n",
      "batch 99 Loss 0.6641352750016342 Acc 0.6181060256888042\n"
     ]
    }
   ],
   "source": [
    "initialize_all = tf.global_variables_initializer()\n",
    "# Apply the function to each line in the file.\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(initialize_all)\n",
    "l_ma=.74\n",
    "acc_ma=.5\n",
    "for epoch in range(5):\n",
    "    for batch in range(train_batches):\n",
    "        data = train[batch*batch_size:(batch+1)*batch_size]\n",
    "        reviews = np.array([sample['w'] for sample in data]).reshape([1,-1,300])\n",
    "        labels  = np.array([sample['y'] for sample in data]).reshape([1,1])\n",
    "        labels = np.array(labels).reshape([-1, 1])\n",
    "        _, l, acc = sess.run([train_step, loss, accuracy], feed_dict={X: reviews, y: labels})\n",
    "        l_ma=.99*l_ma+(.01)*l\n",
    "        acc_ma=.99*acc_ma+(.01)*acc\n",
    "        if (batch+1) % 100 == 0:\n",
    "            print(\"batch\", batch, \"Loss\", l_ma, \"Acc\", acc_ma)\n",
    "    if epoch % 1 == 0:\n",
    "        print(\"Epoch\", epoch, \"Loss\", l_ma, \"Acc\", acc_ma)\n",
    "    random.shuffle(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other materials:\n",
    "Like Reinforcement Learning, Natural Language Processing can also easily be several full courses on its own, both with or without neural networks.\n",
    "Over at UNC, Prof Mohit Bansal has [taught](http://www.cs.unc.edu/~mbansal/teaching/nlp-course-fall17.html) [several](http://www.cs.unc.edu/~mbansal/teaching/nlp-seminar-spring18.html).\n",
    "\n",
    "- [Introduction to LSTMs](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "- [Popular blog post on RNNs](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
