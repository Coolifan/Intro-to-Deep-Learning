{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Duke Community Standard](http://integrity.duke.edu/standard.html): By typing your name below, you are certifying that you have adhered to the Duke Community Standard in completing this assignment.**\n",
    "\n",
    "Name: Yifan Li"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2:  Practical Implementations of Optimization Functions (27 Points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-3d7df07254f2>:4: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "# Create the MLP model\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y_ = tf.placeholder(tf.float32, [None, 10]) #true labels\n",
    "\n",
    "n_hidden_1 = 256 # number of hidden nodes in hidden layer 1\n",
    "n_hidden_2 = 64 # number of hidden nodes in hidden layer 2\n",
    "W1 = tf.Variable(initial_value=tf.truncated_normal([784, n_hidden_1], stddev=0.1)) #layer 1\n",
    "b1 = tf.Variable(tf.zeros([n_hidden_1]))\n",
    "z1 = tf.nn.relu(tf.matmul(x, W1) + b1)\n",
    "W2 = tf.Variable(initial_value=tf.truncated_normal([n_hidden_1, n_hidden_2], stddev=0.1)) #layer 2\n",
    "b2 = tf.Variable(tf.zeros([n_hidden_2]))\n",
    "z2 = tf.nn.relu(tf.matmul(z1, W2) + b2)\n",
    "W3 = tf.Variable(initial_value=tf.truncated_normal([n_hidden_2, 10], stddev=0.1)) #layer 3\n",
    "b3 = tf.Variable(tf.zeros([10]))\n",
    "y = tf.matmul(z2, W3) + b3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SGD trial 1 --- stepsize = 1:\n",
      "\n",
      "Iter: 0: validation accuracy: 0.16040000319480896\n",
      "Iter: 1000: validation accuracy: 0.9714000225067139\n",
      "Iter: 2000: validation accuracy: 0.9688000082969666\n",
      "Iter: 3000: validation accuracy: 0.9746000170707703\n",
      "Iter: 4000: validation accuracy: 0.9783999919891357\n",
      "Iter: 5000: validation accuracy: 0.9801999926567078\n",
      "Iter: 6000: validation accuracy: 0.9801999926567078\n",
      "Iter: 7000: validation accuracy: 0.9796000123023987\n",
      "Iter: 8000: validation accuracy: 0.9797999858856201\n",
      "Iter: 9000: validation accuracy: 0.9796000123023987\n",
      "Iter: 9999: validation accuracy: 0.9787999987602234\n"
     ]
    }
   ],
   "source": [
    "# (a)\n",
    "# SGD trial 1, stepsize = 1\n",
    "# Define loss and optimizer\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_, logits=y))\n",
    "train_step = tf.train.GradientDescentOptimizer(1).minimize(cross_entropy)\n",
    "\n",
    "# Learning\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print('\\nSGD trial 1 --- stepsize = 1:\\n')\n",
    "\n",
    "for i in range(10000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n",
    "    if i % 1000 == 0 or i == 9999: # Evaluation\n",
    "        correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        print('Iter: {}: validation accuracy: {}'.format(i, sess.run(accuracy, feed_dict={x: mnist.validation.images, y_: mnist.validation.labels})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SGD trial 2 --- stepsize = 0.7:\n",
      "\n",
      "Iter: 0: validation accuracy: 0.28279998898506165\n",
      "Iter: 1000: validation accuracy: 0.9649999737739563\n",
      "Iter: 2000: validation accuracy: 0.9753999710083008\n",
      "Iter: 3000: validation accuracy: 0.9764000177383423\n",
      "Iter: 4000: validation accuracy: 0.978600025177002\n",
      "Iter: 5000: validation accuracy: 0.9796000123023987\n",
      "Iter: 6000: validation accuracy: 0.9824000000953674\n",
      "Iter: 7000: validation accuracy: 0.9775999784469604\n",
      "Iter: 8000: validation accuracy: 0.9805999994277954\n",
      "Iter: 9000: validation accuracy: 0.9833999872207642\n",
      "Iter: 9999: validation accuracy: 0.9828000068664551\n"
     ]
    }
   ],
   "source": [
    "# SGD trial 1, stepsize = 0.7\n",
    "# Define loss and optimizer\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_, logits=y))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.7).minimize(cross_entropy)\n",
    "\n",
    "# Learning\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print('\\nSGD trial 2 --- stepsize = 0.7:\\n')\n",
    "\n",
    "for i in range(10000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys}) # fill in the placeholders\n",
    "    if i % 1000 == 0 or i == 9999: # Evaluation\n",
    "        correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        print('Iter: {}: validation accuracy: {}'.format(i, sess.run(accuracy, feed_dict={x: mnist.validation.images, y_: mnist.validation.labels})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SGD trial 3 --- stepsize = 0.5:\n",
      "\n",
      "Iter: 0: validation accuracy: 0.24420000612735748\n",
      "Iter: 1000: validation accuracy: 0.9667999744415283\n",
      "Iter: 2000: validation accuracy: 0.977400004863739\n",
      "Iter: 3000: validation accuracy: 0.978600025177002\n",
      "Iter: 4000: validation accuracy: 0.9796000123023987\n",
      "Iter: 5000: validation accuracy: 0.9842000007629395\n",
      "Iter: 6000: validation accuracy: 0.9846000075340271\n",
      "Iter: 7000: validation accuracy: 0.9851999878883362\n",
      "Iter: 8000: validation accuracy: 0.9843999743461609\n",
      "Iter: 9000: validation accuracy: 0.9846000075340271\n",
      "Iter: 9999: validation accuracy: 0.9850000143051147\n"
     ]
    }
   ],
   "source": [
    "# SGD trial 3, stepsize = 0.5\n",
    "# Define loss and optimizer\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_, logits=y))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "\n",
    "# Learning\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print('\\nSGD trial 3 --- stepsize = 0.5:\\n')\n",
    "\n",
    "for i in range(10000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys}) # fill in the placeholders\n",
    "    if i % 1000 == 0 or i == 9999: # Evaluation\n",
    "        correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        print('Iter: {}: validation accuracy: {}'.format(i, sess.run(accuracy, feed_dict={x: mnist.validation.images, y_: mnist.validation.labels})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SGD trial 4 --- stepsize = 0.01:\n",
      "\n",
      "Iter: 0: validation accuracy: 0.1995999962091446\n",
      "Iter: 1000: validation accuracy: 0.9688000082969666\n",
      "Iter: 2000: validation accuracy: 0.9793999791145325\n",
      "Iter: 3000: validation accuracy: 0.9782000184059143\n",
      "Iter: 4000: validation accuracy: 0.9805999994277954\n",
      "Iter: 5000: validation accuracy: 0.9824000000953674\n",
      "Iter: 6000: validation accuracy: 0.9829999804496765\n",
      "Iter: 7000: validation accuracy: 0.9836000204086304\n",
      "Iter: 8000: validation accuracy: 0.984000027179718\n",
      "Iter: 9000: validation accuracy: 0.9842000007629395\n",
      "Iter: 9999: validation accuracy: 0.9842000007629395\n"
     ]
    }
   ],
   "source": [
    "# SGD trial 4, stepsize = 0.3\n",
    "# Define loss and optimizer\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_, logits=y))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.3).minimize(cross_entropy)\n",
    "\n",
    "# Learning\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print('\\nSGD trial 4 --- stepsize = 0.01:\\n')\n",
    "\n",
    "for i in range(10000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys}) # fill in the placeholders\n",
    "    if i % 1000 == 0 or i == 9999: # Evaluation\n",
    "        correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        print('Iter: {}: validation accuracy: {}'.format(i, sess.run(accuracy, feed_dict={x: mnist.validation.images, y_: mnist.validation.labels})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SGD trial 5 --- stepsize = 0.1:\n",
      "\n",
      "Iter: 0: validation accuracy: 0.1420000046491623\n",
      "Iter: 1000: validation accuracy: 0.949999988079071\n",
      "Iter: 2000: validation accuracy: 0.965399980545044\n",
      "Iter: 3000: validation accuracy: 0.9706000089645386\n",
      "Iter: 4000: validation accuracy: 0.9739999771118164\n",
      "Iter: 5000: validation accuracy: 0.9768000245094299\n",
      "Iter: 6000: validation accuracy: 0.9775999784469604\n",
      "Iter: 7000: validation accuracy: 0.9783999919891357\n",
      "Iter: 8000: validation accuracy: 0.9783999919891357\n",
      "Iter: 9000: validation accuracy: 0.9793999791145325\n",
      "Iter: 9999: validation accuracy: 0.9787999987602234\n"
     ]
    }
   ],
   "source": [
    "# SGD trial 5, stepsize = 0.1\n",
    "# Define loss and optimizer\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_, logits=y))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(cross_entropy)\n",
    "\n",
    "# Learning\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print('\\nSGD trial 5 --- stepsize = 0.1:\\n')\n",
    "\n",
    "for i in range(10000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys}) # fill in the placeholders\n",
    "    if i % 1000 == 0 or i == 9999: # Evaluation\n",
    "        correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        print('Iter: {}: validation accuracy: {}'.format(i, sess.run(accuracy, feed_dict={x: mnist.validation.images, y_: mnist.validation.labels})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SGD trial 6 --- stepsize = 0.01:\n",
      "\n",
      "Iter: 0: validation accuracy: 0.09640000015497208\n",
      "Iter: 1000: validation accuracy: 0.8925999999046326\n",
      "Iter: 2000: validation accuracy: 0.9156000018119812\n",
      "Iter: 3000: validation accuracy: 0.9269999861717224\n",
      "Iter: 4000: validation accuracy: 0.9337999820709229\n",
      "Iter: 5000: validation accuracy: 0.9401999711990356\n",
      "Iter: 6000: validation accuracy: 0.9434000253677368\n",
      "Iter: 7000: validation accuracy: 0.9476000070571899\n",
      "Iter: 8000: validation accuracy: 0.949999988079071\n",
      "Iter: 9000: validation accuracy: 0.954800009727478\n",
      "Iter: 9999: validation accuracy: 0.9563999772071838\n"
     ]
    }
   ],
   "source": [
    "# SGD trial 6, stepsize = 0.01\n",
    "# Define loss and optimizer\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_, logits=y))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n",
    "\n",
    "# Learning\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print('\\nSGD trial 6 --- stepsize = 0.01:\\n')\n",
    "\n",
    "for i in range(10000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys}) # fill in the placeholders\n",
    "    if i % 1000 == 0 or i == 9999: # Evaluation\n",
    "        correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        print('Iter: {}: validation accuracy: {}'.format(i, sess.run(accuracy, feed_dict={x: mnist.validation.images, y_: mnist.validation.labels})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) SGD with momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SGD w/ momentum trial 1 --- stepsize = 0.5, momentum = 0.9:\n",
      "\n",
      "Iter: 0: validation accuracy: 0.22439999878406525\n",
      "Iter: 1000: validation accuracy: 0.11259999871253967\n",
      "Iter: 2000: validation accuracy: 0.11259999871253967\n",
      "Iter: 3000: validation accuracy: 0.11259999871253967\n",
      "Iter: 4000: validation accuracy: 0.11259999871253967\n",
      "Iter: 5000: validation accuracy: 0.09860000014305115\n",
      "Iter: 6000: validation accuracy: 0.10700000077486038\n",
      "Iter: 7000: validation accuracy: 0.10999999940395355\n",
      "Iter: 8000: validation accuracy: 0.11259999871253967\n",
      "Iter: 9000: validation accuracy: 0.0957999974489212\n",
      "Iter: 9999: validation accuracy: 0.0989999994635582\n"
     ]
    }
   ],
   "source": [
    "# (b)\n",
    "\n",
    "# SGD with momentum trial 1, stepsize = 0.5, momentum = 0.9\n",
    "# Define loss and optimizer\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_, logits=y))\n",
    "train_step = tf.train.MomentumOptimizer(learning_rate=0.5, momentum=0.9).minimize(cross_entropy)\n",
    "\n",
    "# Learning\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print('\\nSGD w/ momentum trial 1 --- stepsize = 0.5, momentum = 0.9:\\n')\n",
    "\n",
    "for i in range(10000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n",
    "    if i % 1000 == 0 or i == 9999: # Evaluation\n",
    "        correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        print('Iter: {}: validation accuracy: {}'.format(i, sess.run(accuracy, feed_dict={x: mnist.validation.images, y_: mnist.validation.labels})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SGD w/ momentum trial 2 --- stepsize = 0.1, momentum = 0.9:\n",
      "\n",
      "Iter: 0: validation accuracy: 0.07940000295639038\n",
      "Iter: 1000: validation accuracy: 0.9667999744415283\n",
      "Iter: 2000: validation accuracy: 0.9742000102996826\n",
      "Iter: 3000: validation accuracy: 0.9814000129699707\n",
      "Iter: 4000: validation accuracy: 0.977400004863739\n",
      "Iter: 5000: validation accuracy: 0.9810000061988831\n",
      "Iter: 6000: validation accuracy: 0.9807999730110168\n",
      "Iter: 7000: validation accuracy: 0.9807999730110168\n",
      "Iter: 8000: validation accuracy: 0.9797999858856201\n",
      "Iter: 9000: validation accuracy: 0.9746000170707703\n",
      "Iter: 9999: validation accuracy: 0.9805999994277954\n"
     ]
    }
   ],
   "source": [
    "# SGD with momentum trial 2, stepsize = 0.1, momentum = 0.9\n",
    "# Define loss and optimizer\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_, logits=y))\n",
    "train_step = tf.train.MomentumOptimizer(learning_rate=0.1, momentum=0.9).minimize(cross_entropy)\n",
    "\n",
    "# Learning\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print('\\nSGD w/ momentum trial 2 --- stepsize = 0.1, momentum = 0.9:\\n')\n",
    "\n",
    "for i in range(10000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n",
    "    if i % 1000 == 0 or i == 9999: # Evaluation\n",
    "        correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        print('Iter: {}: validation accuracy: {}'.format(i, sess.run(accuracy, feed_dict={x: mnist.validation.images, y_: mnist.validation.labels})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SGD w/ momentum trial 3 --- stepsize = 0.1, momentum = 0.5:\n",
      "\n",
      "Iter: 0: validation accuracy: 0.17399999499320984\n",
      "Iter: 1000: validation accuracy: 0.9639999866485596\n",
      "Iter: 2000: validation accuracy: 0.9697999954223633\n",
      "Iter: 3000: validation accuracy: 0.9769999980926514\n",
      "Iter: 4000: validation accuracy: 0.979200005531311\n",
      "Iter: 5000: validation accuracy: 0.9793999791145325\n",
      "Iter: 6000: validation accuracy: 0.980400025844574\n",
      "Iter: 7000: validation accuracy: 0.9818000197410583\n",
      "Iter: 8000: validation accuracy: 0.980400025844574\n",
      "Iter: 9000: validation accuracy: 0.979200005531311\n",
      "Iter: 9999: validation accuracy: 0.9832000136375427\n"
     ]
    }
   ],
   "source": [
    "# SGD with momentum trial 3, stepsize = 0.1, momentum = 0.5\n",
    "# Define loss and optimizer\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_, logits=y))\n",
    "train_step = tf.train.MomentumOptimizer(learning_rate=0.1, momentum=0.5).minimize(cross_entropy)\n",
    "\n",
    "# Learning\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print('\\nSGD w/ momentum trial 3 --- stepsize = 0.1, momentum = 0.5:\\n')\n",
    "\n",
    "for i in range(10000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n",
    "    if i % 1000 == 0 or i == 9999: # Evaluation\n",
    "        correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        print('Iter: {}: validation accuracy: {}'.format(i, sess.run(accuracy, feed_dict={x: mnist.validation.images, y_: mnist.validation.labels})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SGD w/ momentum trial 4 --- stepsize = 0.05, momentum = 0.9:\n",
      "\n",
      "Iter: 0: validation accuracy: 0.10440000146627426\n",
      "Iter: 1000: validation accuracy: 0.9648000001907349\n",
      "Iter: 2000: validation accuracy: 0.972599983215332\n",
      "Iter: 3000: validation accuracy: 0.9779999852180481\n",
      "Iter: 4000: validation accuracy: 0.9807999730110168\n",
      "Iter: 5000: validation accuracy: 0.9796000123023987\n",
      "Iter: 6000: validation accuracy: 0.9810000061988831\n",
      "Iter: 7000: validation accuracy: 0.9832000136375427\n",
      "Iter: 8000: validation accuracy: 0.9847999811172485\n",
      "Iter: 9000: validation accuracy: 0.9843999743461609\n",
      "Iter: 9999: validation accuracy: 0.9847999811172485\n"
     ]
    }
   ],
   "source": [
    "# SGD with momentum trial 4, stepsize = 0.05, momentum = 0.9\n",
    "# Define loss and optimizer\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_, logits=y))\n",
    "train_step = tf.train.MomentumOptimizer(learning_rate=0.05, momentum=0.9).minimize(cross_entropy)\n",
    "\n",
    "# Learning\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print('\\nSGD w/ momentum trial 4 --- stepsize = 0.05, momentum = 0.9:\\n')\n",
    "\n",
    "for i in range(10000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n",
    "    if i % 1000 == 0 or i == 9999: # Evaluation\n",
    "        correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        print('Iter: {}: validation accuracy: {}'.format(i, sess.run(accuracy, feed_dict={x: mnist.validation.images, y_: mnist.validation.labels})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SGD w/ momentum trial 5 --- stepsize = 0.07, momentum = 0.9:\n",
      "\n",
      "Iter: 0: validation accuracy: 0.09459999948740005\n",
      "Iter: 1000: validation accuracy: 0.9697999954223633\n",
      "Iter: 2000: validation accuracy: 0.9765999913215637\n",
      "Iter: 3000: validation accuracy: 0.9793999791145325\n",
      "Iter: 4000: validation accuracy: 0.9818000197410583\n",
      "Iter: 5000: validation accuracy: 0.9801999926567078\n",
      "Iter: 6000: validation accuracy: 0.9793999791145325\n",
      "Iter: 7000: validation accuracy: 0.9818000197410583\n",
      "Iter: 8000: validation accuracy: 0.9801999926567078\n",
      "Iter: 9000: validation accuracy: 0.9825999736785889\n",
      "Iter: 9999: validation accuracy: 0.984000027179718\n"
     ]
    }
   ],
   "source": [
    "# SGD with momentum trial 5, stepsize = 0.07, momentum = 0.9\n",
    "# Define loss and optimizer\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_, logits=y))\n",
    "train_step = tf.train.MomentumOptimizer(learning_rate=0.07, momentum=0.9).minimize(cross_entropy)\n",
    "\n",
    "# Learning\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print('\\nSGD w/ momentum trial 5 --- stepsize = 0.07, momentum = 0.9:\\n')\n",
    "\n",
    "for i in range(10000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n",
    "    if i % 1000 == 0 or i == 9999: # Evaluation\n",
    "        correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        print('Iter: {}: validation accuracy: {}'.format(i, sess.run(accuracy, feed_dict={x: mnist.validation.images, y_: mnist.validation.labels})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Adam with default settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adam trial 1 --- default settings:\n",
      "\n",
      "Iter: 0: validation accuracy: 0.1324000060558319\n",
      "Iter: 1000: validation accuracy: 0.9692000150680542\n",
      "Iter: 2000: validation accuracy: 0.977400004863739\n",
      "Iter: 3000: validation accuracy: 0.9782000184059143\n",
      "Iter: 4000: validation accuracy: 0.9760000109672546\n",
      "Iter: 5000: validation accuracy: 0.9807999730110168\n",
      "Iter: 6000: validation accuracy: 0.9757999777793884\n",
      "Iter: 7000: validation accuracy: 0.9814000129699707\n",
      "Iter: 8000: validation accuracy: 0.9775999784469604\n",
      "Iter: 9000: validation accuracy: 0.9779999852180481\n",
      "Iter: 9999: validation accuracy: 0.9797999858856201\n"
     ]
    }
   ],
   "source": [
    "# Adam trial 1, default settings\n",
    "# Define loss and optimizer\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_, logits=y))\n",
    "train_step = tf.train.AdamOptimizer().minimize(cross_entropy)\n",
    "\n",
    "# Learning\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print('\\nAdam trial 1 --- default settings:\\n')\n",
    "\n",
    "for i in range(10000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n",
    "    if i % 1000 == 0 or i == 9999: # Evaluation\n",
    "        correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        print('Iter: {}: validation accuracy: {}'.format(i, sess.run(accuracy, feed_dict={x: mnist.validation.images, y_: mnist.validation.labels})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) Adam with various stepsizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adam trial 2 --- stepsize = 0.5:\n",
      "\n",
      "Iter: 0: validation accuracy: 0.10999999940395355\n",
      "Iter: 1000: validation accuracy: 0.2134000062942505\n",
      "Iter: 2000: validation accuracy: 0.11259999871253967\n",
      "Iter: 3000: validation accuracy: 0.11259999871253967\n",
      "Iter: 4000: validation accuracy: 0.11259999871253967\n",
      "Iter: 5000: validation accuracy: 0.0957999974489212\n",
      "Iter: 6000: validation accuracy: 0.10700000077486038\n",
      "Iter: 7000: validation accuracy: 0.10019999742507935\n",
      "Iter: 8000: validation accuracy: 0.09239999949932098\n",
      "Iter: 9000: validation accuracy: 0.11259999871253967\n",
      "Iter: 9999: validation accuracy: 0.09860000014305115\n"
     ]
    }
   ],
   "source": [
    "# Adam trial 2, stepsize = 0.5\n",
    "# Define loss and optimizer\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_, logits=y))\n",
    "train_step = tf.train.AdamOptimizer(learning_rate=0.5).minimize(cross_entropy)\n",
    "\n",
    "# Learning\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print('\\nAdam trial 2 --- stepsize = 0.5:\\n')\n",
    "\n",
    "for i in range(10000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n",
    "    if i % 1000 == 0 or i == 9999: # Evaluation\n",
    "        correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        print('Iter: {}: validation accuracy: {}'.format(i, sess.run(accuracy, feed_dict={x: mnist.validation.images, y_: mnist.validation.labels})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adam trial 3 --- stepsize = 0.1:\n",
      "\n",
      "Iter: 0: validation accuracy: 0.14100000262260437\n",
      "Iter: 1000: validation accuracy: 0.6850000023841858\n",
      "Iter: 2000: validation accuracy: 0.5473999977111816\n",
      "Iter: 3000: validation accuracy: 0.40880000591278076\n",
      "Iter: 4000: validation accuracy: 0.5590000152587891\n",
      "Iter: 5000: validation accuracy: 0.4869999885559082\n",
      "Iter: 6000: validation accuracy: 0.4740000069141388\n",
      "Iter: 7000: validation accuracy: 0.45260000228881836\n",
      "Iter: 8000: validation accuracy: 0.37880000472068787\n",
      "Iter: 9000: validation accuracy: 0.43939998745918274\n",
      "Iter: 9999: validation accuracy: 0.257999986410141\n"
     ]
    }
   ],
   "source": [
    "# Adam trial 3, stepsize = 0.1\n",
    "# Define loss and optimizer\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_, logits=y))\n",
    "train_step = tf.train.AdamOptimizer(learning_rate=0.1).minimize(cross_entropy)\n",
    "\n",
    "# Learning\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print('\\nAdam trial 3 --- stepsize = 0.1:\\n')\n",
    "\n",
    "for i in range(10000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n",
    "    if i % 1000 == 0 or i == 9999: # Evaluation\n",
    "        correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        print('Iter: {}: validation accuracy: {}'.format(i, sess.run(accuracy, feed_dict={x: mnist.validation.images, y_: mnist.validation.labels})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adam trial 4 --- stepsize = 0.01:\n",
      "\n",
      "Iter: 0: validation accuracy: 0.21860000491142273\n",
      "Iter: 1000: validation accuracy: 0.9607999920845032\n",
      "Iter: 2000: validation accuracy: 0.9684000015258789\n",
      "Iter: 3000: validation accuracy: 0.9739999771118164\n",
      "Iter: 4000: validation accuracy: 0.9711999893188477\n",
      "Iter: 5000: validation accuracy: 0.968999981880188\n",
      "Iter: 6000: validation accuracy: 0.974399983882904\n",
      "Iter: 7000: validation accuracy: 0.9721999764442444\n",
      "Iter: 8000: validation accuracy: 0.9715999960899353\n",
      "Iter: 9000: validation accuracy: 0.9757999777793884\n",
      "Iter: 9999: validation accuracy: 0.9729999899864197\n"
     ]
    }
   ],
   "source": [
    "# Adam trial 4, stepsize = 0.01\n",
    "# Define loss and optimizer\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_, logits=y))\n",
    "train_step = tf.train.AdamOptimizer(learning_rate=0.01).minimize(cross_entropy)\n",
    "\n",
    "# Learning\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print('\\nAdam trial 4 --- stepsize = 0.01:\\n')\n",
    "\n",
    "for i in range(10000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n",
    "    if i % 1000 == 0 or i == 9999: # Evaluation\n",
    "        correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        print('Iter: {}: validation accuracy: {}'.format(i, sess.run(accuracy, feed_dict={x: mnist.validation.images, y_: mnist.validation.labels})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adam trial 5 --- stepsize = 0.001:\n",
      "\n",
      "Iter: 0: validation accuracy: 0.16740000247955322\n",
      "Iter: 1000: validation accuracy: 0.9715999960899353\n",
      "Iter: 2000: validation accuracy: 0.9753999710083008\n",
      "Iter: 3000: validation accuracy: 0.9797999858856201\n",
      "Iter: 4000: validation accuracy: 0.980400025844574\n",
      "Iter: 5000: validation accuracy: 0.9807999730110168\n",
      "Iter: 6000: validation accuracy: 0.9805999994277954\n",
      "Iter: 7000: validation accuracy: 0.9819999933242798\n",
      "Iter: 8000: validation accuracy: 0.9782000184059143\n",
      "Iter: 9000: validation accuracy: 0.9814000129699707\n",
      "Iter: 9999: validation accuracy: 0.978600025177002\n"
     ]
    }
   ],
   "source": [
    "# Adam trial 5, stepsize = 0.001\n",
    "# Define loss and optimizer\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_, logits=y))\n",
    "train_step = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cross_entropy)\n",
    "\n",
    "# Learning\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print('\\nAdam trial 5 --- stepsize = 0.001:\\n')\n",
    "\n",
    "for i in range(10000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n",
    "    if i % 1000 == 0 or i == 9999: # Evaluation\n",
    "        correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        print('Iter: {}: validation accuracy: {}'.format(i, sess.run(accuracy, feed_dict={x: mnist.validation.images, y_: mnist.validation.labels})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adam trial 6 --- stepsize = 0.0005:\n",
      "\n",
      "Iter: 0: validation accuracy: 0.10920000076293945\n",
      "Iter: 1000: validation accuracy: 0.9617999792098999\n",
      "Iter: 2000: validation accuracy: 0.9732000231742859\n",
      "Iter: 3000: validation accuracy: 0.978600025177002\n",
      "Iter: 4000: validation accuracy: 0.9801999926567078\n",
      "Iter: 5000: validation accuracy: 0.977400004863739\n",
      "Iter: 6000: validation accuracy: 0.9793999791145325\n",
      "Iter: 7000: validation accuracy: 0.9814000129699707\n",
      "Iter: 8000: validation accuracy: 0.9811999797821045\n",
      "Iter: 9000: validation accuracy: 0.9824000000953674\n",
      "Iter: 9999: validation accuracy: 0.9818000197410583\n"
     ]
    }
   ],
   "source": [
    "# Adam trial 6, stepsize = 0.0005\n",
    "# Define loss and optimizer\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_, logits=y))\n",
    "train_step = tf.train.AdamOptimizer(learning_rate=0.0005).minimize(cross_entropy)\n",
    "\n",
    "# Learning\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print('\\nAdam trial 6 --- stepsize = 0.0005:\\n')\n",
    "\n",
    "for i in range(10000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n",
    "    if i % 1000 == 0 or i == 9999: # Evaluation\n",
    "        correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        print('Iter: {}: validation accuracy: {}'.format(i, sess.run(accuracy, feed_dict={x: mnist.validation.images, y_: mnist.validation.labels})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Short Answers\n",
    "** (a) **\n",
    "Qualitatively, all 4 algorithms (normal SGD, SGD with momentum, default Adam, Adam with various stepsizes) have pretty decent outputs. Each one of these can reach an accuracy of ~98%.\n",
    "\n",
    "Based on the results shown above, the optimal constant stepsize for my multilayer perceptron classifier with 2 hidden layers is about 0.5. Actually, the validation accuracy reaches ~98% after stepsize increases to 0.1, and accuracy continues to increase as learning rate increases. After a learning rate threshold of around 0.5, the accuracy starts to decrease. We can see that the validation accuracy is not fully optimized when stepsize is either too large (1) or too small (0.01).\n",
    "\n",
    "The optimal settings for the SGD optimizer with momentum for my multilayer perceptron classifier is: learning rate = 0.05, momentum = 0.9. In fact, the accuracy is pretty high (~98%) when learning rate is about 0.1 and momentum is 0.9. The accuracy slightly increases as I continue to lower these two parameters. However, a relatively large learning rate of ~0.5 yields a very bad accuracy of 10%.\n",
    "\n",
    "The Adam optimizer with default settings could provide an accuracy of ~98%. And Adam optimizer is quite sensitive to learning rates. For instance, a learning rate of 0.1 is common to use on other optimizers that could generate a decent output, but without changing other parameters, the learning rate of 0.1 will generate a horrible accuracy if used on Adam Optimizer. Generally speaking, when testing on MNIST using my MLP, a smaller learning rate is better for Adam optimizer.\n",
    "\n",
    "** (b) **\n",
    "It is hard to fine-tune the optimal settings for different optimizers. But as long as a acceptable output is generated, I could follow the general trends when tweaking the parameters. When dealing with diffrent datasets, different maching learning models, and different gradient descent optimizers, it will be difficult to find a good entry point.\n",
    "\n",
    "** (c) ** \n",
    "As mentioned above in part (a), all 4 algorithms could perform very well. All of them could reach an validation accuracy of ~98% for MNIST dataset trained on my MLP with 2 hidden layers. Among all the trials, default Adam and SGD with momentum have better overall accuracy when trying with different learning rates. Adam with various learning rates is very sensitive to learning rate if other parameters are set to default. The normal SGD also performs well, but the learning rate is the highest.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Bookkeeping (3 points)\n",
    "\n",
    "### (a) ~ 4 hours\n",
    "\n",
    "### (b)\n",
    "**[Duke Community Standard](http://integrity.duke.edu/standard.html): By typing your name below, you are certifying that you have adhered to the Duke Community Standard in completing this assignment.**\n",
    "\n",
    "Name: Yifan Li\n",
    "\n",
    "I adhered to the Duke Community Standard in the completion of this assignment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
