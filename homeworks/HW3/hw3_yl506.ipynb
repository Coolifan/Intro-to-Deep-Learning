{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "anRuOBj8gFjl"
   },
   "source": [
    "# Intro to Deep Learning, HW3 \n",
    "# Yifan Li, yl506 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OYi3CAutM2h7"
   },
   "source": [
    "**Problem 2: Implementation of a 2-layer CNN with TensorFlow (24 points)**\n",
    "\n",
    "To gain experience with the implementation of CNNs in TensorFlow, please implement a 2-layer\n",
    "CNN followed by 2 fully connected layers as an architecture for a model to classify MNIST digits.\n",
    "\n",
    "a) Specify the network without stride and 3x3 or 5x5 filter sizes. You can choose the number\n",
    "of filters in each layer and the number of hidden units of the first fully connected layer.\n",
    "\n",
    "b) Calculate the receptive field of your 2-layer CNN for a 28x28 MNIST image.\n",
    "\n",
    "Notes:\n",
    "\n",
    "• For the implementation make sure of using TensorFlow’s Core API. You are welcome to\n",
    "try other implementations using higher level APIs (e.g., layers, Slim).\n",
    "\n",
    "• You are only required to use the built-in cross entropy with logits and stochastic gradient\n",
    "descent. If you would like to try any of the optimization rules discussed earlier, you are\n",
    "welcome to.\n",
    "\n",
    "• When initializing the weights of the model (not including biases) it is important to set their\n",
    "initial values to random values as discussed in class. You can use truncated normal\n",
    "distributions as discussed in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "lBe2yRtNM2h9",
    "outputId": "9d8d4bc4-012f-464f-afde-8e77419963b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "(60000, 784)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Import data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "# Split the dataset\n",
    "x_train = np.concatenate((mnist.train.images, mnist.validation.images), axis=0)\n",
    "y_train = np.concatenate((mnist.train.labels, mnist.validation.labels), axis=0)\n",
    "print(x_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1377
    },
    "colab_type": "code",
    "id": "FvHI5WX8ryYD",
    "outputId": "8a15f848-cb01-475c-d495-f03e3c56d444"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------Epoch 1--------------------\n",
      "Iteration 0, Validation Accuracy: 0.09749999642372131\n",
      "Iteration 100, Validation Accuracy: 0.3068999946117401\n",
      "Iteration 200, Validation Accuracy: 0.4569999873638153\n",
      "Iteration 300, Validation Accuracy: 0.6028000116348267\n",
      "Iteration 400, Validation Accuracy: 0.6535999774932861\n",
      "Iteration 500, Validation Accuracy: 0.7569000124931335\n",
      "Iteration 599, Validation Accuracy: 0.7842000126838684\n",
      "----------------Epoch 2--------------------\n",
      "Iteration 0, Validation Accuracy: 0.7703999876976013\n",
      "Iteration 100, Validation Accuracy: 0.8151999711990356\n",
      "Iteration 200, Validation Accuracy: 0.8288999795913696\n",
      "Iteration 300, Validation Accuracy: 0.838699996471405\n",
      "Iteration 400, Validation Accuracy: 0.8439000248908997\n",
      "Iteration 500, Validation Accuracy: 0.8605999946594238\n",
      "Iteration 599, Validation Accuracy: 0.8677999973297119\n",
      "----------------Epoch 3--------------------\n",
      "Iteration 0, Validation Accuracy: 0.8689000010490417\n",
      "Iteration 100, Validation Accuracy: 0.8755999803543091\n",
      "Iteration 200, Validation Accuracy: 0.8792999982833862\n",
      "Iteration 300, Validation Accuracy: 0.8827000260353088\n",
      "Iteration 400, Validation Accuracy: 0.8842999935150146\n",
      "Iteration 500, Validation Accuracy: 0.8939999938011169\n",
      "Iteration 599, Validation Accuracy: 0.8942000269889832\n",
      "----------------Epoch 4--------------------\n",
      "Iteration 0, Validation Accuracy: 0.8952000141143799\n",
      "Iteration 100, Validation Accuracy: 0.9021999835968018\n",
      "Iteration 200, Validation Accuracy: 0.9006999731063843\n",
      "Iteration 300, Validation Accuracy: 0.9053000211715698\n",
      "Iteration 400, Validation Accuracy: 0.9039000272750854\n",
      "Iteration 500, Validation Accuracy: 0.9096999764442444\n",
      "Iteration 599, Validation Accuracy: 0.9081000089645386\n",
      "----------------Epoch 5--------------------\n",
      "Iteration 0, Validation Accuracy: 0.9088000059127808\n",
      "Iteration 100, Validation Accuracy: 0.9156000018119812\n",
      "Iteration 200, Validation Accuracy: 0.9128999710083008\n",
      "Iteration 300, Validation Accuracy: 0.9169999957084656\n",
      "Iteration 400, Validation Accuracy: 0.9150000214576721\n",
      "Iteration 500, Validation Accuracy: 0.9192000031471252\n",
      "Iteration 599, Validation Accuracy: 0.9179999828338623\n",
      "----------------Epoch 6--------------------\n",
      "Iteration 0, Validation Accuracy: 0.9174000024795532\n",
      "Iteration 100, Validation Accuracy: 0.9236000180244446\n",
      "Iteration 200, Validation Accuracy: 0.9211000204086304\n",
      "Iteration 300, Validation Accuracy: 0.9276000261306763\n",
      "Iteration 400, Validation Accuracy: 0.921999990940094\n",
      "Iteration 500, Validation Accuracy: 0.9276999831199646\n",
      "Iteration 599, Validation Accuracy: 0.925000011920929\n",
      "----------------Epoch 7--------------------\n",
      "Iteration 0, Validation Accuracy: 0.9248999953269958\n",
      "Iteration 100, Validation Accuracy: 0.9301999807357788\n",
      "Iteration 200, Validation Accuracy: 0.9294000267982483\n",
      "Iteration 300, Validation Accuracy: 0.9329000115394592\n",
      "Iteration 400, Validation Accuracy: 0.9294999837875366\n",
      "Iteration 500, Validation Accuracy: 0.933899998664856\n",
      "Iteration 599, Validation Accuracy: 0.9319000244140625\n",
      "----------------Epoch 8--------------------\n",
      "Iteration 0, Validation Accuracy: 0.9312000274658203\n",
      "Iteration 100, Validation Accuracy: 0.9362999796867371\n",
      "Iteration 200, Validation Accuracy: 0.9352999925613403\n",
      "Iteration 300, Validation Accuracy: 0.9373000264167786\n",
      "Iteration 400, Validation Accuracy: 0.933899998664856\n",
      "Iteration 500, Validation Accuracy: 0.9391999840736389\n",
      "Iteration 599, Validation Accuracy: 0.9381999969482422\n",
      "----------------Epoch 9--------------------\n",
      "Iteration 0, Validation Accuracy: 0.9362999796867371\n",
      "Iteration 100, Validation Accuracy: 0.9415000081062317\n",
      "Iteration 200, Validation Accuracy: 0.9401999711990356\n",
      "Iteration 300, Validation Accuracy: 0.9401000142097473\n",
      "Iteration 400, Validation Accuracy: 0.9375\n",
      "Iteration 500, Validation Accuracy: 0.9433000087738037\n",
      "Iteration 599, Validation Accuracy: 0.9409999847412109\n",
      "----------------Epoch 10--------------------\n",
      "Iteration 0, Validation Accuracy: 0.9399999976158142\n",
      "Iteration 100, Validation Accuracy: 0.9453999996185303\n",
      "Iteration 200, Validation Accuracy: 0.9434000253677368\n",
      "Iteration 300, Validation Accuracy: 0.9447000026702881\n",
      "Iteration 400, Validation Accuracy: 0.9409000277519226\n",
      "Iteration 500, Validation Accuracy: 0.9466999769210815\n",
      "Iteration 599, Validation Accuracy: 0.9455000162124634\n"
     ]
    }
   ],
   "source": [
    "#Q2(a). TF Core Implementation\n",
    "# filter size 5*5, no pooling, no dropout\n",
    "num_epochs = 10\n",
    "batch_size = 100\n",
    "n_classes = 10\n",
    "# TF Graph Input\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "def maxpool2d(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "\n",
    "weights = {\n",
    "    # First 5*5 convolution, 1 input image, 32 outputs\n",
    "    'W_conv1': tf.Variable(tf.truncated_normal([5, 5, 1, 32], stddev=0.1)),\n",
    "    # Second 5*5 convolution, 32 inputs, 64 outputs\n",
    "    'W_conv2': tf.Variable(tf.truncated_normal([5, 5, 32, 64], stddev=0.1)),\n",
    "    # First fully connected, 28*28*64 inputs, 1024 outputs\n",
    "    'W_fc': tf.Variable(tf.truncated_normal([28*28*64, 1024], stddev=0.1)),\n",
    "    # Output fully connected, 1024 inputs, 10 outputs\n",
    "    'out': tf.Variable(tf.truncated_normal([1024, n_classes], stddev=0.1))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'b_conv1': tf.Variable(tf.random_normal([32])),\n",
    "    'b_conv2': tf.Variable(tf.random_normal([64])),\n",
    "    'b_fc': tf.Variable(tf.constant(0.0, shape=[1024])),\n",
    "    'out': tf.Variable(tf.constant(0.0, shape=[10]))\n",
    "}\n",
    "\n",
    "# Reshape input to a 4D tensor \n",
    "inputs = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "# Convolution Layer\n",
    "conv1 = tf.nn.relu(conv2d(inputs, weights['W_conv1']) + biases['b_conv1'])\n",
    "conv2 = tf.nn.relu(conv2d(conv1, weights['W_conv2']) + biases['b_conv2'])\n",
    "# Fully-connected Layer\n",
    "fc = tf.reshape(conv2, [-1, 28*28*64])\n",
    "fc = tf.nn.relu(tf.matmul(fc, weights['W_fc']) + biases['b_fc'])\n",
    "predictions = tf.matmul(fc, weights['out']) + biases['out']\n",
    "\n",
    "  \n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=predictions))\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.001).minimize(cross_entropy) \n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  print('----------------Epoch {}--------------------' .format(epoch+1))\n",
    "  for i in range(int(num_train/batch_size)):\n",
    "      batch_xs = x_train[i*100 : i*100+batch_size, :]\n",
    "      batch_ys = y_train[i*100 : i*100+batch_size, :]\n",
    "      sess.run(optimizer, feed_dict={x: batch_xs, y: batch_ys})\n",
    "      if i % 100 == 0 or i == 599:\n",
    "        correct_prediction = tf.equal(tf.argmax(tf.nn.softmax(predictions), 1), tf.argmax(y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        print('Iteration {}, Validation Accuracy: {}' .format(i, sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels}) ))\n",
    "\n",
    "\n",
    "sess.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-BffbPhyLFYb"
   },
   "source": [
    "**Q2(b) Answer**\n",
    "\n",
    "To calculate the receptive field, it is assumed that the input and the filters are square. Denote the filter size as $k*k$, stride = $s$, current receptive field size = $r*r$, the stride (distance between 2 adjacent features) =$j$. The derived expressions to calculate receptive field size from problem 1 are: \n",
    "\n",
    "$j_{out} = j_{in} * s$\n",
    "\n",
    "$r_{out} = r_{in} + (k-1)j_{in}$\n",
    "\n",
    "Also, for the input image, $j_0 = r_0 = 1$ , and for the network I am using,  $s = 1, k = 5$, the output feature map has the same size as the input.\n",
    "\n",
    " So, after the first convolutional layer, $j_1 = j_0 s = 1, r_1 = r_0 + (k-1)j_0 = 5$\n",
    " And after the second convolutional layer, $j_2 = 1, r_2 = 9$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HMCZA8JaWQF7"
   },
   "source": [
    "**Problem 3: Adding pooling and dropout to a 2-layer CNN with TensorFlow (24 points)**\n",
    "\n",
    "To gain a better understanding of how pooling and dropout affect the performance of CNNs,\n",
    "please try the following:\n",
    "\n",
    "a) Add 2x2 pooling layers after each convolutional layer for the specification in Problem 2.\n",
    "\n",
    "b) Add dropout after the first fully connected layer for the specification in Problem 2. You\n",
    "can choose the probability of keeping (not setting to zero) hidden unit elements.\n",
    "\n",
    "Notes:\n",
    "• For the pooling layer, you are free to choose between max and average pooling, however\n",
    "we encourage you to use max pooling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1377
    },
    "colab_type": "code",
    "id": "MV-uKCSCO0wl",
    "outputId": "d7b67ace-e2c0-4b1d-e1e6-f7f4e35b9b30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------Epoch 1--------------------\n",
      "Iteration 0, Validation Accuracy: 0.11349999904632568\n",
      "Iteration 100, Validation Accuracy: 0.5482000112533569\n",
      "Iteration 200, Validation Accuracy: 0.7017999887466431\n",
      "Iteration 300, Validation Accuracy: 0.7153000235557556\n",
      "Iteration 400, Validation Accuracy: 0.8154000043869019\n",
      "Iteration 500, Validation Accuracy: 0.8363000154495239\n",
      "Iteration 599, Validation Accuracy: 0.8562999963760376\n",
      "----------------Epoch 2--------------------\n",
      "Iteration 0, Validation Accuracy: 0.8567000031471252\n",
      "Iteration 100, Validation Accuracy: 0.8679999709129333\n",
      "Iteration 200, Validation Accuracy: 0.8641999959945679\n",
      "Iteration 300, Validation Accuracy: 0.871399998664856\n",
      "Iteration 400, Validation Accuracy: 0.8835999965667725\n",
      "Iteration 500, Validation Accuracy: 0.8863999843597412\n",
      "Iteration 599, Validation Accuracy: 0.891700029373169\n",
      "----------------Epoch 3--------------------\n",
      "Iteration 0, Validation Accuracy: 0.8937000036239624\n",
      "Iteration 100, Validation Accuracy: 0.8992000222206116\n",
      "Iteration 200, Validation Accuracy: 0.8937000036239624\n",
      "Iteration 300, Validation Accuracy: 0.8953999876976013\n",
      "Iteration 400, Validation Accuracy: 0.9036999940872192\n",
      "Iteration 500, Validation Accuracy: 0.9038000106811523\n",
      "Iteration 599, Validation Accuracy: 0.9074000120162964\n",
      "----------------Epoch 4--------------------\n",
      "Iteration 0, Validation Accuracy: 0.9103000164031982\n",
      "Iteration 100, Validation Accuracy: 0.9121000170707703\n",
      "Iteration 200, Validation Accuracy: 0.9086999893188477\n",
      "Iteration 300, Validation Accuracy: 0.9103999733924866\n",
      "Iteration 400, Validation Accuracy: 0.9164999723434448\n",
      "Iteration 500, Validation Accuracy: 0.9161999821662903\n",
      "Iteration 599, Validation Accuracy: 0.9175999760627747\n",
      "----------------Epoch 5--------------------\n",
      "Iteration 0, Validation Accuracy: 0.9190999865531921\n",
      "Iteration 100, Validation Accuracy: 0.9226999878883362\n",
      "Iteration 200, Validation Accuracy: 0.9174000024795532\n",
      "Iteration 300, Validation Accuracy: 0.919700026512146\n",
      "Iteration 400, Validation Accuracy: 0.9251999855041504\n",
      "Iteration 500, Validation Accuracy: 0.923799991607666\n",
      "Iteration 599, Validation Accuracy: 0.9244999885559082\n",
      "----------------Epoch 6--------------------\n",
      "Iteration 0, Validation Accuracy: 0.9265000224113464\n",
      "Iteration 100, Validation Accuracy: 0.9298999905586243\n",
      "Iteration 200, Validation Accuracy: 0.9251000285148621\n",
      "Iteration 300, Validation Accuracy: 0.9269999861717224\n",
      "Iteration 400, Validation Accuracy: 0.9305999875068665\n",
      "Iteration 500, Validation Accuracy: 0.9314000010490417\n",
      "Iteration 599, Validation Accuracy: 0.9301999807357788\n",
      "----------------Epoch 7--------------------\n",
      "Iteration 0, Validation Accuracy: 0.932200014591217\n",
      "Iteration 100, Validation Accuracy: 0.9351000189781189\n",
      "Iteration 200, Validation Accuracy: 0.930899977684021\n",
      "Iteration 300, Validation Accuracy: 0.9319999814033508\n",
      "Iteration 400, Validation Accuracy: 0.9351000189781189\n",
      "Iteration 500, Validation Accuracy: 0.9366000294685364\n",
      "Iteration 599, Validation Accuracy: 0.9351000189781189\n",
      "----------------Epoch 8--------------------\n",
      "Iteration 0, Validation Accuracy: 0.9391999840736389\n",
      "Iteration 100, Validation Accuracy: 0.9394000172615051\n",
      "Iteration 200, Validation Accuracy: 0.9359999895095825\n",
      "Iteration 300, Validation Accuracy: 0.9362999796867371\n",
      "Iteration 400, Validation Accuracy: 0.9394999742507935\n",
      "Iteration 500, Validation Accuracy: 0.941100001335144\n",
      "Iteration 599, Validation Accuracy: 0.940500020980835\n",
      "----------------Epoch 9--------------------\n",
      "Iteration 0, Validation Accuracy: 0.9433000087738037\n",
      "Iteration 100, Validation Accuracy: 0.9431999921798706\n",
      "Iteration 200, Validation Accuracy: 0.9398000240325928\n",
      "Iteration 300, Validation Accuracy: 0.9401999711990356\n",
      "Iteration 400, Validation Accuracy: 0.9435999989509583\n",
      "Iteration 500, Validation Accuracy: 0.9449999928474426\n",
      "Iteration 599, Validation Accuracy: 0.9447000026702881\n",
      "----------------Epoch 10--------------------\n",
      "Iteration 0, Validation Accuracy: 0.9465000033378601\n",
      "Iteration 100, Validation Accuracy: 0.9462000131607056\n",
      "Iteration 200, Validation Accuracy: 0.9424999952316284\n",
      "Iteration 300, Validation Accuracy: 0.944100022315979\n",
      "Iteration 400, Validation Accuracy: 0.9473000168800354\n",
      "Iteration 500, Validation Accuracy: 0.9484000205993652\n",
      "Iteration 599, Validation Accuracy: 0.947700023651123\n"
     ]
    }
   ],
   "source": [
    "#Q3(a). TF Core Implementation\n",
    "# filter size 5*5, 2*2 pooling with stride 2, no dropout\n",
    "num_epochs = 10\n",
    "batch_size = 100\n",
    "n_classes = 10\n",
    "# TF Graph Input\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "def maxpool2d(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "\n",
    "weights = {\n",
    "    # First 5*5 convolution, 1 input image, 32 outputs\n",
    "    'W_conv1': tf.Variable(tf.truncated_normal([5, 5, 1, 32], stddev=0.1)),\n",
    "    # Second 5*5 convolution, 32 inputs, 64 outputs\n",
    "    'W_conv2': tf.Variable(tf.truncated_normal([5, 5, 32, 64], stddev=0.1)),\n",
    "    # First fully connected, 7*7*64 inputs, 1024 outputs\n",
    "    'W_fc': tf.Variable(tf.truncated_normal([7*7*64, 1024], stddev=0.1)),\n",
    "    # Output fully connected, 1024 inputs, 10 outputs\n",
    "    'out': tf.Variable(tf.truncated_normal([1024, n_classes], stddev=0.1))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'b_conv1': tf.Variable(tf.random_normal([32])),\n",
    "    'b_conv2': tf.Variable(tf.random_normal([64])),\n",
    "    'b_fc': tf.Variable(tf.constant(0.0, shape=[1024])),\n",
    "    'out': tf.Variable(tf.constant(0.0, shape=[10]))\n",
    "}\n",
    "\n",
    "# Reshape input to a 4D tensor \n",
    "inputs = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "# Convolution Layer\n",
    "conv1 = tf.nn.relu(conv2d(inputs, weights['W_conv1']) + biases['b_conv1'])\n",
    "pool1 = maxpool2d(conv1)\n",
    "conv2 = tf.nn.relu(conv2d(pool1, weights['W_conv2']) + biases['b_conv2'])\n",
    "pool2 = maxpool2d(conv2)\n",
    "# Fully-connected Layer\n",
    "fc = tf.reshape(pool2, [-1, 7*7*64])\n",
    "fc = tf.nn.relu(tf.matmul(fc, weights['W_fc']) + biases['b_fc'])\n",
    "predictions = tf.matmul(fc, weights['out']) + biases['out']\n",
    "\n",
    "  \n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=predictions))\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.001).minimize(cross_entropy) \n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print('----------------Epoch {}--------------------' .format(epoch+1))\n",
    "    for i in range(int(num_train/batch_size)):\n",
    "        batch_xs = x_train[i*100 : i*100+batch_size, :]\n",
    "        batch_ys = y_train[i*100 : i*100+batch_size, :]\n",
    "        sess.run(optimizer, feed_dict={x: batch_xs, y: batch_ys})\n",
    "        if i % 100 == 0 or i == 599:\n",
    "          correct_prediction = tf.equal(tf.argmax(tf.nn.softmax(predictions), 1), tf.argmax(y, 1))\n",
    "          accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "          print('Iteration {}, Validation Accuracy: {}' .format(i, sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels}) ))\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1377
    },
    "colab_type": "code",
    "id": "R-BFn8qZM2iC",
    "outputId": "b7b50fae-bde2-4c2f-8a12-73485a8fb5ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------Epoch 1--------------------\n",
      "Iteration 0, Validation Accuracy: 0.07900000363588333\n",
      "Iteration 100, Validation Accuracy: 0.45210000872612\n",
      "Iteration 200, Validation Accuracy: 0.6517000198364258\n",
      "Iteration 300, Validation Accuracy: 0.7516000270843506\n",
      "Iteration 400, Validation Accuracy: 0.7817000150680542\n",
      "Iteration 500, Validation Accuracy: 0.8209999799728394\n",
      "Iteration 599, Validation Accuracy: 0.8327999711036682\n",
      "----------------Epoch 2--------------------\n",
      "Iteration 0, Validation Accuracy: 0.8422999978065491\n",
      "Iteration 100, Validation Accuracy: 0.8568999767303467\n",
      "Iteration 200, Validation Accuracy: 0.866100013256073\n",
      "Iteration 300, Validation Accuracy: 0.8787000179290771\n",
      "Iteration 400, Validation Accuracy: 0.8847000002861023\n",
      "Iteration 500, Validation Accuracy: 0.8895000219345093\n",
      "Iteration 599, Validation Accuracy: 0.88919997215271\n",
      "----------------Epoch 3--------------------\n",
      "Iteration 0, Validation Accuracy: 0.8939999938011169\n",
      "Iteration 100, Validation Accuracy: 0.9007999897003174\n",
      "Iteration 200, Validation Accuracy: 0.9028000235557556\n",
      "Iteration 300, Validation Accuracy: 0.904699981212616\n",
      "Iteration 400, Validation Accuracy: 0.9075999855995178\n",
      "Iteration 500, Validation Accuracy: 0.9110999703407288\n",
      "Iteration 599, Validation Accuracy: 0.9135000109672546\n",
      "----------------Epoch 4--------------------\n",
      "Iteration 0, Validation Accuracy: 0.9126999974250793\n",
      "Iteration 100, Validation Accuracy: 0.9182000160217285\n",
      "Iteration 200, Validation Accuracy: 0.9192000031471252\n",
      "Iteration 300, Validation Accuracy: 0.917900025844574\n",
      "Iteration 400, Validation Accuracy: 0.9226999878883362\n",
      "Iteration 500, Validation Accuracy: 0.9248999953269958\n",
      "Iteration 599, Validation Accuracy: 0.9279999732971191\n",
      "----------------Epoch 5--------------------\n",
      "Iteration 0, Validation Accuracy: 0.928600013256073\n",
      "Iteration 100, Validation Accuracy: 0.9286999702453613\n",
      "Iteration 200, Validation Accuracy: 0.9297000169754028\n",
      "Iteration 300, Validation Accuracy: 0.9305999875068665\n",
      "Iteration 400, Validation Accuracy: 0.9336000084877014\n",
      "Iteration 500, Validation Accuracy: 0.9336000084877014\n",
      "Iteration 599, Validation Accuracy: 0.9348999857902527\n",
      "----------------Epoch 6--------------------\n",
      "Iteration 0, Validation Accuracy: 0.9351999759674072\n",
      "Iteration 100, Validation Accuracy: 0.9363999962806702\n",
      "Iteration 200, Validation Accuracy: 0.9366999864578247\n",
      "Iteration 300, Validation Accuracy: 0.9368000030517578\n",
      "Iteration 400, Validation Accuracy: 0.9373000264167786\n",
      "Iteration 500, Validation Accuracy: 0.9384999871253967\n",
      "Iteration 599, Validation Accuracy: 0.9404000043869019\n",
      "----------------Epoch 7--------------------\n",
      "Iteration 0, Validation Accuracy: 0.939300000667572\n",
      "Iteration 100, Validation Accuracy: 0.940500020980835\n",
      "Iteration 200, Validation Accuracy: 0.9409000277519226\n",
      "Iteration 300, Validation Accuracy: 0.9438999891281128\n",
      "Iteration 400, Validation Accuracy: 0.9430000185966492\n",
      "Iteration 500, Validation Accuracy: 0.9420999884605408\n",
      "Iteration 599, Validation Accuracy: 0.9451000094413757\n",
      "----------------Epoch 8--------------------\n",
      "Iteration 0, Validation Accuracy: 0.9440000057220459\n",
      "Iteration 100, Validation Accuracy: 0.9455999732017517\n",
      "Iteration 200, Validation Accuracy: 0.9452999830245972\n",
      "Iteration 300, Validation Accuracy: 0.948199987411499\n",
      "Iteration 400, Validation Accuracy: 0.9463000297546387\n",
      "Iteration 500, Validation Accuracy: 0.9466999769210815\n",
      "Iteration 599, Validation Accuracy: 0.9473000168800354\n",
      "----------------Epoch 9--------------------\n",
      "Iteration 0, Validation Accuracy: 0.9455999732017517\n",
      "Iteration 100, Validation Accuracy: 0.9480999708175659\n",
      "Iteration 200, Validation Accuracy: 0.9476000070571899\n",
      "Iteration 300, Validation Accuracy: 0.9498999714851379\n",
      "Iteration 400, Validation Accuracy: 0.9501000046730042\n",
      "Iteration 500, Validation Accuracy: 0.9502000212669373\n",
      "Iteration 599, Validation Accuracy: 0.9505000114440918\n",
      "----------------Epoch 10--------------------\n",
      "Iteration 0, Validation Accuracy: 0.9495999813079834\n",
      "Iteration 100, Validation Accuracy: 0.9505000114440918\n",
      "Iteration 200, Validation Accuracy: 0.9524999856948853\n",
      "Iteration 300, Validation Accuracy: 0.9527000188827515\n",
      "Iteration 400, Validation Accuracy: 0.9520000219345093\n",
      "Iteration 500, Validation Accuracy: 0.9526000022888184\n",
      "Iteration 599, Validation Accuracy: 0.953499972820282\n"
     ]
    }
   ],
   "source": [
    "#Q3(b). TF Core Implementation\n",
    "# filter size 5*5, 2*2 pooling with stride 2, dropout rate 0.1\n",
    "num_epochs = 10\n",
    "batch_size = 100\n",
    "n_classes = 10\n",
    "# TF Graph Input\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "def maxpool2d(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "\n",
    "weights = {\n",
    "    # First 5*5 convolution, 1 input image, 32 outputs\n",
    "    'W_conv1': tf.Variable(tf.truncated_normal([5, 5, 1, 32], stddev=0.1)),\n",
    "    # Second 5*5 convolution, 32 inputs, 64 outputs\n",
    "    'W_conv2': tf.Variable(tf.truncated_normal([5, 5, 32, 64], stddev=0.1)),\n",
    "    # First fully connected, 7*7*64 inputs, 1024 outputs\n",
    "    'W_fc': tf.Variable(tf.truncated_normal([7*7*64, 1024], stddev=0.1)),\n",
    "    # Output fully connected, 1024 inputs, 10 outputs\n",
    "    'out': tf.Variable(tf.truncated_normal([1024, n_classes], stddev=0.1))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'b_conv1': tf.Variable(tf.random_normal([32])),\n",
    "    'b_conv2': tf.Variable(tf.random_normal([64])),\n",
    "    'b_fc': tf.Variable(tf.constant(0.0, shape=[1024])),\n",
    "    'out': tf.Variable(tf.constant(0.0, shape=[10]))\n",
    "}\n",
    "\n",
    "# Reshape input to a 4D tensor \n",
    "inputs = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "# Convolution Layer\n",
    "conv1 = tf.nn.relu(conv2d(inputs, weights['W_conv1']) + biases['b_conv1'])\n",
    "pool1 = maxpool2d(conv1)\n",
    "conv2 = tf.nn.relu(conv2d(pool1, weights['W_conv2']) + biases['b_conv2'])\n",
    "pool2 = maxpool2d(conv2)\n",
    "# Fully-connected Layer\n",
    "fc = tf.reshape(pool2, [-1, 7*7*64])\n",
    "fc = tf.nn.relu(tf.matmul(fc, weights['W_fc']) + biases['b_fc'])\n",
    "fc_drop = tf.nn.dropout(fc, keep_prob)\n",
    "predictions = tf.matmul(fc_drop, weights['out']) + biases['out']\n",
    "\n",
    "  \n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=predictions))\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.001).minimize(cross_entropy) \n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print('----------------Epoch {}--------------------' .format(epoch+1))\n",
    "    for i in range(int(num_train/batch_size)):\n",
    "        batch_xs = x_train[i*100 : i*100+batch_size, :]\n",
    "        batch_ys = y_train[i*100 : i*100+batch_size, :]\n",
    "        sess.run(optimizer, feed_dict={x: batch_xs, y: batch_ys, keep_prob: 0.9})\n",
    "        if i % 100 == 0 or i == 599:\n",
    "          correct_prediction = tf.equal(tf.argmax(tf.nn.softmax(predictions), 1), tf.argmax(y, 1))\n",
    "          accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "          print('Iteration {}, Validation Accuracy: {}' .format(i, sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels, keep_prob: 1}) ))\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uDOauQNaTWOP"
   },
   "source": [
    "**Problem 4: Performance comparison (24 points)**\n",
    "\n",
    "a) What is the validation accuracy of the CNN with and without pooling?\n",
    "\n",
    "b) Did you observe any performance improvements after adding dropout?\n",
    "\n",
    "c) How does the CNN model compare, in terms of performance, to the multi-class logistic\n",
    "regression and multi-class MLP from HW2?\n",
    "\n",
    "d) How does the number of trainable parameters in the CNN models compare to that of the\n",
    "multi-class logistic regression and multi-class MLP from HW2?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H45_z-_vQkFv"
   },
   "source": [
    "**Q4 Answers:**\n",
    "\n",
    "The CNNs I built have the following parameters: \n",
    "\n",
    "*   Training set contains 60,000 images that are originally from both the training set and the validation set. And validation set contains 10,000 images originally from the test set.\n",
    "*   Convolutional filter size = 5*5, stride = 1\n",
    "*   Padding is set to 'SAME' for all convolutional layers\n",
    "*   Batch size = 100, number of epochs = 10. So there are 600 iterations for each epoch.\n",
    "*   Stochastic gradient descent with learning rate of 0.001 is used as the optimizer. And cross-entropy is used as loss function.\n",
    "*   Weights are generated using truncated normal distribution with stdev=0.1. The biases of filters are generated using random normal distribution, and the biases of fully connected layers are initialized to 0.\n",
    "\n",
    "**(a). **\n",
    "The validation accuracy of the vanilla CNN that consists of 2 convolutional layers followed by 2 fully connected layers is 94.55%, as shown before. The result is decent given the fact that the CNN is very simple, and the model was only trained for a short time. And the accuracy improves a little bit to 94.77% after pooling layers were added. What's more, I recorded the time it took to complete the learning process, and the CNN without pooling layers took about 353 seconds, while the CNN with pooling layers only took about 89 seconds to get the output results.\n",
    "\n",
    "**(b)**\n",
    "Indeed,  the accuracy improves to 95.35% after adding the dropout, and the training time is almost the same as the one without dropout.\n",
    "\n",
    "**(c)**\n",
    "The CNN models are definitely better than the logistic regression models from previous HW. But actually, my most complex multi-class MLP performs slightly better than the CNN models by about 1%. There are several factors that might cause this. For example, the learning rate of the MLP was insensely explored and the best value is chosen and applied to the MLP, while the learning rate of 0.001 might not be the optimal value for CNNs. Also, the MLP has more layers than the CNNs, and given the fact that MNIST dataset is not extremely complex, MLPs are also able to perform well.\n",
    "\n",
    "**(d)**\n",
    "The trainable parameters in the CNN models are definitely much more than that of the multi-class MLPs or logistic regression classifiers. CNNs are computationally expensive. They were trained using GPUs and took much longer than the training process of MLPs using CPUs. But we expect that such additional costs would pay off when large-scale computation or networks are trained using CNNs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "heIZvyawfTDn"
   },
   "source": [
    "**Problem 5: Bookkeeping (4 points)**\n",
    "\n",
    "**(a)** About 10 hours. It took some time to get familiar with DCC, and I was actually having some trouble requesting a GPU on DCC (I did manage to run programs on DCC a few weeks ago), so I completed the homework on Google Colab.\n",
    "\n",
    "**(b)** I adhered to the Duke Community Standard in the completion of this assignment"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "hw3_yl506.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
