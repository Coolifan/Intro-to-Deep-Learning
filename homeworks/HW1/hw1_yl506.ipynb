{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Deep Learning, HW 1 Template\n",
    "This document gives a suggested outline for the coding assignment.  Please see the assignment pdf for a more complete description of the assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Duke Community Standard](http://integrity.duke.edu/standard.html): By typing your name below, you are certifying that you have adhered to the Duke Community Standard in completing this assignment.**\n",
    "\n",
    "Name: Yifan Li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Algorithmic Implementation of a Multi-Class Logistic Regression without Tensorflow (30 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Succintly, for this problem we have an input image that we have vectorized to have $p=784$ features, and the output space is $C=10$ dimensional.  To get the full details on logistic regression, please visit the example codes and and the lectures.  Succinctly, the multiclass logistic regression model is as follows:\n",
    "\n",
    "$$\\mathbf{\\gamma}=\\mathbf{W}\\mathbf{x}+\\mathbf{b},\\quad \\mathbf{W}\\in\\mathbb{R}^{C\\times p},\\quad \\mathbf{b}\\in\\mathbb{R}^{C}$$\n",
    "$$ p(y=j)=\\text{softmax}(\\mathbf{\\gamma})_j$$\n",
    "$$\\ell(y,\\gamma)=\\sum_{j=1}^C1_{(y=j)}\\log(\\text{softmax}(\\mathbf{\\gamma})_j)$$\n",
    "or, if $\\mathbf{r}$ is a one-hot encoding of $y$, then\n",
    "$$\\ell(r,\\gamma)=\\mathbf{r}\\cdot \\log(\\text{softmax}(\\mathbf{\\gamma}))$$\n",
    "We want to implement this model in more basic codes and learn it to build a better understanding of what's going on before moving to using deep learning toolkits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In is permissable in the context of this problem to use the MNIST dataset and minibatcher from Tensorflow, which should reduce the amount of bespoke coding that you have to do.\n",
    "\n",
    "Note that this function is depreciated, but it will work for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-69c65344baec>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the data to make sure that its understood:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training image data:  (55000, 784)\n",
      "Testing image data:  (10000, 784)\n",
      "28 x 28 =  784\n",
      "\n",
      "Train image 1 is labelled one-hot as [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1c23efec50>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAADhNJREFUeJzt3V2MVPUZx/HfU9Eb9EJZBKKwWGOw\n1Qslq2kiEo0BoTEBLjS+xNC0ssZoUrQXxZeoCYKmKRa4QddIxER8CbCVGKwa0yBNGsKbUWRBjaFA\nISyIiRovjO7Tiz00K+75n2HmzJxZnu8nMTszz5yZp9P9cWb2mXP+5u4CEM8vqm4AQDUIPxAU4QeC\nIvxAUIQfCIrwA0ERfiAowg8ERfiBoEa18snMjK8TAk3m7lbL/Rra85vZLDPbZ2afm9miRh4LQGtZ\nvd/tN7OzJH0qaYakQ5K2SbrD3fcktmHPDzRZK/b810r63N2/cPfvJb0maU4DjweghRoJ/0WSDg65\nfii77SfMrNvMtpvZ9gaeC0DJGvmD33BvLX72tt7deyT1SLztB9pJI3v+Q5ImDrl+saTDjbUDoFUa\nCf82SZeZ2SVmdo6k2yVtLKctAM1W99t+d//BzB6Q9I6ksyStdvdPSusMQFPVPeqr68n4zA80XUu+\n5ANg5CL8QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8ERfiBoAg/\nEBThB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gqLqX6JYkM9sv\n6RtJP0r6wd27ymgKrdPZ2Zms33PPPcn6o48+mqynVoE2Sy8m29fXl6w/9thjyXpvb2+yHl1D4c/c\n6O7HS3gcAC3E234gqEbD75LeNbMdZtZdRkMAWqPRt/3XufthM7tQ0ntmttfdPxh6h+wfBf5hANpM\nQ3t+dz+c/eyX1Cvp2mHu0+PuXfwxEGgvdYffzEab2XknL0uaKWl3WY0BaK5G3vaPk9SbjWtGSVrr\n7v8opSsATWepOWzpT2bWuicLZOzYsbm1hx9+OLntXXfdlayPGTMmWS+a1Tcy5y/63Tx48GCyfs01\n1+TWjh8/c6fT7p5+YTOM+oCgCD8QFOEHgiL8QFCEHwiK8ANBMeobAYoOm128eHFurej/32aP244d\nO5asp3R0dCTrkydPTtb37NmTW7viiivqaWlEYNQHIInwA0ERfiAowg8ERfiBoAg/EBThB4Jizj8C\nbNu2LVmfOnVqbq3ROX9qVi5JN954Y7LeyKGz06ZNS9Y3b96crKf+t48aVcaJq9sTc34ASYQfCIrw\nA0ERfiAowg8ERfiBoAg/EBRz/jZw+eWXJ+tFc/4vv/wyt1Z0PH3RHP7BBx9M1hcuXJisL126NLd2\n4MCB5LZFin53BwYGcmv33Xdfctuenp66emoHzPkBJBF+ICjCDwRF+IGgCD8QFOEHgiL8QFCFc34z\nWy3pFkn97n5ldtsFkl6XNFnSfkm3uftXhU/GnL8uRd8DSM3qG12Kuru7O1lftWpVsp5aJnvnzp3J\nbefNm5esr1u3LllP/W6PHz8+ue1IXsK7zDn/S5JmnXLbIknvu/tlkt7PrgMYQQrD7+4fSDpxys1z\nJK3JLq+RNLfkvgA0Wb2f+ce5+xFJyn5eWF5LAFqh6ScyM7NuSekPjgBart49/1EzmyBJ2c/+vDu6\ne4+7d7l7V53PBaAJ6g3/Rknzs8vzJb1ZTjsAWqUw/Gb2qqR/S5piZofM7A+SnpE0w8w+kzQjuw5g\nBCn8zO/ud+SUbiq5F+TYu3dvZc9ddD6Affv2Jeupcw0UnStg0aL0BLlozYFmfv/hTMA3/ICgCD8Q\nFOEHgiL8QFCEHwiK8ANBnbnrFAcyffr03FrR4cBFo7y+vr5kfcqUKcn61q1bc2tjx45Nblt0uHlR\n77Nnz07Wo2PPDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBMec/A9x55525tQULFiS3LTostoZTuyfr\nqVl+I4fkStLKlSuT9aJTg0fHnh8IivADQRF+ICjCDwRF+IGgCD8QFOEHgmLOf4YrmtNXuf2WLVuS\n2z700EPJOnP8xrDnB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgCuf8ZrZa0i2S+t39yuy2JyUtkHTy\nxOmPuPumZjWJtLVr1+bWOjs7k9t2dHQk60Xn/R89enSynvL4448n68zxm6uWPf9LkmYNc/vf3P2q\n7D+CD4wwheF39w8knWhBLwBaqJHP/A+Y2UdmttrMzi+tIwAtUW/4V0m6VNJVko5IWpZ3RzPrNrPt\nZra9zucC0AR1hd/dj7r7j+4+IOkFSdcm7tvj7l3u3lVvkwDKV1f4zWzCkKvzJO0upx0ArVLLqO9V\nSTdI6jCzQ5KekHSDmV0lySXtl3RvE3sE0ATW6PHap/VkZq17MpSiaM7/1FNPJetz587Nre3atSu5\n7ezZs5P1ovP6R+Xu6QURMnzDDwiK8ANBEX4gKMIPBEX4gaAIPxAUo74apZaaPnbsWG4turfffju3\ndvPNNye3LTp19/Lly+vq6UzHqA9AEuEHgiL8QFCEHwiK8ANBEX4gKMIPBMUS3Znp06cn68uW5Z6p\nTHv37k1ue/fdd9fV05lgyZIlubWZM2cmt50yZUrZ7WAI9vxAUIQfCIrwA0ERfiAowg8ERfiBoAg/\nEFSYOX/qeHxJeu6555L1/v7+3FrkOX7REt3PP/98bs2spsPO0STs+YGgCD8QFOEHgiL8QFCEHwiK\n8ANBEX4gqMI5v5lNlPSypPGSBiT1uPsKM7tA0uuSJkvaL+k2d/+qea02Zt68ecl60bHjmzdvLrOd\nEaNoie7169cn66nXtWjNiKLzJKAxtez5f5D0J3f/laTfSLrfzH4taZGk9939MknvZ9cBjBCF4Xf3\nI+6+M7v8jaQ+SRdJmiNpTXa3NZLmNqtJAOU7rc/8ZjZZ0tWStkoa5+5HpMF/ICRdWHZzAJqn5u/2\nm9m5ktZLWujuX9f6vWwz65bUXV97AJqlpj2/mZ2tweC/4u4bspuPmtmErD5B0rBHvrh7j7t3uXtX\nGQ0DKEdh+G1wF/+ipD53f3ZIaaOk+dnl+ZLeLL89AM1SuES3mU2TtEXSxxoc9UnSIxr83P+GpEmS\nDki61d1PFDxWZUt0F42s+vr6kvU9e/bk1p5++umGHnvHjh3JepHOzs7c2vXXX5/ctmgEOndu+u+4\nRR//Ur9fK1asSG5btEQ3hlfrEt2Fn/nd/V+S8h7sptNpCkD74Bt+QFCEHwiK8ANBEX4gKMIPBEX4\ngaAK5/ylPlmFc/4i69atS9ZT8+5GZt2StGvXrmS9yKRJk3JrY8aMSW7baO9F26eW6F65cmVy2+PH\njyfrGF6tc372/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QFHP+TNES3ps2bcqtdXWlT1I0MDCQrDdz\n1l607XfffZesF50+e+nSpcl6b29vso7yMecHkET4gaAIPxAU4QeCIvxAUIQfCIrwA0Ex569RR0dH\nbm3x4sUNPXZ3d3o1sw0bNiTrjRz3XnTufJbJHnmY8wNIIvxAUIQfCIrwA0ERfiAowg8ERfiBoArn\n/GY2UdLLksZLGpDU4+4rzOxJSQskHcvu+oi75x/0rpE95wdGilrn/LWEf4KkCe6+08zOk7RD0lxJ\nt0n61t3/WmtThB9ovlrDP6qGBzoi6Uh2+Rsz65N0UWPtAajaaX3mN7PJkq6WtDW76QEz+8jMVpvZ\n+TnbdJvZdjPb3lCnAEpV83f7zexcSZslLXH3DWY2TtJxSS5psQY/Gvy+4DF42w80WWmf+SXJzM6W\n9Jakd9z92WHqkyW95e5XFjwO4QearLQDe2zw1LAvSuobGvzsD4EnzZO0+3SbBFCdWv7aP03SFkkf\na3DUJ0mPSLpD0lUafNu/X9K92R8HU4/Fnh9oslLf9peF8APNx/H8AJIIPxAU4QeCIvxAUIQfCIrw\nA0ERfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IivADQRWewLNkxyX9Z8j1juy2dtSuvbVrXxK91avM\n3jprvWNLj+f/2ZObbXf3rsoaSGjX3tq1L4ne6lVVb7ztB4Ii/EBQVYe/p+LnT2nX3tq1L4ne6lVJ\nb5V+5gdQnar3/AAqUkn4zWyWme0zs8/NbFEVPeQxs/1m9rGZfVj1EmPZMmj9ZrZ7yG0XmNl7ZvZZ\n9nPYZdIq6u1JM/tv9tp9aGa/rai3iWb2TzPrM7NPzOyP2e2VvnaJvip53Vr+tt/MzpL0qaQZkg5J\n2ibpDnff09JGcpjZfkld7l75TNjMpkv6VtLLJ1dDMrO/SDrh7s9k/3Ce7+5/bpPentRprtzcpN7y\nVpb+nSp87cpc8boMVez5r5X0ubt/4e7fS3pN0pwK+mh77v6BpBOn3DxH0prs8hoN/vK0XE5vbcHd\nj7j7zuzyN5JOrixd6WuX6KsSVYT/IkkHh1w/pPZa8tslvWtmO8ysu+pmhjHu5MpI2c8LK+7nVIUr\nN7fSKStLt81rV8+K12WrIvzDrSbSTiOH69x9qqTZku7P3t6iNqskXarBZdyOSFpWZTPZytLrJS10\n96+r7GWoYfqq5HWrIvyHJE0ccv1iSYcr6GNY7n44+9kvqVeDH1PaydGTi6RmP/sr7uf/3P2ou//o\n7gOSXlCFr122svR6Sa+4+4bs5spfu+H6qup1qyL82yRdZmaXmNk5km6XtLGCPn7GzEZnf4iRmY2W\nNFPtt/rwRknzs8vzJb1ZYS8/0S4rN+etLK2KX7t2W/G6ki/5ZKOM5ZLOkrTa3Ze0vIlhmNkvNbi3\nlwaPeFxbZW9m9qqkGzR41NdRSU9I+rukNyRNknRA0q3u3vI/vOX0doNOc+XmJvWWt7L0VlX42pW5\n4nUp/fANPyAmvuEHBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiCo/wGTnJDl40xJsQAAAABJRU5E\nrkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Dataset statistics# Datas \n",
    "print('Training image data: ', mnist.train.images.shape)\n",
    "print('Testing image data: ', mnist.test.images.shape)\n",
    "print('28 x 28 = ', 28*28)\n",
    "\n",
    "# Example image\n",
    "print('\\nTrain image 1 is labelled one-hot as {0}'.format(mnist.train.labels[1,:]))\n",
    "image = np.reshape(mnist.train.images[1,:],[28,28])\n",
    "plt.imshow(image, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can pull a new data example from MNIST by the following:\n",
    "\n",
    "Note that the digit will change each time you run this because it is randomly sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train image 1 is labelled one-hot as [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1c2a6e5828>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n8ANBEX4gKMIPBPUPebtniOwqm1cAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "current_data=mnist.train.next_batch(1)\n",
    "# Example image\n",
    "print('\\nTrain image 1 is labelled one-hot as {0}'.format(current_data[1]))\n",
    "image = np.reshape(current_data[0],[28,28])\n",
    "plt.imshow(image, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the functions that you need to define to make this work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lr_gradient(current_parameters, batch_xs, batch_ys):\n",
    "    # calculate the gradient on the data\n",
    "    \n",
    "    # Forward propagation\n",
    "    z = batch_xs.dot(current_parameters[0]) + current_parameters[1] # z = Wx+b\n",
    "    probs = softmax(z)\n",
    "    # Backpropagation\n",
    "    delta = probs - batch_ys\n",
    "    W_grad = (batch_xs.T).dot(delta)\n",
    "    b_grad = np.sum(delta, axis=0, keepdims=True)\n",
    "    \n",
    "    return W_grad, b_grad\n",
    "\n",
    "\n",
    "def softmax(z):\n",
    "    exp = np.exp(z-np.max(z, axis=1).reshape((-1,1)))\n",
    "    norms = np.sum(exp, axis=1).reshape((-1,1))\n",
    "    return exp / norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lr_loss(current_parameters,batch_xs, batch_ys):\n",
    "    # calculate the loss\n",
    "    \n",
    "    z = batch_xs.dot(current_parameters[0]) + current_parameters[1] # z = Wx+b\n",
    "    probs = softmax(z)\n",
    "    avg_loss = - np.sum(batch_ys * np.log(probs))/100\n",
    "    \n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the stochastic gradient descent optimization loop.  Note that you need to fill in the values to make this work well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_iterations = 10000 # choose the max number of iterations\n",
    "step_size = 0.05 # choose your step size\n",
    "W = np.zeros((784, 10)) # choose your starting parameters (connection weights)\n",
    "b = np.zeros(10) # choose your starting parameters (biases)\n",
    "training_loss_history=[]\n",
    "for iter in range(max_iterations):  \n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    current_parameters = [W, b]\n",
    "    # note you need to change this to your preferred data format.\n",
    "    W_grad,b_grad=lr_gradient(current_parameters,batch_xs, batch_ys)\n",
    "    training_loss_history.append(lr_loss(current_parameters,batch_xs, batch_ys))\n",
    "    W=W-step_size*W_grad\n",
    "    b=b-step_size*b_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be a helpful diagnostic tool to visualize the learning curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Average loss vs. iterations')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAiKikGMgICIKOQYCIqKQ+38vdb6zQJbPuQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(training_loss_history)\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Average loss vs. iterations')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate both your training loss and accuracy and your validation loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg training loss: 0.713565, training accuracy: 0.894200\n",
      "Avg validation loss: 0.800189, validation accuracy: 0.892200\n",
      "Avg test loss: 0.849438, test accuracy: 0.886300\n"
     ]
    }
   ],
   "source": [
    "# Training set\n",
    "z_train = mnist.train.images.dot(W) + b # z = Wx+b\n",
    "probs_train = softmax(z_train) # y^ = softmax(z)\n",
    "num_train = mnist.train.images.shape[0] # number of training set data points\n",
    "correct = 0\n",
    "for i in range(0, num_train-1):\n",
    "    if np.argmax(probs_train[i]) == np.argmax(mnist.train.labels[i]):\n",
    "        correct += 1\n",
    "        \n",
    "train_accuracy = correct / num_train\n",
    "avg_loss_train = - np.sum(mnist.train.labels * np.log(probs_train))/num_train\n",
    "print('Avg training loss: %f, training accuracy: %f' %(avg_loss_train, train_accuracy))\n",
    "\n",
    "# Validation set\n",
    "z_validation = mnist.validation.images.dot(W) + b # z = Wx+b\n",
    "probs_validation = softmax(z_validation)\n",
    "num_validation = mnist.validation.images.shape[0]\n",
    "correct = 0\n",
    "for i in range(0, num_validation-1):\n",
    "    if np.argmax(probs_validation[i]) == np.argmax(mnist.validation.labels[i]):\n",
    "        correct += 1\n",
    "        \n",
    "validation_accuracy = correct / num_validation\n",
    "avg_loss_validation = - np.sum(mnist.validation.labels * np.log(probs_validation))/num_validation\n",
    "print('Avg validation loss: %f, validation accuracy: %f' %(avg_loss_validation, validation_accuracy))\n",
    "\n",
    "\n",
    "# Test set\n",
    "z_test = mnist.test.images.dot(W) + b\n",
    "probs_test = softmax(z_test)\n",
    "num_test = mnist.test.images.shape[0]\n",
    "correct = 0\n",
    "for i in range(0, num_test-1):\n",
    "    if np.argmax(probs_test[i]) == np.argmax(mnist.test.labels[i]):\n",
    "        correct += 1\n",
    "        \n",
    "test_accuracy = correct / num_test\n",
    "avg_loss_test = - np.sum(mnist.test.labels * np.log(probs_test))/num_test\n",
    "print('Avg test loss: %f, test accuracy: %f' %(avg_loss_test, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Algorithmic Implementation of a Multi-Class Logistic Regression with Tensorflow (30 Points)\n",
    "As above, but now you are allowed to use tensorflow to perform model learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a)\tSet up a logistic regression network, and learn it on MNIST using stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Modeling Definition\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# Import data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "# Create the model\n",
    "W = tf.Variable(tf.zeros([784,10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y_ = tf.placeholder(tf.float32, [None, 10]) #true labels\n",
    "y = tf.matmul(x, W) + b\n",
    "# Define loss and optimizer\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_, logits=y))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.05).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0: validation accuracy: 0.3508000075817108\n",
      "Iter: 1000: validation accuracy: 0.8999999761581421\n",
      "Iter: 2000: validation accuracy: 0.9120000004768372\n",
      "Iter: 3000: validation accuracy: 0.9154000282287598\n",
      "Iter: 4000: validation accuracy: 0.9175999760627747\n",
      "Iter: 5000: validation accuracy: 0.9204000234603882\n",
      "Iter: 6000: validation accuracy: 0.9228000044822693\n",
      "Iter: 7000: validation accuracy: 0.9240000247955322\n",
      "Iter: 8000: validation accuracy: 0.9223999977111816\n",
      "Iter: 9000: validation accuracy: 0.9246000051498413\n",
      "Iter: 9999: validation accuracy: 0.9254000186920166\n"
     ]
    }
   ],
   "source": [
    "# Learning\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(10000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys}) # fill in the placeholders\n",
    "    if i % 1000 == 0 or i == 9999: # Evaluation\n",
    "        correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        print('Iter: {}: validation accuracy: {}'.format(i, sess.run(accuracy, feed_dict={x: mnist.validation.images, y_: mnist.validation.labels})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9225999712944031\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "print('Test accuracy: {0}'.format(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels})))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b)\tSet up an MLP with a single hidden layer (you can choose the number of hidden nodes) and learn it on MNIST using stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Modeling Definition\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# Import data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "# Create the model\n",
    "n_hidden = 256 # number of hidden nodes\n",
    "W1 = tf.Variable(initial_value=tf.truncated_normal([784,n_hidden], stddev=0.1)) #layer 1\n",
    "b1 = tf.Variable(tf.zeros([n_hidden]))\n",
    "W2 = tf.Variable(initial_value=tf.truncated_normal([n_hidden, 10], stddev=0.1)) #layer 2\n",
    "b2 = tf.Variable(tf.zeros([10]))\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y_ = tf.placeholder(tf.float32, [None, 10]) #true labels\n",
    "\n",
    "z = tf.nn.relu(tf.matmul(x, W1) + b1)\n",
    "y = tf.matmul(z, W2) + b2\n",
    "\n",
    "# Define loss and optimizer\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_, logits=y))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.05).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0: validation accuracy: 0.24740000069141388\n",
      "Iter: 1000: validation accuracy: 0.9291999936103821\n",
      "Iter: 2000: validation accuracy: 0.9448000192642212\n",
      "Iter: 3000: validation accuracy: 0.9535999894142151\n",
      "Iter: 4000: validation accuracy: 0.9620000123977661\n",
      "Iter: 5000: validation accuracy: 0.9638000130653381\n",
      "Iter: 6000: validation accuracy: 0.9679999947547913\n",
      "Iter: 7000: validation accuracy: 0.9688000082969666\n",
      "Iter: 8000: validation accuracy: 0.9711999893188477\n",
      "Iter: 9000: validation accuracy: 0.973800003528595\n",
      "Iter: 9999: validation accuracy: 0.9729999899864197\n"
     ]
    }
   ],
   "source": [
    "# Learning\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(10000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys}) # fill in the placeholders\n",
    "    if i % 1000 == 0 or i == 9999: # Evaluation\n",
    "        correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        print('Iter: {}: validation accuracy: {}'.format(i, sess.run(accuracy, feed_dict={x: mnist.validation.images, y_: mnist.validation.labels})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9736999869346619\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "print('Test accuracy: {0}'.format(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels})))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c)\tSet up an MLP with two hidden layers (i.e. lecture 2, slide 55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Modeling Definition\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# Import data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "# Create the model\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y_ = tf.placeholder(tf.float32, [None, 10]) #true labels\n",
    "\n",
    "n_hidden_1 = 256 # number of hidden nodes in hidden layer 1\n",
    "n_hidden_2 = 64 # number of hidden nodes in hidden layer 2\n",
    "W1 = tf.Variable(initial_value=tf.truncated_normal([784, n_hidden_1], stddev=0.1)) #layer 1\n",
    "b1 = tf.Variable(tf.zeros([n_hidden_1]))\n",
    "z1 = tf.nn.relu(tf.matmul(x, W1) + b1)\n",
    "W2 = tf.Variable(initial_value=tf.truncated_normal([n_hidden_1, n_hidden_2], stddev=0.1)) #layer 2\n",
    "b2 = tf.Variable(tf.zeros([n_hidden_2]))\n",
    "z2 = tf.nn.relu(tf.matmul(z1, W2) + b2)\n",
    "W3 = tf.Variable(initial_value=tf.truncated_normal([n_hidden_2, 10], stddev=0.1)) #layer 3\n",
    "b3 = tf.Variable(tf.zeros([10]))\n",
    "y = tf.matmul(z2, W3) + b3\n",
    "\n",
    "# Define loss and optimizer\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_, logits=y))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.05).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0: validation accuracy: 0.14159999787807465\n",
      "Iter: 1000: validation accuracy: 0.9368000030517578\n",
      "Iter: 2000: validation accuracy: 0.9549999833106995\n",
      "Iter: 3000: validation accuracy: 0.9610000252723694\n",
      "Iter: 4000: validation accuracy: 0.9652000069618225\n",
      "Iter: 5000: validation accuracy: 0.9697999954223633\n",
      "Iter: 6000: validation accuracy: 0.972000002861023\n",
      "Iter: 7000: validation accuracy: 0.9747999906539917\n",
      "Iter: 8000: validation accuracy: 0.9757999777793884\n",
      "Iter: 9000: validation accuracy: 0.9760000109672546\n",
      "Iter: 9999: validation accuracy: 0.9775999784469604\n"
     ]
    }
   ],
   "source": [
    "# Learning\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(10000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys}) # fill in the placeholders\n",
    "    if i % 1000 == 0 or i == 9999: # Evaluation\n",
    "        correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        print('Iter: {}: validation accuracy: {}'.format(i, sess.run(accuracy, feed_dict={x: mnist.validation.images, y_: mnist.validation.labels})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9751999974250793\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "print('Test accuracy: {0}'.format(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels})))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4: Performance Comparison (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a)\tDid your implementations and Tensorflows implementations from problems 2 and 3 perform the same?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Both implementations work, but Tensorflow's implementation gives better accuracy when keeping learning rate and number of iterations the same. My implementation has an test accuracy of about 89%, while Tensorflow's version is about 3 percent higher. Results were shown in previous problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b)\tWhat is the validation accuracy from the multi-class logistic regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The validation accuracy from the multi-class logistic regression is about 92%, as shown in problem 3(a). The accuracy is pretty decent, possibly due to the large number of iterations chosen (10000). The machine is able to learn all parameters quickly, given the simple network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c)\tWhat is the validation accuracy from the multi-class MLP with a single hidden layer?  If you change the number of nodes in the hidden layer, how susceptible is the hold out performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the number of hidden nodes is set to 256, the validation accuracy from the multi-class MLP with a single hidden layer is about 97.2%, as we can see from the test results shown in problem 3(b). We can see an obvious accuracy improvement, but the machine learns and updates all parameters much slower due to the increased complexity of the network. In general, a larger number of hidden nodes yields higher accuracy, but will eventually converge. Larger number of nodes will also increase the complexity of the model and thus increase the computational cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d)\tWhat is the validation accuracy from the multi-class MLP with two hidden layer?  If you change the number of nodes in the hidden layers, how susceptible is the hold out performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the number of hidden nodes is set to 256 and 64 respectively, the validation accuracy from the multi-class MLP with 2 hidden layers is about 97.5%, as we can see from the test results shown in problem 3(c). We can see a further accuracy improvement, but at a cost of even slower learning speed/higher computational cost. This MLP with 2 hidden layers is quite complex, and the accuracy only varies a little bit when changing the number of hidden nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (e)\tDo you match my reported accuracies (lecture 2, slide 58)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results match, and the results that I got were even higher, possibly due to the large number of iterations used. The MLP with 1 hidden layer produces the accuracy of ~96% (I got 97%), and the simple MLP without hidden layers has an accuracy of ~91% (I got 92%)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
