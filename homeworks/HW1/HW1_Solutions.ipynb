{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Deep Learning, HW 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Algorithmic Implementation of a Multi-Class Logistic Regression without TensorFlow (30 Points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Import the MNIST dataset from TensorFlow\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the gradient and loss functions for logistic regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lr_gradient(current_W,current_b,data_minibatch):\n",
    "    minibatch_size = data_minibatch[0].shape[0]\n",
    "    \n",
    "    # calculate the gradient on the data\n",
    "    z = np.matmul(data_minibatch[0], current_W) + current_b\n",
    "    a = np.exp(z) / np.sum(np.exp(z),axis=1).reshape((minibatch_size,1))\n",
    "\n",
    "    dLdz = a - data_minibatch[1]\n",
    "    dzdW = data_minibatch[0]\n",
    "    \n",
    "    W_grad = np.matmul(dzdW.T, dLdz) / minibatch_size\n",
    "    b_grad = np.mean(dLdz, axis=0)\n",
    "    return W_grad, b_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lr_loss(current_W,current_b,minibatch_images, minibatch_labels):\n",
    "    minibatch_size = minibatch_images.shape[0]\n",
    "    # calculate the loss\n",
    "    z = np.matmul(minibatch_images, current_W) + current_b\n",
    "    a = np.exp(z) / np.sum(np.exp(z),axis=1).reshape((minibatch_size,1))\n",
    "    avg_loss = np.mean(-np.sum(np.multiply(minibatch_labels, np.log(a)),axis=1))\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the stochastic gradient descent optimization loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_iterations = 5000 # choose the max number of iterations\n",
    "step_size = 0.1 # step size\n",
    "W = np.random.normal(scale=0.1, size=(784,10)) # starting parameters (connection weights)\n",
    "b = np.zeros(10) # starting parameters (biases)\n",
    "training_loss_history = []\n",
    "for iter in range(0, max_iterations):\n",
    "    data_minibatch = mnist.train.next_batch(100)\n",
    "    W_grad, b_grad = lr_gradient(W, b, data_minibatch)\n",
    "    training_loss_history.append(lr_loss(W,b,data_minibatch[0], data_minibatch[1]))\n",
    "    W = W - step_size * W_grad\n",
    "    b = b - step_size * b_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the learning curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x10385a550>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(training_loss_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating training loss and accuracy and validation loss and accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.28995070285598074\n",
      "Training accuracy: 0.9182727272727272\n",
      "\n",
      "Test loss: 0.28291572542156607\n",
      "Test accuracy: 0.9211\n"
     ]
    }
   ],
   "source": [
    "train_loss = lr_loss(W, b, mnist.train.images, mnist.train.labels)\n",
    "z_train = np.matmul(mnist.train.images, W) + b\n",
    "a_train = np.exp(z_train) / np.sum(np.exp(z_train))\n",
    "train_accuracy = np.mean(np.equal(np.argmax(a_train, 1), np.argmax(mnist.train.labels, 1)))\n",
    "\n",
    "test_loss = lr_loss(W, b, mnist.test.images, mnist.test.labels)\n",
    "z_test = np.matmul(mnist.test.images, W) + b\n",
    "a_test = np.exp(z_test) / np.sum(np.exp(z_test))\n",
    "test_accuracy = np.mean(np.equal(np.argmax(a_test, 1), np.argmax(mnist.test.labels, 1)))\n",
    "\n",
    "print('Training loss: {0}'.format(train_loss))\n",
    "print('Training accuracy: {0}\\n'.format(train_accuracy))\n",
    "\n",
    "print('Test loss: {0}'.format(test_loss))\n",
    "print('Test accuracy: {0}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Algorithmic Implementation of a Multi-Class Logistic Regression with TensorFlow (30 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a)\tSet up a logistic regression network, and learn it on MNIST using stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.2894737422466278\n",
      "Training accuracy: 0.9205999970436096\n",
      "\n",
      "Test loss: 0.2852165400981903\n",
      "Test accuracy: 0.9222000241279602\n"
     ]
    }
   ],
   "source": [
    "# Modeling Definition\n",
    "# Create the model\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "W = tf.Variable(tf.truncated_normal([784, 10], stddev=0.1))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "y = tf.matmul(x, W) + b\n",
    "\n",
    "# Define loss and optimizer\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(cross_entropy)\n",
    "\n",
    "# Learning\n",
    "# Create a Session object, initialize all variables\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Train\n",
    "for iter in range(5000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n",
    "\n",
    "# Evaluation\n",
    "# Test trained model\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "print('Training loss: {0}'.format(sess.run(cross_entropy, feed_dict={x: mnist.train.images, y_:mnist.train.labels})))\n",
    "print('Training accuracy: {0}\\n'.format(sess.run(accuracy, feed_dict={x: mnist.train.images, y_: mnist.train.labels})))\n",
    "print('Test loss: {0}'.format(sess.run(cross_entropy, feed_dict={x: mnist.test.images, y_:mnist.test.labels})))\n",
    "print('Test accuracy: {0}'.format(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels})))\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b)\tSet up an MLP with a single hidden layer (you can choose the number of hidden nodes) and learn it on MNIST using stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 hidden nodes accuracy: 0.9688000082969666\n",
      "300 hidden nodes accuracy: 0.9750000238418579\n",
      "500 hidden nodes accuracy: 0.9765999913215637\n",
      "700 hidden nodes accuracy: 0.9764000177383423\n",
      "\n",
      "Using 500 hidden nodes for final test accuracy.\n",
      "\n",
      "Test loss: 0.0905374214053154\n",
      "Test accuracy: 0.972599983215332\n"
     ]
    }
   ],
   "source": [
    "def train_MLP(num_hidden_nodes, dataset):\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    # Model parameters\n",
    "    W1 = tf.Variable(tf.truncated_normal([784, num_hidden_nodes], stddev=0.1))\n",
    "    b1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "    W2 = tf.Variable(tf.truncated_normal([num_hidden_nodes, 10], stddev=0.1))\n",
    "    b2 = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "    # Placeholders\n",
    "    x = tf.placeholder(tf.float32, [None, 784])\n",
    "    y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "    # MLP\n",
    "    h = tf.nn.relu(tf.matmul(x, W1) + b1)\n",
    "    y = tf.matmul(h, W2) + b2\n",
    "\n",
    "    # Cross-entropy loss and optimizer\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))\n",
    "    train_step = tf.train.GradientDescentOptimizer(0.1).minimize(cross_entropy)\n",
    "    \n",
    "    # Create a Session object, initialize all variables\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        # Train\n",
    "        for iter in range(5000):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "            sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n",
    "        \n",
    "        # Evaluation\n",
    "        correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        valid_loss = sess.run(cross_entropy, feed_dict={x: dataset.images, y_: dataset.labels})\n",
    "        valid_accuracy = sess.run(accuracy, feed_dict={x: dataset.images, y_: dataset.labels})\n",
    "    \n",
    "    return valid_loss, valid_accuracy\n",
    "\n",
    "# Effect of number of hidden nodes\n",
    "hidden_nodes = np.array([100, 300, 500, 700])\n",
    "hidden_accuracy = []\n",
    "\n",
    "for num_nodes in hidden_nodes:\n",
    "    valid_loss, valid_accuracy = train_MLP(num_nodes, mnist.validation)\n",
    "    print('{0} hidden nodes accuracy: {1}'.format(num_nodes, valid_accuracy))\n",
    "    hidden_accuracy.append(valid_accuracy)\n",
    "\n",
    "# Evaluation\n",
    "# Test trained model\n",
    "best_model = hidden_nodes[np.argmax(hidden_accuracy)]\n",
    "print('\\nUsing {0} hidden nodes for final test accuracy.\\n'.format(best_model))\n",
    "test_loss, test_accuracy = train_MLP(best_model, mnist.test)\n",
    "\n",
    "print('Test loss: {0}'.format(test_loss))\n",
    "print('Test accuracy: {0}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c)\tSet up an MLP with two hidden layers (i.e. lecture 2, slide 55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300/100 hidden nodes accuracy: 0.977400004863739\n",
      "500/100 hidden nodes accuracy: 0.9775999784469604\n",
      "700/100 hidden nodes accuracy: 0.9778000116348267\n",
      "500/300 hidden nodes accuracy: 0.977400004863739\n",
      "700/300 hidden nodes accuracy: 0.975600004196167\n",
      "700/500 hidden nodes accuracy: 0.9805999994277954\n",
      "\n",
      "Using 700/500 hidden nodes for final test accuracy.\n",
      "\n",
      "Test loss: 0.07037129998207092\n",
      "Test accuracy: 0.9789000153541565\n"
     ]
    }
   ],
   "source": [
    "def train_MLP(num_hidden_nodes_1, num_hidden_nodes_2, dataset):\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    # Model parameters\n",
    "    W1 = tf.Variable(tf.truncated_normal([784, num_hidden_nodes_1], stddev=0.1))\n",
    "    b1 = tf.Variable(tf.zeros([num_hidden_nodes_1]))\n",
    "    W2 = tf.Variable(tf.truncated_normal([num_hidden_nodes_1, num_hidden_nodes_2], stddev=0.1))\n",
    "    b2 = tf.Variable(tf.zeros([num_hidden_nodes_2]))\n",
    "    W3 = tf.Variable(tf.truncated_normal([num_hidden_nodes_2, 10], stddev=0.1))\n",
    "    b3 = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "    # Placeholders\n",
    "    x = tf.placeholder(tf.float32, [None, 784])\n",
    "    y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "    # MLP\n",
    "    h1 = tf.nn.relu(tf.matmul(x,  W1) + b1)\n",
    "    h2 = tf.nn.relu(tf.matmul(h1, W2) + b2)\n",
    "    y = tf.matmul(h2, W3) + b3\n",
    "\n",
    "    # Cross-entropy loss and optimizer\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))\n",
    "    train_step = tf.train.GradientDescentOptimizer(0.1).minimize(cross_entropy)\n",
    "    \n",
    "    # Create a Session object, initialize all variables\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        # Train\n",
    "        for iter in range(5000):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "            sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n",
    "        \n",
    "        # Evaluation\n",
    "        correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        valid_loss = sess.run(cross_entropy, feed_dict={x: dataset.images, y_: dataset.labels})\n",
    "        valid_accuracy = sess.run(accuracy, feed_dict={x: dataset.images, y_: dataset.labels})\n",
    "    \n",
    "    return valid_loss, valid_accuracy\n",
    "\n",
    "# Effect of number of hidden nodes\n",
    "hidden_nodes_1 = [300, 500, 700, 500, 700, 700]\n",
    "hidden_nodes_2 = [100, 100, 100, 300, 300, 500]\n",
    "hidden_accuracy = []\n",
    "\n",
    "for i in range(len(hidden_nodes_1)):\n",
    "    valid_loss, valid_accuracy = train_MLP(hidden_nodes_1[i], hidden_nodes_2[i], mnist.validation)\n",
    "    print('{0}/{1} hidden nodes accuracy: {2}'.format(hidden_nodes_1[i], hidden_nodes_2[i], valid_accuracy))\n",
    "    hidden_accuracy.append(valid_accuracy)\n",
    "\n",
    "# Evaluation\n",
    "# Test trained model\n",
    "best_model = (hidden_nodes_1[np.argmax(hidden_accuracy)], hidden_nodes_2[np.argmax(hidden_accuracy)])\n",
    "print('\\nUsing {0}/{1} hidden nodes for final test accuracy.\\n'.format(best_model[0], best_model[1]))\n",
    "test_loss, test_accuracy = train_MLP(best_model[0], best_model[1], mnist.test)\n",
    "\n",
    "print('Test loss: {0}'.format(test_loss))\n",
    "print('Test accuracy: {0}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4: Performance Comparison (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a)\tDid your implementations and TensorFlow’s implementations from problems 2 and 3 perform the same?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes. As shown above in the printed validation accuracies, both the manual implementation of logistic regression without TensorFlow and the implementation with TensorFlow achieved accuracies of approximately 92% (respectively, accuracies of 92.1% and 92.2%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b)\tWhat is the validation accuracy from the multi-class logistic regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, as shown above, the multi-class logistic regression achieved a classification accuracy of approximately 92%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c)\tWhat is the validation accuracy from the multi-class MLP with a single hidden layer?  If you change the number of nodes in the hidden layer, how susceptible is the hold out performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following numbers of hidden nodes were tested: 100, 300, 500, and 700. As shown above, increasing the number of nodes from 100 to 300 to 500 caused the performance to improve slightly (from 96.9% to 97.5% to 97.7%), while 500 and 700 hidden nodes had similar performances.\n",
    "\n",
    "Ultimately, with 500 hidden nodes, the test accuracy was 97.3%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d)\tWhat is the validation accuracy from the multi-class MLP with two hidden layers?  If you change the number of nodes in the hidden layers, how susceptible is the hold out performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following combinations of numbers of hidden nodes were tested:\n",
    "1. 300 in the first layer and 100 in the second layer\n",
    "2. 500 in the first layer and 100 in the second layer\n",
    "3. 700 in the first layer and 100 in the second layer\n",
    "4. 500 in the first layer and 300 in the second layer\n",
    "5. 700 in the first layer and 300 in the second layer\n",
    "6. 700 in the first layer and 500 in the second layer\n",
    "\n",
    "As shown above, varying the number of nodes did not significantly impact the hold out performance, though the best-performing combination was 700 nodes in first hidden layer and 500 nodes inthe second hidden layer (validation accuracy of 98.1%).\n",
    "\n",
    "Ultimately, with this combination, the accuracy on the final test dataset was 97.9%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (e)\tDo you match my reported accuracies (lecture 2, slide 58)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes. Logistic regression was reported to have an accuracy of approximately 91%, and I found an accuracy of 92%. A single-hidden-layer MLP was reported to have an accuracy of approximately 96%, and I found an accuracy of 97.3%. The slight discrepancies are just due to randomness in the trained model; no bias is expected since the test dataset was evaluated only a single time after all training/validation was complete."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
