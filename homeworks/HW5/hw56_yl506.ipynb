{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw1_template.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "aRY78dD67B93",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Introduction to Deep Learning, HW 5+6\n",
        "\n",
        "**Name: Yifan Li**\n",
        "\n",
        "**NetID: yl506**"
      ]
    },
    {
      "metadata": {
        "id": "UtkODo2y7B94",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**[Duke Community Standard](http://integrity.duke.edu/standard.html): By typing your name below, you are certifying that you have adhered to the Duke Community Standard in completing this assignment.**\n",
        "\n",
        "Name:  Yifan Li"
      ]
    },
    {
      "metadata": {
        "id": "plwREGKS7B98",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Problem 2: Recurrent Neural Networks (30 points)"
      ]
    },
    {
      "metadata": {
        "id": "b68bAV9B7B-A",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Here we will work on several problems in recurrent neural networks and NLP. First, you should\n",
        "just play with word embeddings to get a greater understanding of what they are and how they\n",
        "work (and when normalized and unnormalized vectors are better).**\n",
        "\n",
        "**We would like to evaluate whether the word embeddings are helping us on our sentiment\n",
        "analysis task. On one of the movie review datasets, do the following:**\n"
      ]
    },
    {
      "metadata": {
        "id": "G5n6HoGr87f-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### (i) Train an MLP off of the average word embedding to predict sentiment (as done in class) but optimize the network settings to maximize performance."
      ]
    },
    {
      "metadata": {
        "id": "HXYP4CGO9T4s",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Word embeddings, or word vectors, provide a way of mapping words from a vocabulary into a low-dimensional space, where words with similar meanings are close together. Let's play around with a set of pre-trained word vectors, to get used to their properties. There exist many sets of pretrained word embeddings; here, we use ConceptNet Numberbatch, which provides a relatively small download in an easy-to-work-with format (h5)."
      ]
    },
    {
      "metadata": {
        "id": "9ciLchWR7B-B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Download word vectors\n",
        "from urllib.request import urlretrieve\n",
        "import os\n",
        "if not os.path.isfile('mini.h5'):\n",
        "    print(\"Downloading Conceptnet Numberbatch word embeddings...\")\n",
        "    conceptnet_url = 'http://conceptnet.s3.amazonaws.com/precomputed-data/2016/numberbatch/17.06/mini.h5'\n",
        "    urlretrieve(conceptnet_url, 'mini.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9jZiP6hz7B-E",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To read an `h5` file, we'll need to use the `h5py` package. Below, we use the package to open the `mini.h5` file we just downloaded. We extract from the file a list of utf-8-encoded words, as well as their $300$-dimensional vectors."
      ]
    },
    {
      "metadata": {
        "id": "tXo42Tq77B-E",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import h5py\n",
        "with h5py.File('mini.h5', 'r') as f:\n",
        "    all_words = [word.decode('utf-8') for word in f['mat']['axis1'][:]]\n",
        "    all_embeddings = f['mat']['block0_values'][:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5HoK4Fth7B-H",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " Now,  `all_words` is a list of $V$ strings (what we call our *vocabulary*), and `all_embeddings` is a $V \\times 300$ matrix. The strings are of the form `/c/language_code/word` — for example, `/c/en/cat` and `/c/es/gato`.\n",
        " \n",
        "We are interested only in the English words. We use Python list comprehensions to pull out the indices of the English words, then extract just the English words (stripping the six-character `/c/en/` prefix) and their embeddings."
      ]
    },
    {
      "metadata": {
        "id": "mnlzq1WE7B-H",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "english_words = [word[6:] for word in all_words if word.startswith('/c/en/')]\n",
        "english_word_indices = [i for i, word in enumerate(all_words) if word.startswith('/c/en/')]\n",
        "english_embedddings = all_embeddings[english_word_indices]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-8lk9WQ97B-N",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The magnitude of a word vector is less important than its direction; the magnitude can be thought of as representing frequency of use, independent of the semantics of the word. \n",
        "Here, we will be interested in semantics, so we normalize our vectors, dividing each by its length. \n",
        "The result is that all of our word vectors are length 1, and as such, lie on a unit circle. \n",
        "The dot product of two vectors is proportional to the cosine of the angle between them, and provides a measure of similarity (the bigger the cosine, the smaller the angle)."
      ]
    },
    {
      "metadata": {
        "id": "KfA62QV17B-O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "norms = np.linalg.norm(english_embedddings, axis=1)\n",
        "normalized_embeddings = english_embedddings.astype('float32') / norms.astype('float32').reshape([-1, 1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "f8bXxKOC7B-R",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We want to look up words easily, so we create a dictionary that maps us from a word to its index in the word embeddings matrix."
      ]
    },
    {
      "metadata": {
        "id": "zR5Zmb2t7B-S",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "index = {word: i for i, word in enumerate(english_words)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R3Q6BfHH7B-V",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " Now we are ready to measure the similarity between pairs of words. We use numpy to take dot products."
      ]
    },
    {
      "metadata": {
        "id": "ppdia7oK7B-V",
        "colab_type": "code",
        "outputId": "bbbd0eee-091a-41fd-cfe2-9e24e4481033",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "def similarity_score(w1, w2):\n",
        "    score = np.dot(normalized_embeddings[index[w1], :], normalized_embeddings[index[w2], :])\n",
        "    return score\n",
        "\n",
        "def print_similarity(w1,w2):\n",
        "    try:\n",
        "        print('{0}\\t{1}\\t'.format(w1,w2), \\\n",
        "          similarity_score('{}'.format(w1), '{}'.format(w2)))\n",
        "    except:\n",
        "        print('One of the words is not in the dictionary.')\n",
        "    return None\n",
        "  \n",
        "# A word is as similar with itself as possible:\n",
        "print('cat\\tcat\\t', similarity_score('cat', 'cat'))\n",
        "# Closely related words still get high scores:\n",
        "print('cat\\tfeline\\t', similarity_score('cat', 'feline'))\n",
        "print('cat\\tdog\\t', similarity_score('cat', 'dog'))\n",
        "# Unrelated words, not so much\n",
        "print('cat\\tmoo\\t', similarity_score('cat', 'moo'))\n",
        "print('cat\\tfreeze\\t', similarity_score('cat', 'freeze'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cat\tcat\t 1.0000001\n",
            "cat\tfeline\t 0.8199548\n",
            "cat\tdog\t 0.590724\n",
            "cat\tmoo\t 0.0039538303\n",
            "cat\tfreeze\t -0.030225191\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LPmBeqYRAgWu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Word embeddings are fun to play around with, but their primary use is that they allow us to think of words as existing in a continuous, Euclidean space; we can then use an existing arsenal of techniques for machine learning with continuous numerical data (like logistic regression or neural networks) to process text.\n",
        "\n",
        "Let's take a look at an especially simple version of this. We'll perform *sentiment analysis* on a set of movie reviews: in particular, we will attempt to classify a movie review as positive or negative based on its text.\n",
        "\n",
        "We will use a Simple Word Embedding Model (SWEM, Shen et al. 2018) to do so. We will represent a review as the *mean* of the embeddings of the words in the review. Then we'll train a three-layer MLP (a neural network) to classify the review as positive or negative.\n",
        "\n",
        "Download the `movie-simple.txt` file from the repository into this directory. Each line of that file contains\n",
        "1. the numeral 0 (for negative) or the numeral 1 (for positive), followed by\n",
        "2. a tab (the whitespace character), and then\n",
        "3. the review itself."
      ]
    },
    {
      "metadata": {
        "id": "ZAhV69Xc-8m0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import string\n",
        "remove_punct=str.maketrans('','',string.punctuation)\n",
        "\n",
        "# This function converts a line of our data file into\n",
        "# a tuple (x, y), where x is 300-dimensional representation\n",
        "# of the words in a review, and y is its label.\n",
        "def convert_line_to_example(line):\n",
        "    # Pull out the first character: that's our label (0 or 1)\n",
        "    y = int(line[0])\n",
        "    # Split the line into words using Python's split() function\n",
        "    words = line[2:].translate(remove_punct).lower().split()\n",
        "    # Look up the embeddings of each word, ignoring words not\n",
        "    # in our pretrained vocabulary.\n",
        "    embeddings = [normalized_embeddings[index[w]] for w in words\n",
        "                  if w in index]\n",
        "    # Take the mean of the embeddings\n",
        "    x = np.mean(np.vstack(embeddings), axis=0)\n",
        "    return {'x': x, 'y': y, 'w':embeddings}\n",
        "\n",
        "# Apply the function to each line in the file.\n",
        "enc = 'utf-8' # This is necessary from within the singularity shell\n",
        "with open(\"movie-simple.txt\", \"r\", encoding=enc) as f:\n",
        "    dataset = [convert_line_to_example(l) for l in f.readlines()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XrTFJk6TIkVc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " Now that we have a dataset, let's shuffle it and do a train/test split. We use a quarter of the dataset for testing, 3/4 for training (but also ensure that we have a whole number of batches in our training set, to make the code nicer later)."
      ]
    },
    {
      "metadata": {
        "id": "859iJzmtIh2C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import random\n",
        "random.shuffle(dataset)\n",
        "\n",
        "batch_size = 100\n",
        "total_batches = len(dataset) // batch_size\n",
        "train_batches = 3 * total_batches // 4\n",
        "train, test = dataset[:train_batches*batch_size], dataset[train_batches*batch_size:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cC2UXo2nIutM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Time to build our MLP in Tensorflow. We'll use placeholders for X and y as usual."
      ]
    },
    {
      "metadata": {
        "id": "zgHREMoQIsSK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.reset_default_graph()\n",
        "\n",
        "# Placeholders for input\n",
        "X = tf.placeholder(tf.float32, [None, 300]) # 300-D vectors\n",
        "y = tf.placeholder(tf.float32, [None, 1]) # Binary output (T/F)\n",
        "\n",
        "# Three-layer MLP\n",
        "h1 = tf.layers.dense(X, 100, tf.nn.relu)\n",
        "h2 = tf.layers.dense(h1, 20, tf.nn.relu)\n",
        "logits = tf.layers.dense(h2, 1)\n",
        "probabilities = tf.sigmoid(logits)\n",
        "\n",
        "# Loss and metrics\n",
        "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=y))\n",
        "accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.round(tf.sigmoid(logits)), y), tf.float32))\n",
        "\n",
        "# Training using Adam !\n",
        "train_step = tf.train.AdamOptimizer().minimize(loss)\n",
        "\n",
        "# Initialization of variables\n",
        "initialize_all = tf.global_variables_initializer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZiCDbatGJdUl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can now begin a session and train our model. We'll train for 250 epochs. When we're finished, we'll evaluate our accuracy on all the test data."
      ]
    },
    {
      "metadata": {
        "id": "9yDsGw80JeJT",
        "colab_type": "code",
        "outputId": "909d5978-75c8-487e-ccae-686a8066180b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "cell_type": "code",
      "source": [
        "sess = tf.InteractiveSession()\n",
        "sess.run(initialize_all)\n",
        "for epoch in range(250):\n",
        "    for batch in range(train_batches):\n",
        "        data = train[batch*batch_size:(batch+1)*batch_size]\n",
        "        reviews = [sample['x'] for sample in data]\n",
        "        labels  = [sample['y'] for sample in data]\n",
        "        labels = np.array(labels).reshape([-1, 1])\n",
        "        _, l, acc = sess.run([train_step, loss, accuracy], feed_dict={X: reviews, y: labels})\n",
        "    if epoch % 10 == 0:\n",
        "        print(\"Epoch\", epoch, \"Loss\", l, \"Acc\", acc)\n",
        "    random.shuffle(train)\n",
        "\n",
        "# Evaluate on test set\n",
        "test_reviews = [sample['x'] for sample in test]\n",
        "test_labels  = [sample['y'] for sample in test]\n",
        "test_labels = np.array(test_labels).reshape([-1, 1])\n",
        "acc = sess.run(accuracy, feed_dict={X: test_reviews, y: test_labels})\n",
        "print(\"Final accuracy:\", acc)\n",
        "sess.close()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 Loss 0.681476 Acc 0.64\n",
            "Epoch 10 Loss 0.19830504 Acc 0.97\n",
            "Epoch 20 Loss 0.10060603 Acc 0.95\n",
            "Epoch 30 Loss 0.08067612 Acc 0.97\n",
            "Epoch 40 Loss 0.03943902 Acc 0.99\n",
            "Epoch 50 Loss 0.044562403 Acc 1.0\n",
            "Epoch 60 Loss 0.02040045 Acc 1.0\n",
            "Epoch 70 Loss 0.01597068 Acc 1.0\n",
            "Epoch 80 Loss 0.01323399 Acc 1.0\n",
            "Epoch 90 Loss 0.0072068465 Acc 1.0\n",
            "Epoch 100 Loss 0.0061071985 Acc 1.0\n",
            "Epoch 110 Loss 0.013492703 Acc 0.99\n",
            "Epoch 120 Loss 0.0024972514 Acc 1.0\n",
            "Epoch 130 Loss 0.0014498745 Acc 1.0\n",
            "Epoch 140 Loss 0.0019460714 Acc 1.0\n",
            "Epoch 150 Loss 0.0007931841 Acc 1.0\n",
            "Epoch 160 Loss 0.0010455147 Acc 1.0\n",
            "Epoch 170 Loss 0.011070423 Acc 0.99\n",
            "Epoch 180 Loss 0.0008882606 Acc 1.0\n",
            "Epoch 190 Loss 0.017560422 Acc 0.99\n",
            "Epoch 200 Loss 0.0006506664 Acc 1.0\n",
            "Epoch 210 Loss 0.001140778 Acc 1.0\n",
            "Epoch 220 Loss 0.010405387 Acc 0.99\n",
            "Epoch 230 Loss 0.0005293918 Acc 1.0\n",
            "Epoch 240 Loss 0.0005838051 Acc 1.0\n",
            "Final accuracy: 0.96350366\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0R9KHtdZLYTJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This model works great for such a simple dataset, but does a little less well on something more complex. movie-pang02.txt, for instance, has 2000 longer, more complex movie reviews. It's in the same format as our simple dataset. On those longer reviews, this model achieves only 60-80% accuracy. (Increasing the number of epochs to, say, 1000, does help.)"
      ]
    },
    {
      "metadata": {
        "id": "4rRUd8FY7B-g",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### (ii) Train a RNN from the word embeddings to predict sentiment (as done in class) and optimize the network settings to maximize performance."
      ]
    },
    {
      "metadata": {
        "id": "cLdHF4NrLgW_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In the context of deep learning, natural language is commonly modeled with Recurrent Neural Networks (RNNs).\n",
        "RNNs pass the output of a neuron back to the input of the next time step of the same neuron.\n",
        "These directed cycles in the RNN architecture gives them the ability to model temporal dynamics, making them particularly suited for modeling sequences (e.g. text).\n"
      ]
    },
    {
      "metadata": {
        "id": "6IzKuR9E7B-h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with open(\"movie-simple.txt\", \"r\",encoding=enc) as f:\n",
        "    dataset = [convert_line_to_example(l) for l in f.readlines()]\n",
        "import random\n",
        "random.shuffle(dataset)\n",
        "batch_size = 1\n",
        "total_batches = len(dataset) // batch_size\n",
        "train_batches = 3 * total_batches // 4\n",
        "train, test = dataset[:train_batches*batch_size], dataset[train_batches*batch_size:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KUIbQLVp7B-k",
        "colab_type": "code",
        "outputId": "58ecc465-b8eb-48ee-abfe-985a1bd86611",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()\n",
        "# sizes\n",
        "n_steps = None\n",
        "n_inputs = 300 # 300-D vectors\n",
        "n_neurons = 50\n",
        "n_outputs = 1 # binary output\n",
        "\n",
        "# Build RNN\n",
        "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
        "y = tf.placeholder(tf.float32, [None, n_outputs])\n",
        "basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons, activation=tf.nn.tanh)\n",
        "outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32)\n",
        "y_ = tf.layers.dense(outputs[:,-1,:], n_outputs) #logits\n",
        "\n",
        "# Loss and metrics\n",
        "cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=y_)\n",
        "loss = tf.reduce_mean(cross_entropy)\n",
        "accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.round(tf.sigmoid(y_)), y), tf.float32))\n",
        "\n",
        "# Training\n",
        "train_step = tf.train.AdamOptimizer(0.001).minimize(loss)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-26-762f75fccaf4>:11: BasicRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.SimpleRNNCell, and will be replaced by that in Tensorflow 2.0.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3pw1VitK7B-q",
        "colab_type": "code",
        "outputId": "0b954764-f851-4b01-b278-6468c0babf04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        }
      },
      "cell_type": "code",
      "source": [
        "num_epochs = 250 # same # as part (a), BUT TOO SLOW, ACUTALLY TOOK ~40 MINs\n",
        "initialize_all = tf.global_variables_initializer()\n",
        "sess = tf.InteractiveSession()\n",
        "sess.run(initialize_all)\n",
        "l_ma=.74\n",
        "acc_ma=.5\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in range(train_batches):\n",
        "        data = train[batch*batch_size:(batch+1)*batch_size]\n",
        "        reviews = np.array([sample['w'] for sample in data]).reshape([1,-1,300])\n",
        "        labels  = np.array([sample['y'] for sample in data]).reshape([1,1])\n",
        "        labels = np.array(labels).reshape([-1, 1])\n",
        "        _, l, acc = sess.run([train_step, loss, accuracy], feed_dict={X: reviews, y: labels})\n",
        "        l_ma=.99*l_ma+(.01)*l\n",
        "        acc_ma=.99*acc_ma+(.01)*acc\n",
        "    if epoch % 10 == 0:\n",
        "        print(\"Epoch\", epoch, \"Loss\", l_ma, \"Acc\", acc_ma)\n",
        "    random.shuffle(train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
            "  warnings.warn('An interactive session is already active. This can '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 Loss 0.3573888612832883 Acc 0.8842813596940621\n",
            "Epoch 10 Loss 0.11019765479581882 Acc 0.9690619773356354\n",
            "Epoch 20 Loss 0.13173155836368167 Acc 0.9600977942283906\n",
            "Epoch 30 Loss 0.029505709341571203 Acc 0.991241870358205\n",
            "Epoch 40 Loss 0.006868877635509963 Acc 0.9986339901496604\n",
            "Epoch 50 Loss 0.00264358100856324 Acc 0.9993817448549868\n",
            "Epoch 60 Loss 0.004026522684435096 Acc 0.9966224546675948\n",
            "Epoch 70 Loss 0.00020721015478110267 Acc 0.9999862073682555\n",
            "Epoch 80 Loss 0.010600609836024849 Acc 0.9941732041121452\n",
            "Epoch 90 Loss 0.0006369693345750222 Acc 0.9998326888023266\n",
            "Epoch 100 Loss 0.16830466682156225 Acc 0.9551185903276994\n",
            "Epoch 110 Loss 0.03868497111872263 Acc 0.9882958010740001\n",
            "Epoch 120 Loss 0.03737030617026399 Acc 0.9914410601484754\n",
            "Epoch 130 Loss 0.00045571810966438557 Acc 0.9998360185799864\n",
            "Epoch 140 Loss 0.000163468162970757 Acc 0.9999930481999387\n",
            "Epoch 150 Loss 0.0006911297575595828 Acc 0.9995130627859232\n",
            "Epoch 160 Loss 0.0006516873042593447 Acc 0.9999503993038104\n",
            "Epoch 170 Loss 0.0012587722234305818 Acc 0.9992142878636435\n",
            "Epoch 180 Loss 0.000610943270302319 Acc 0.9996716528592052\n",
            "Epoch 190 Loss 0.00046025925041829013 Acc 0.999861997050983\n",
            "Epoch 200 Loss 0.001699517751933497 Acc 0.9987564933160098\n",
            "Epoch 210 Loss 0.01059273861006571 Acc 0.9934434073295705\n",
            "Epoch 220 Loss 0.00015748218467278582 Acc 0.9999754566536658\n",
            "Epoch 230 Loss 0.05429191297791474 Acc 0.9887439323972788\n",
            "Epoch 240 Loss 5.692727052516328e-05 Acc 0.9999638863054883\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uwVopGbfTYTT",
        "colab_type": "code",
        "outputId": "726ae9dd-1b56-4182-883e-67493b1c88a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Evaluate on test set\n",
        "test_acc=0\n",
        "n=0\n",
        "for sample in test:\n",
        "    test_reviews = np.array([sample['w'] ]).reshape([1,-1,300])\n",
        "    test_labels  = np.array([sample['y']]).reshape([1,1])\n",
        "    test_labels = np.array(test_labels).reshape([-1, 1])\n",
        "    test_acc += sess.run(accuracy, feed_dict={X: test_reviews, y: test_labels})\n",
        "    n+=1\n",
        "acc=test_acc/n \n",
        "print(\"Final accuracy:\", acc)\n",
        "sess.close()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Final accuracy: 0.9518413597733711\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KeQUcvI17B-t",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### (iii) Encode each vocabulary word as a one-hot vector. Train an MLP on the average of the onehot vectors."
      ]
    },
    {
      "metadata": {
        "id": "iXni8bNJ7B-u",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "one_hot_representation = np.identity(len(english_words), dtype=np.float32)\n",
        "\n",
        "# Same as part (a)\n",
        "import string\n",
        "remove_punct=str.maketrans('','',string.punctuation)\n",
        "\n",
        "# This function converts a line of our data file into\n",
        "# a tuple (x, y), where x is len(english_words)-dimensional representation (1-hot)\n",
        "# of the words in a review, and y is its label.\n",
        "def convert_line_to_one_hot_example(line):\n",
        "    # Pull out the first character: that's our label (0 or 1)\n",
        "    y = int(line[0])\n",
        "    # Split the line into words using Python's split() function\n",
        "    words = line[2:].translate(remove_punct).lower().split()\n",
        "    # Look up the embeddings of each word, ignoring words not\n",
        "    # in our pretrained vocabulary.\n",
        "    embeddings = [one_hot_representation[index[w]] for w in words\n",
        "                  if w in index]\n",
        "    # Take the mean of the embeddings\n",
        "    x = np.mean(np.vstack(embeddings), axis=0)\n",
        "    return {'x': x, 'y': y, 'w':embeddings}\n",
        "\n",
        "# Apply the function to each line in the file.\n",
        "enc = 'utf-8' # This is necessary from within the singularity shell\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FVZe0YBZ7B-z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Same implementation as (a)\n",
        "with open(\"movie-simple.txt\", \"r\", encoding=enc) as f:\n",
        "    # Now convert line to one-hot embeddings\n",
        "    dataset = [convert_line_to_one_hot_example(l) for l in f.readlines()] \n",
        "import random\n",
        "random.shuffle(dataset)\n",
        "\n",
        "batch_size = 100\n",
        "total_batches = len(dataset) // batch_size\n",
        "train_batches = 3 * total_batches // 4\n",
        "train, test = dataset[:train_batches*batch_size], dataset[train_batches*batch_size:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WjUni26E7B-1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.reset_default_graph()\n",
        "\n",
        "# Placeholders for input\n",
        "X = tf.placeholder(tf.float32, [None, len(english_words)]) # 150875-D vectors\n",
        "y = tf.placeholder(tf.float32, [None, 1]) # Binary output (T/F)\n",
        "\n",
        "# Three-layer MLP\n",
        "h1 = tf.layers.dense(X, 100, tf.nn.relu)\n",
        "h2 = tf.layers.dense(h1, 20, tf.nn.relu)\n",
        "logits = tf.layers.dense(h2, 1)\n",
        "probabilities = tf.sigmoid(logits)\n",
        "\n",
        "# Loss and metrics\n",
        "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=y))\n",
        "accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.round(tf.sigmoid(logits)), y), tf.float32))\n",
        "\n",
        "# Training using Adam !\n",
        "train_step = tf.train.AdamOptimizer().minimize(loss)\n",
        "\n",
        "# Initialization of variables\n",
        "initialize_all = tf.global_variables_initializer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sqchTTCmYgdJ",
        "colab_type": "code",
        "outputId": "f1b2c50d-514c-4c69-aa15-b0fd53ddad40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        }
      },
      "cell_type": "code",
      "source": [
        "# Train\n",
        "sess = tf.InteractiveSession()\n",
        "sess.run(initialize_all)\n",
        "for epoch in range(250):\n",
        "    for batch in range(train_batches):\n",
        "        data = train[batch*batch_size:(batch+1)*batch_size]\n",
        "        reviews = [sample['x'] for sample in data]\n",
        "        labels  = [sample['y'] for sample in data]\n",
        "        labels = np.array(labels).reshape([-1, 1])\n",
        "        _, l, acc = sess.run([train_step, loss, accuracy], feed_dict={X: reviews, y: labels})\n",
        "    if epoch % 10 == 0:\n",
        "        print(\"Epoch\", epoch, \"Loss\", l, \"Acc\", acc)\n",
        "    random.shuffle(train)\n",
        "\n",
        "# Evaluate on test set\n",
        "test_reviews = [sample['x'] for sample in test]\n",
        "test_labels  = [sample['y'] for sample in test]\n",
        "test_labels = np.array(test_labels).reshape([-1, 1])\n",
        "acc = sess.run(accuracy, feed_dict={X: test_reviews, y: test_labels})\n",
        "print(\"Final accuracy:\", acc)\n",
        "sess.close()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
            "  warnings.warn('An interactive session is already active. This can '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 Loss 0.69226265 Acc 0.5\n",
            "Epoch 10 Loss 0.6821994 Acc 0.53\n",
            "Epoch 20 Loss 0.68925375 Acc 0.49\n",
            "Epoch 30 Loss 0.6647892 Acc 0.59\n",
            "Epoch 40 Loss 0.6734055 Acc 0.52\n",
            "Epoch 50 Loss 0.63869524 Acc 0.67\n",
            "Epoch 60 Loss 0.6288469 Acc 0.63\n",
            "Epoch 70 Loss 0.57186645 Acc 0.78\n",
            "Epoch 80 Loss 0.50734836 Acc 0.84\n",
            "Epoch 90 Loss 0.39258626 Acc 0.93\n",
            "Epoch 100 Loss 0.36231154 Acc 0.86\n",
            "Epoch 110 Loss 0.31141827 Acc 0.92\n",
            "Epoch 120 Loss 0.24365845 Acc 0.96\n",
            "Epoch 130 Loss 0.2425871 Acc 0.94\n",
            "Epoch 140 Loss 0.24263132 Acc 0.92\n",
            "Epoch 150 Loss 0.2242392 Acc 0.92\n",
            "Epoch 160 Loss 0.18156032 Acc 0.95\n",
            "Epoch 170 Loss 0.18917479 Acc 0.91\n",
            "Epoch 180 Loss 0.112309545 Acc 0.97\n",
            "Epoch 190 Loss 0.14119118 Acc 0.98\n",
            "Epoch 200 Loss 0.15195604 Acc 0.94\n",
            "Epoch 210 Loss 0.08990037 Acc 0.98\n",
            "Epoch 220 Loss 0.18237732 Acc 0.95\n",
            "Epoch 230 Loss 0.11635973 Acc 0.97\n",
            "Epoch 240 Loss 0.15585178 Acc 0.94\n",
            "Final accuracy: 0.9659367\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VKtsJ-ObYzoZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### (iv) As in (iii), but use an RNN on the one-hot encodings."
      ]
    },
    {
      "metadata": {
        "id": "lVsm1D15Y6oO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with open(\"movie-simple.txt\", \"r\",encoding=enc) as f:\n",
        "    dataset = [convert_line_to_one_hot_example(l) for l in f.readlines()]\n",
        "import random\n",
        "random.shuffle(dataset)\n",
        "batch_size = 1\n",
        "total_batches = len(dataset) // batch_size\n",
        "train_batches = 3 * total_batches // 4\n",
        "train, test = dataset[:train_batches*batch_size], dataset[train_batches*batch_size:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Qtl-3vxcZKyo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()\n",
        "# sizes\n",
        "n_steps = None\n",
        "n_inputs = len(english_words) # 150875-D vectors\n",
        "n_neurons = 50\n",
        "n_outputs = 1 # binary output\n",
        "\n",
        "# Build RNN\n",
        "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
        "y = tf.placeholder(tf.float32, [None, n_outputs])\n",
        "basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons, activation=tf.nn.tanh)\n",
        "outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32)\n",
        "y_ = tf.layers.dense(outputs[:,-1,:], n_outputs) #logits\n",
        "\n",
        "# Loss and metrics\n",
        "cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=y_)\n",
        "loss = tf.reduce_mean(cross_entropy)\n",
        "accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.round(tf.sigmoid(y_)), y), tf.float32))\n",
        "\n",
        "# Training\n",
        "train_step = tf.train.AdamOptimizer(0.001).minimize(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SrbMgA6WZoUB",
        "colab_type": "code",
        "outputId": "621b8f0e-6e25-4fad-f1ff-209dde58682f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "cell_type": "code",
      "source": [
        "num_epochs = 10 # CHANGED SINCE TRAINING IS TOO SLOW!\n",
        "initialize_all = tf.global_variables_initializer()\n",
        "sess = tf.InteractiveSession()\n",
        "sess.run(initialize_all)\n",
        "l_ma=.74\n",
        "acc_ma=.5\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in range(train_batches):\n",
        "        data = train[batch*batch_size:(batch+1)*batch_size]\n",
        "        reviews = np.array([sample['w'] for sample in data]).reshape([1,-1,len(english_words)])\n",
        "        labels  = np.array([sample['y'] for sample in data]).reshape([1,1])\n",
        "        labels = np.array(labels).reshape([-1, 1])\n",
        "        _, l, acc = sess.run([train_step, loss, accuracy], feed_dict={X: reviews, y: labels})\n",
        "        l_ma=.99*l_ma+(.01)*l\n",
        "        acc_ma=.99*acc_ma+(.01)*acc\n",
        "    if epoch % 1 == 0:\n",
        "        print(\"Epoch\", epoch, \"Loss\", l_ma, \"Acc\", acc_ma)\n",
        "    random.shuffle(train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
            "  warnings.warn('An interactive session is already active. This can '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 Loss 0.40502152455505647 Acc 0.796964593683877\n",
            "Epoch 1 Loss 0.1662961574440515 Acc 0.9355787352018449\n",
            "Epoch 2 Loss 0.12527569385690035 Acc 0.9513816659501054\n",
            "Epoch 3 Loss 0.134874889028147 Acc 0.9387998497851943\n",
            "Epoch 4 Loss 0.0890855157124037 Acc 0.9595905268229524\n",
            "Epoch 5 Loss 0.07715123540561743 Acc 0.9806638545230753\n",
            "Epoch 6 Loss 0.05880502201158864 Acc 0.9819542680566435\n",
            "Epoch 7 Loss 0.1555484656472933 Acc 0.948687954174636\n",
            "Epoch 8 Loss 0.1668978511977483 Acc 0.948808927511394\n",
            "Epoch 9 Loss 0.03226694494256042 Acc 0.9796306662474786\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5dc7_-opZ_ZO",
        "colab_type": "code",
        "outputId": "67f1612b-5e6d-42c0-c21b-ccd84e295500",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Evaluate on test set\n",
        "test_acc=0\n",
        "n=0\n",
        "for sample in test:\n",
        "    test_reviews = np.array([sample['w'] ]).reshape([1,-1,len(english_words)])\n",
        "    test_labels  = np.array([sample['y']]).reshape([1,1])\n",
        "    test_labels = np.array(test_labels).reshape([-1, 1])\n",
        "    test_acc += sess.run(accuracy, feed_dict={X: test_reviews, y: test_labels})\n",
        "    n+=1\n",
        "acc=test_acc/n \n",
        "print(\"Final accuracy:\", acc)\n",
        "sess.close()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Final accuracy: 0.9405099150141643\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-wIhjKHuaOop",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### (v) Why did the word embeddings work better (hint: the word embeddings will work better…)"
      ]
    },
    {
      "metadata": {
        "id": "0P3xD0qLtYZH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Answer:**\n",
        "\n",
        "One-hot vectors are typically high-dimensional, and in this problem, the one-hot matrix has a very large size (~150,000) and also very sparse. In contrast, word embeddings are low-dimensional (300 in this problem). Word embeddings have the ability to generalize, due to semantically similar words having similar vectors, which is not the case in one-hot vectors. In our problem, the length of the given dataset is about 1,400. Thus, the input dimension of 150,000 introduced by ont-hot embeddings is too large to generalize. Also, it costs much more computational resources."
      ]
    },
    {
      "metadata": {
        "id": "RlK4Ytq_wQG6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Time-series prediction is an important task, which is commonly approached with RNNs.**"
      ]
    },
    {
      "metadata": {
        "id": "jY3AW3KhwU9G",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### (vi) How does cross-validation change when considering a time-series instead of multiple instances (as in our movie reviews)? Only a description is needed.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "Gj8FfMNxw5JT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Answer:**\n",
        "\n",
        "When dealing with time series, unlike the traditional k-fold cross validation, our modified cross-validation strategy/solution is to break the time series into k continuous chunks, then train on the first k sets of data and evaluate on the k+1 th set of data. However, this will not work well when we are dealing with multiple instances because one assumption is that different instances are independently distributed.  We will lose some information if we split these instances since each instance is distinct."
      ]
    },
    {
      "metadata": {
        "id": "IpCSaTSYwq5a",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Much of the strength of deep learning comes from the ability to stitch together different deep\n",
        "learning modules and approaches into a larger framework. In class, we discussed training an\n",
        "RNN against a loss function. Instead, CNNs, RNNs, GANs can all be combined.**"
      ]
    },
    {
      "metadata": {
        "id": "tMSgHVblwvEG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### (vii) In our previous homework assignment we considered the conditional GAN. In that case, the conditional label was known and given. Instead, consider generating images to match text. One approach could be to use an RNN to encode text to a vector that is fed to a conditional GAN (e.g. http://proceedings.mlr.press/v48/reed16.pdf). Draw a graph (but do not implement) how such a system could work. Any implementation here is completely optional, we are only looking for a description of how this could work."
      ]
    },
    {
      "metadata": {
        "id": "LnfUPGPl1h5-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Answer: \n",
        "\n",
        "See PDF file."
      ]
    },
    {
      "metadata": {
        "id": "m-d1lg5Y7B-3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Problem 4: SARSA Q-Learning (30 points)"
      ]
    },
    {
      "metadata": {
        "id": "XnBJV5Nc7B-5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**SARSA is an alternative way of learning a Q-Value function with a lot of similarity to the simpler\n",
        "policy introduced during the code exercises.**\n",
        "\n",
        "**Q-learning update rule:**\n",
        "\n",
        "**$Q^{new}(s_t, a_t) \\leftarrow (1 - \\alpha)Q^{old}(s_t, a_t) + \\alpha[r_t + \\gamma max_aQ^{old}(s_{t+1},a)]$**\n",
        "\n",
        "**SARSA update rule:**\n",
        "\n",
        "**$Q^{new}(s_t, a_t) \\leftarrow (1 - \\alpha)Q^{old}(s_t, a_t) + \\alpha[r_t + \\gamma Q^{old}(s_{t+1},a_{t+1})]$**\n",
        "\n",
        "**Unlike Q-learning, which is considered an *off-policy* network, SARSA is an *on-policy* algorithm.\n",
        "When Q-learning calculates the estimated future reward, it must \"guess\" the future, starting\n",
        "with the next action the agent will take. In Q-learning, we assume the agent will take the best\n",
        "possible action. SARSA, on the other hand, uses the action that was actually taken next in the\n",
        "episode we are learning from. In other words, SARSA learns from the next action he actually\n",
        "took (on policy), as opposed to what the max possible Q value for the next state was (off policy).**\n",
        "\n",
        "**Here, we will use the same setup that we discussed in class with the cart pole problem.**"
      ]
    },
    {
      "metadata": {
        "id": "aLCm3AxfQVrA",
        "colab_type": "code",
        "outputId": "20b77b59-9812-4d14-a856-7ec3e16dc413",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install gym"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.10.9)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.14.6)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (2.18.4)\n",
            "Requirement already satisfied: pyglet>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym) (1.11.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.1.0)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (2.6)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (1.22)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (2018.10.15)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet>=1.2.0->gym) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yiQPzl1aQbc6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### (a) Implement a deep Q-learning approach to the cart pole problem (was done in class) using an $\\epsilon$-Greedy approach"
      ]
    },
    {
      "metadata": {
        "id": "5FLDjM19QcX1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Based on: https://gym.openai.com/evaluations/eval_EIcM1ZBnQW2LBaFN6FY65g/\n",
        "\n",
        "import random\n",
        "import gym\n",
        "import math\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from collections import deque"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CjIOF0OKRfEe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274
        },
        "outputId": "cd070c2b-bbeb-412a-d999-7e5941b807e4"
      },
      "cell_type": "code",
      "source": [
        "class DQNCartPoleSolver():\n",
        "    def __init__(self, n_episodes=1000, n_win_ticks=195, max_env_steps=None, gamma=1.0, epsilon=1.0, epsilon_min=0.01, epsilon_log_decay=0.995, alpha=0.01, alpha_decay=0.01, batch_size=64, monitor=False, quiet=False):\n",
        "        self.memory = deque(maxlen=100000)\n",
        "        self.env = gym.make('CartPole-v0')\n",
        "        if monitor: self.env = gym.wrappers.Monitor(self.env, '../data/cartpole-1', force=True)\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_min = epsilon_min\n",
        "        self.epsilon_decay = epsilon_log_decay\n",
        "        self.alpha = alpha\n",
        "        self.alpha_decay = alpha_decay\n",
        "        self.n_episodes = n_episodes\n",
        "        self.n_win_ticks = n_win_ticks\n",
        "        self.batch_size = batch_size\n",
        "        self.quiet = quiet\n",
        "        if max_env_steps is not None: self.env._max_episode_steps = max_env_steps\n",
        "\n",
        "        # Init model\n",
        "        self.state_ = tf.placeholder(tf.float32, shape=[None, 4])\n",
        "        h = tf.layers.dense(self.state_, units=24, activation=tf.nn.tanh)\n",
        "        h = tf.layers.dense(h, units=48, activation=tf.nn.tanh)\n",
        "        self.Q = tf.layers.dense(h, units=2)\n",
        "        \n",
        "        self.Q_ = tf.placeholder(tf.float32, shape=[None, 2])\n",
        "        loss = tf.losses.mean_squared_error(self.Q_, self.Q)\n",
        "        self.global_step = tf.Variable(0, name='global_step', trainable=False)\n",
        "        lr = tf.train.exponential_decay(0.01, self.global_step, 0.995, 1)\n",
        "        self.train_step = tf.train.AdamOptimizer(lr).minimize(loss, global_step=self.global_step)\n",
        "        \n",
        "        self.sess = tf.Session()\n",
        "        self.sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def choose_action(self, state, epsilon):\n",
        "        return self.env.action_space.sample() if (np.random.random() <= epsilon) else np.argmax(self.sess.run(self.Q, feed_dict={self.state_: state}))\n",
        "\n",
        "    def get_epsilon(self, t):\n",
        "        return max(self.epsilon_min, min(self.epsilon, 1.0 - math.log10((t + 1) * self.epsilon_decay)))\n",
        "\n",
        "    def preprocess_state(self, state):\n",
        "        return np.reshape(state, [1, 4])\n",
        "\n",
        "    def replay(self, batch_size):\n",
        "        x_batch, y_batch = [], []\n",
        "        minibatch = random.sample(self.memory, min(len(self.memory), batch_size))\n",
        "        for state, action, reward, next_state, done in minibatch:\n",
        "            y_target = self.sess.run(self.Q, feed_dict={self.state_: state})\n",
        "            y_target[0][action] = reward if done else reward + self.gamma * np.max(self.sess.run(self.Q, feed_dict={self.state_: next_state})[0])\n",
        "            x_batch.append(state[0])\n",
        "            y_batch.append(y_target[0])\n",
        "        \n",
        "        self.sess.run(self.train_step, feed_dict={self.state_: np.array(x_batch), self.Q_: np.array(y_batch)})\n",
        "\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "    def run(self):\n",
        "        scores = deque(maxlen=100)\n",
        "\n",
        "        for e in range(self.n_episodes):\n",
        "            state = self.preprocess_state(self.env.reset())\n",
        "            done = False\n",
        "            i = 0\n",
        "            while not done:\n",
        "                #if e % 100 == 0 and not self.quiet:\n",
        "                #    self.env.render()\n",
        "                action = self.choose_action(state, self.get_epsilon(e))\n",
        "                next_state, reward, done, _ = self.env.step(action)\n",
        "                next_state = self.preprocess_state(next_state)\n",
        "                self.remember(state, action, reward, next_state, done)\n",
        "                state = next_state\n",
        "                i += 1\n",
        "\n",
        "            scores.append(i)\n",
        "            mean_score = np.mean(scores)\n",
        "            if mean_score >= self.n_win_ticks and e >= 100:\n",
        "                if not self.quiet: print('Ran {} episodes. Solved after {} trials ✔'.format(e, e - 100))\n",
        "                return e - 100\n",
        "            if e % 100 == 0 and not self.quiet:\n",
        "                print('[Episode {}] - Mean survival time over last 100 episodes was {} ticks.'.format(e, mean_score))\n",
        "\n",
        "            self.replay(self.batch_size)\n",
        "        \n",
        "        if not self.quiet: print('Did not solve after {} episodes 😞'.format(e))\n",
        "        return e\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    agent = DQNCartPoleSolver()\n",
        "    agent.run()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
            "  result = entry_point.load(False)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Episode 0] - Mean survival time over last 100 episodes was 12.0 ticks.\n",
            "[Episode 100] - Mean survival time over last 100 episodes was 20.75 ticks.\n",
            "[Episode 200] - Mean survival time over last 100 episodes was 28.49 ticks.\n",
            "[Episode 300] - Mean survival time over last 100 episodes was 92.88 ticks.\n",
            "[Episode 400] - Mean survival time over last 100 episodes was 131.63 ticks.\n",
            "[Episode 500] - Mean survival time over last 100 episodes was 123.88 ticks.\n",
            "[Episode 600] - Mean survival time over last 100 episodes was 133.79 ticks.\n",
            "[Episode 700] - Mean survival time over last 100 episodes was 141.96 ticks.\n",
            "[Episode 800] - Mean survival time over last 100 episodes was 141.36 ticks.\n",
            "[Episode 900] - Mean survival time over last 100 episodes was 162.49 ticks.\n",
            "Did not solve after 999 episodes 😞\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HSAgjaAqV_S6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "outputId": "f2c9a800-fabb-4337-bf59-1e679936b3b3"
      },
      "cell_type": "code",
      "source": [
        "class DQNCartPoleSolver(): # REDO \n",
        "    def __init__(self, n_episodes=1000, n_win_ticks=195, max_env_steps=None, gamma=1.0, epsilon=1.0, epsilon_min=0.01, epsilon_log_decay=0.995, alpha=0.01, alpha_decay=0.01, batch_size=64, monitor=False, quiet=False):\n",
        "        self.memory = deque(maxlen=100000)\n",
        "        self.env = gym.make('CartPole-v0')\n",
        "        if monitor: self.env = gym.wrappers.Monitor(self.env, '../data/cartpole-1', force=True)\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_min = epsilon_min\n",
        "        self.epsilon_decay = epsilon_log_decay\n",
        "        self.alpha = alpha\n",
        "        self.alpha_decay = alpha_decay\n",
        "        self.n_episodes = n_episodes\n",
        "        self.n_win_ticks = n_win_ticks\n",
        "        self.batch_size = batch_size\n",
        "        self.quiet = quiet\n",
        "        if max_env_steps is not None: self.env._max_episode_steps = max_env_steps\n",
        "\n",
        "        # Init model\n",
        "        self.state_ = tf.placeholder(tf.float32, shape=[None, 4])\n",
        "        h = tf.layers.dense(self.state_, units=24, activation=tf.nn.tanh)\n",
        "        h = tf.layers.dense(h, units=48, activation=tf.nn.tanh)\n",
        "        self.Q = tf.layers.dense(h, units=2)\n",
        "        \n",
        "        self.Q_ = tf.placeholder(tf.float32, shape=[None, 2])\n",
        "        loss = tf.losses.mean_squared_error(self.Q_, self.Q)\n",
        "        self.global_step = tf.Variable(0, name='global_step', trainable=False)\n",
        "        lr = tf.train.exponential_decay(0.01, self.global_step, 0.995, 1)\n",
        "        self.train_step = tf.train.AdamOptimizer(lr).minimize(loss, global_step=self.global_step)\n",
        "        \n",
        "        self.sess = tf.Session()\n",
        "        self.sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def choose_action(self, state, epsilon):\n",
        "        return self.env.action_space.sample() if (np.random.random() <= epsilon) else np.argmax(self.sess.run(self.Q, feed_dict={self.state_: state}))\n",
        "\n",
        "    def get_epsilon(self, t):\n",
        "        return max(self.epsilon_min, min(self.epsilon, 1.0 - math.log10((t + 1) * self.epsilon_decay)))\n",
        "\n",
        "    def preprocess_state(self, state):\n",
        "        return np.reshape(state, [1, 4])\n",
        "\n",
        "    def replay(self, batch_size):\n",
        "        x_batch, y_batch = [], []\n",
        "        minibatch = random.sample(self.memory, min(len(self.memory), batch_size))\n",
        "        for state, action, reward, next_state, done in minibatch:\n",
        "            y_target = self.sess.run(self.Q, feed_dict={self.state_: state})\n",
        "            y_target[0][action] = reward if done else reward + self.gamma * np.max(self.sess.run(self.Q, feed_dict={self.state_: next_state})[0])\n",
        "            x_batch.append(state[0])\n",
        "            y_batch.append(y_target[0])\n",
        "        \n",
        "        self.sess.run(self.train_step, feed_dict={self.state_: np.array(x_batch), self.Q_: np.array(y_batch)})\n",
        "\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "    def run(self):\n",
        "        scores = deque(maxlen=100)\n",
        "\n",
        "        for e in range(self.n_episodes):\n",
        "            state = self.preprocess_state(self.env.reset())\n",
        "            done = False\n",
        "            i = 0\n",
        "            while not done:\n",
        "                #if e % 100 == 0 and not self.quiet:\n",
        "                #    self.env.render()\n",
        "                action = self.choose_action(state, self.get_epsilon(e))\n",
        "                next_state, reward, done, _ = self.env.step(action)\n",
        "                next_state = self.preprocess_state(next_state)\n",
        "                self.remember(state, action, reward, next_state, done)\n",
        "                state = next_state\n",
        "                i += 1\n",
        "\n",
        "            scores.append(i)\n",
        "            mean_score = np.mean(scores)\n",
        "            if mean_score >= self.n_win_ticks and e >= 100:\n",
        "                if not self.quiet: print('Ran {} episodes. Solved after {} trials ✔'.format(e, e - 100))\n",
        "                return e - 100\n",
        "            if e % 100 == 0 and not self.quiet:\n",
        "                print('[Episode {}] - Mean survival time over last 100 episodes was {} ticks.'.format(e, mean_score))\n",
        "\n",
        "            self.replay(self.batch_size)\n",
        "        \n",
        "        if not self.quiet: print('Did not solve after {} episodes 😞'.format(e))\n",
        "        return e\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    agent = DQNCartPoleSolver(n_episodes=1500)\n",
        "    agent.run()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
            "  result = entry_point.load(False)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Episode 0] - Mean survival time over last 100 episodes was 23.0 ticks.\n",
            "[Episode 100] - Mean survival time over last 100 episodes was 22.32 ticks.\n",
            "[Episode 200] - Mean survival time over last 100 episodes was 58.41 ticks.\n",
            "[Episode 300] - Mean survival time over last 100 episodes was 91.81 ticks.\n",
            "[Episode 400] - Mean survival time over last 100 episodes was 124.66 ticks.\n",
            "[Episode 500] - Mean survival time over last 100 episodes was 140.79 ticks.\n",
            "[Episode 600] - Mean survival time over last 100 episodes was 112.16 ticks.\n",
            "[Episode 700] - Mean survival time over last 100 episodes was 148.73 ticks.\n",
            "[Episode 800] - Mean survival time over last 100 episodes was 156.81 ticks.\n",
            "[Episode 900] - Mean survival time over last 100 episodes was 135.08 ticks.\n",
            "[Episode 1000] - Mean survival time over last 100 episodes was 155.75 ticks.\n",
            "[Episode 1100] - Mean survival time over last 100 episodes was 94.18 ticks.\n",
            "[Episode 1200] - Mean survival time over last 100 episodes was 135.42 ticks.\n",
            "[Episode 1300] - Mean survival time over last 100 episodes was 129.91 ticks.\n",
            "[Episode 1400] - Mean survival time over last 100 episodes was 152.66 ticks.\n",
            "Ran 1475 episodes. Solved after 1375 trials ✔\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kJDg6I9G7B-8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### (b) Implement a SARSA approach with a deep network to solve the cart pole problem using an $\\epsilon$-Greedy approach"
      ]
    },
    {
      "metadata": {
        "id": "9GmpZ7SgWWfq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "outputId": "d4d1ad49-6baa-4fa1-95b7-1e0a985e465d"
      },
      "cell_type": "code",
      "source": [
        "class SARSACartPoleSolver():\n",
        "    def __init__(self, n_episodes=1000, n_win_ticks=195, max_env_steps=None, gamma=1.0, epsilon=1.0, epsilon_min=0.01, epsilon_log_decay=0.995, alpha=0.01, alpha_decay=0.01, batch_size=64, monitor=False, quiet=False):\n",
        "        self.memory = deque(maxlen=100000)\n",
        "        self.env = gym.make('CartPole-v0')\n",
        "        if monitor: self.env = gym.wrappers.Monitor(self.env, '../data/cartpole-1', force=True)\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_min = epsilon_min\n",
        "        self.epsilon_decay = epsilon_log_decay\n",
        "        self.alpha = alpha\n",
        "        self.alpha_decay = alpha_decay\n",
        "        self.n_episodes = n_episodes\n",
        "        self.n_win_ticks = n_win_ticks\n",
        "        self.batch_size = batch_size\n",
        "        self.quiet = quiet\n",
        "        if max_env_steps is not None: self.env._max_episode_steps = max_env_steps\n",
        "\n",
        "        # Init model\n",
        "        self.state_ = tf.placeholder(tf.float32, shape=[None, 4])\n",
        "        h = tf.layers.dense(self.state_, units=24, activation=tf.nn.tanh)\n",
        "        h = tf.layers.dense(h, units=48, activation=tf.nn.tanh)\n",
        "        self.Q = tf.layers.dense(h, units=2)\n",
        "        \n",
        "        self.Q_ = tf.placeholder(tf.float32, shape=[None, 2])\n",
        "        loss = tf.losses.mean_squared_error(self.Q_, self.Q)\n",
        "        self.global_step = tf.Variable(0, name='global_step', trainable=False)\n",
        "        lr = tf.train.exponential_decay(0.01, self.global_step, 0.995, 1)\n",
        "        self.train_step = tf.train.AdamOptimizer(lr).minimize(loss, global_step=self.global_step)\n",
        "        \n",
        "        self.sess = tf.Session()\n",
        "        self.sess.run(tf.global_variables_initializer())\n",
        "    # Modification: Add a_t+1 term (next action)\n",
        "    def remember(self, state, action, reward, next_state, next_action, done):\n",
        "        self.memory.append((state, action, reward, next_state, next_action, done))\n",
        "\n",
        "    def choose_action(self, state, epsilon):\n",
        "        return self.env.action_space.sample() if (np.random.random() <= epsilon) else np.argmax(self.sess.run(self.Q, feed_dict={self.state_: state}))\n",
        "\n",
        "    def get_epsilon(self, t):\n",
        "        return max(self.epsilon_min, min(self.epsilon, 1.0 - math.log10((t + 1) * self.epsilon_decay)))\n",
        "\n",
        "    def preprocess_state(self, state):\n",
        "        return np.reshape(state, [1, 4])\n",
        "\n",
        "    def replay(self, batch_size):\n",
        "        x_batch, y_batch = [], []\n",
        "        minibatch = random.sample(self.memory, min(len(self.memory), batch_size))\n",
        "        for state, action, reward, next_state, next_action, done in minibatch: # Modification: Add a_t+1 term (next action)\n",
        "            y_target = self.sess.run(self.Q, feed_dict={self.state_: state})\n",
        "            # Modification: Q^old(s_t+1, a_t+1)\n",
        "            y_target[0][action] = reward if done else reward + self.gamma * self.sess.run(self.Q, feed_dict={self.state_: next_state})[0][next_action]\n",
        "            x_batch.append(state[0])\n",
        "            y_batch.append(y_target[0])\n",
        "        \n",
        "        self.sess.run(self.train_step, feed_dict={self.state_: np.array(x_batch), self.Q_: np.array(y_batch)})\n",
        "\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "    def run(self):\n",
        "        scores = deque(maxlen=100)\n",
        "\n",
        "        for e in range(self.n_episodes):\n",
        "            state = self.preprocess_state(self.env.reset())\n",
        "            # Modification: Add initial conditions a0\n",
        "            action = self.choose_action(state, self.get_epsilon(e))\n",
        "            \n",
        "            done = False\n",
        "            i = 0\n",
        "            while not done:\n",
        "                #if e % 100 == 0 and not self.quiet:\n",
        "                #    self.env.render()\n",
        "                # action = self.choose_action(state, self.get_epsilon(e))\n",
        "                next_state, reward, done, _ = self.env.step(action)\n",
        "                next_state = self.preprocess_state(next_state)\n",
        "                # Modification: Choose next action based on next state\n",
        "                action_next = self.choose_action(next_state, self.get_epsilon(e))\n",
        "                self.remember(state, action, reward, next_state, action_next, done)\n",
        "                state = next_state\n",
        "                action = action_next # Update action as well\n",
        "                i += 1\n",
        "\n",
        "            scores.append(i)\n",
        "            mean_score = np.mean(scores)\n",
        "            if mean_score >= self.n_win_ticks and e >= 100:\n",
        "                if not self.quiet: print('Ran {} episodes. Solved after {} trials ✔'.format(e, e - 100))\n",
        "                return e - 100\n",
        "            if e % 100 == 0 and not self.quiet:\n",
        "                print('[Episode {}] - Mean survival time over last 100 episodes was {} ticks.'.format(e, mean_score))\n",
        "\n",
        "            self.replay(self.batch_size)\n",
        "        \n",
        "        if not self.quiet: print('Did not solve after {} episodes 😞'.format(e))\n",
        "        return e\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    agent = SARSACartPoleSolver()\n",
        "    agent.run()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
            "  result = entry_point.load(False)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Episode 0] - Mean survival time over last 100 episodes was 19.0 ticks.\n",
            "[Episode 100] - Mean survival time over last 100 episodes was 11.19 ticks.\n",
            "[Episode 200] - Mean survival time over last 100 episodes was 23.51 ticks.\n",
            "[Episode 300] - Mean survival time over last 100 episodes was 98.03 ticks.\n",
            "[Episode 400] - Mean survival time over last 100 episodes was 181.52 ticks.\n",
            "Ran 496 episodes. Solved after 396 trials ✔\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QElyKlCf7B--",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### (c) Evaluate the impact of $\\gamma$ in the cart pole problem. How important is this parameter? How does it affect stability and learning?"
      ]
    },
    {
      "metadata": {
        "id": "AcmC0vF-brMz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3165
        },
        "outputId": "b53e328b-ff6d-43a2-f8ef-377e91aeaa72"
      },
      "cell_type": "code",
      "source": [
        "for gamma in [0, 0.25, 0.5, 0.75, 1]:\n",
        "    print(\"*************Gamma selected = {}*************\\n\".format(gamma))\n",
        "    print(\"*************Solving using DQN***************\")\n",
        "    agent1 = DQNCartPoleSolver(n_episodes=1500, gamma=gamma)\n",
        "    agent1.run()\n",
        "    print(\"*************Solving using SARSA*************\")\n",
        "    agent2 = SARSACartPoleSolver(n_episodes=1500, gamma=gamma)\n",
        "    agent2.run()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "*************Gamma selected = 0*************\n",
            "\n",
            "*************Solving using DQN***************\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
            "  result = entry_point.load(False)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Episode 0] - Mean survival time over last 100 episodes was 21.0 ticks.\n",
            "[Episode 100] - Mean survival time over last 100 episodes was 15.46 ticks.\n",
            "[Episode 200] - Mean survival time over last 100 episodes was 9.27 ticks.\n",
            "[Episode 300] - Mean survival time over last 100 episodes was 9.44 ticks.\n",
            "[Episode 400] - Mean survival time over last 100 episodes was 9.55 ticks.\n",
            "[Episode 500] - Mean survival time over last 100 episodes was 9.5 ticks.\n",
            "[Episode 600] - Mean survival time over last 100 episodes was 9.55 ticks.\n",
            "[Episode 700] - Mean survival time over last 100 episodes was 9.58 ticks.\n",
            "[Episode 800] - Mean survival time over last 100 episodes was 9.43 ticks.\n",
            "[Episode 900] - Mean survival time over last 100 episodes was 9.49 ticks.\n",
            "[Episode 1000] - Mean survival time over last 100 episodes was 9.97 ticks.\n",
            "[Episode 1100] - Mean survival time over last 100 episodes was 10.12 ticks.\n",
            "[Episode 1200] - Mean survival time over last 100 episodes was 9.45 ticks.\n",
            "[Episode 1300] - Mean survival time over last 100 episodes was 9.4 ticks.\n",
            "[Episode 1400] - Mean survival time over last 100 episodes was 11.35 ticks.\n",
            "Did not solve after 1499 episodes 😞\n",
            "*************Solving using SARSA*************\n",
            "[Episode 0] - Mean survival time over last 100 episodes was 20.0 ticks.\n",
            "[Episode 100] - Mean survival time over last 100 episodes was 31.17 ticks.\n",
            "[Episode 200] - Mean survival time over last 100 episodes was 8.93 ticks.\n",
            "[Episode 300] - Mean survival time over last 100 episodes was 8.93 ticks.\n",
            "[Episode 400] - Mean survival time over last 100 episodes was 9.36 ticks.\n",
            "[Episode 500] - Mean survival time over last 100 episodes was 9.27 ticks.\n",
            "[Episode 600] - Mean survival time over last 100 episodes was 9.38 ticks.\n",
            "[Episode 700] - Mean survival time over last 100 episodes was 9.41 ticks.\n",
            "[Episode 800] - Mean survival time over last 100 episodes was 9.49 ticks.\n",
            "[Episode 900] - Mean survival time over last 100 episodes was 9.87 ticks.\n",
            "[Episode 1000] - Mean survival time over last 100 episodes was 10.31 ticks.\n",
            "[Episode 1100] - Mean survival time over last 100 episodes was 15.33 ticks.\n",
            "[Episode 1200] - Mean survival time over last 100 episodes was 12.25 ticks.\n",
            "[Episode 1300] - Mean survival time over last 100 episodes was 9.72 ticks.\n",
            "[Episode 1400] - Mean survival time over last 100 episodes was 12.38 ticks.\n",
            "Did not solve after 1499 episodes 😞\n",
            "*************Gamma selected = 0.25*************\n",
            "\n",
            "*************Solving using DQN***************\n",
            "[Episode 0] - Mean survival time over last 100 episodes was 10.0 ticks.\n",
            "[Episode 100] - Mean survival time over last 100 episodes was 21.81 ticks.\n",
            "[Episode 200] - Mean survival time over last 100 episodes was 17.19 ticks.\n",
            "[Episode 300] - Mean survival time over last 100 episodes was 11.26 ticks.\n",
            "[Episode 400] - Mean survival time over last 100 episodes was 9.99 ticks.\n",
            "[Episode 500] - Mean survival time over last 100 episodes was 9.87 ticks.\n",
            "[Episode 600] - Mean survival time over last 100 episodes was 10.48 ticks.\n",
            "[Episode 700] - Mean survival time over last 100 episodes was 11.57 ticks.\n",
            "[Episode 800] - Mean survival time over last 100 episodes was 9.39 ticks.\n",
            "[Episode 900] - Mean survival time over last 100 episodes was 9.92 ticks.\n",
            "[Episode 1000] - Mean survival time over last 100 episodes was 12.81 ticks.\n",
            "[Episode 1100] - Mean survival time over last 100 episodes was 9.93 ticks.\n",
            "[Episode 1200] - Mean survival time over last 100 episodes was 9.35 ticks.\n",
            "[Episode 1300] - Mean survival time over last 100 episodes was 9.45 ticks.\n",
            "[Episode 1400] - Mean survival time over last 100 episodes was 9.51 ticks.\n",
            "Did not solve after 1499 episodes 😞\n",
            "*************Solving using SARSA*************\n",
            "[Episode 0] - Mean survival time over last 100 episodes was 23.0 ticks.\n",
            "[Episode 100] - Mean survival time over last 100 episodes was 50.47 ticks.\n",
            "[Episode 200] - Mean survival time over last 100 episodes was 10.2 ticks.\n",
            "[Episode 300] - Mean survival time over last 100 episodes was 10.64 ticks.\n",
            "[Episode 400] - Mean survival time over last 100 episodes was 10.27 ticks.\n",
            "[Episode 500] - Mean survival time over last 100 episodes was 10.08 ticks.\n",
            "[Episode 600] - Mean survival time over last 100 episodes was 9.47 ticks.\n",
            "[Episode 700] - Mean survival time over last 100 episodes was 10.27 ticks.\n",
            "[Episode 800] - Mean survival time over last 100 episodes was 11.23 ticks.\n",
            "[Episode 900] - Mean survival time over last 100 episodes was 9.36 ticks.\n",
            "[Episode 1000] - Mean survival time over last 100 episodes was 9.63 ticks.\n",
            "[Episode 1100] - Mean survival time over last 100 episodes was 10.41 ticks.\n",
            "[Episode 1200] - Mean survival time over last 100 episodes was 11.81 ticks.\n",
            "[Episode 1300] - Mean survival time over last 100 episodes was 9.6 ticks.\n",
            "[Episode 1400] - Mean survival time over last 100 episodes was 12.31 ticks.\n",
            "Did not solve after 1499 episodes 😞\n",
            "*************Gamma selected = 0.5*************\n",
            "\n",
            "*************Solving using DQN***************\n",
            "[Episode 0] - Mean survival time over last 100 episodes was 47.0 ticks.\n",
            "[Episode 100] - Mean survival time over last 100 episodes was 15.07 ticks.\n",
            "[Episode 200] - Mean survival time over last 100 episodes was 21.9 ticks.\n",
            "[Episode 300] - Mean survival time over last 100 episodes was 20.29 ticks.\n",
            "[Episode 400] - Mean survival time over last 100 episodes was 40.99 ticks.\n",
            "[Episode 500] - Mean survival time over last 100 episodes was 60.13 ticks.\n",
            "[Episode 600] - Mean survival time over last 100 episodes was 58.5 ticks.\n",
            "[Episode 700] - Mean survival time over last 100 episodes was 61.0 ticks.\n",
            "[Episode 800] - Mean survival time over last 100 episodes was 110.79 ticks.\n",
            "[Episode 900] - Mean survival time over last 100 episodes was 88.35 ticks.\n",
            "[Episode 1000] - Mean survival time over last 100 episodes was 70.42 ticks.\n",
            "[Episode 1100] - Mean survival time over last 100 episodes was 83.66 ticks.\n",
            "[Episode 1200] - Mean survival time over last 100 episodes was 107.65 ticks.\n",
            "[Episode 1300] - Mean survival time over last 100 episodes was 100.04 ticks.\n",
            "[Episode 1400] - Mean survival time over last 100 episodes was 78.63 ticks.\n",
            "Did not solve after 1499 episodes 😞\n",
            "*************Solving using SARSA*************\n",
            "[Episode 0] - Mean survival time over last 100 episodes was 12.0 ticks.\n",
            "[Episode 100] - Mean survival time over last 100 episodes was 9.59 ticks.\n",
            "[Episode 200] - Mean survival time over last 100 episodes was 9.64 ticks.\n",
            "[Episode 300] - Mean survival time over last 100 episodes was 10.14 ticks.\n",
            "[Episode 400] - Mean survival time over last 100 episodes was 14.42 ticks.\n",
            "[Episode 500] - Mean survival time over last 100 episodes was 10.3 ticks.\n",
            "[Episode 600] - Mean survival time over last 100 episodes was 10.35 ticks.\n",
            "[Episode 700] - Mean survival time over last 100 episodes was 9.81 ticks.\n",
            "[Episode 800] - Mean survival time over last 100 episodes was 11.24 ticks.\n",
            "[Episode 900] - Mean survival time over last 100 episodes was 11.08 ticks.\n",
            "[Episode 1000] - Mean survival time over last 100 episodes was 11.09 ticks.\n",
            "[Episode 1100] - Mean survival time over last 100 episodes was 12.25 ticks.\n",
            "[Episode 1200] - Mean survival time over last 100 episodes was 57.14 ticks.\n",
            "[Episode 1300] - Mean survival time over last 100 episodes was 76.52 ticks.\n",
            "[Episode 1400] - Mean survival time over last 100 episodes was 83.41 ticks.\n",
            "Did not solve after 1499 episodes 😞\n",
            "*************Gamma selected = 0.75*************\n",
            "\n",
            "*************Solving using DQN***************\n",
            "[Episode 0] - Mean survival time over last 100 episodes was 27.0 ticks.\n",
            "[Episode 100] - Mean survival time over last 100 episodes was 22.55 ticks.\n",
            "[Episode 200] - Mean survival time over last 100 episodes was 77.59 ticks.\n",
            "[Episode 300] - Mean survival time over last 100 episodes was 114.7 ticks.\n",
            "[Episode 400] - Mean survival time over last 100 episodes was 112.33 ticks.\n",
            "[Episode 500] - Mean survival time over last 100 episodes was 119.56 ticks.\n",
            "[Episode 600] - Mean survival time over last 100 episodes was 131.54 ticks.\n",
            "[Episode 700] - Mean survival time over last 100 episodes was 129.34 ticks.\n",
            "[Episode 800] - Mean survival time over last 100 episodes was 144.43 ticks.\n",
            "[Episode 900] - Mean survival time over last 100 episodes was 154.3 ticks.\n",
            "[Episode 1000] - Mean survival time over last 100 episodes was 154.77 ticks.\n",
            "[Episode 1100] - Mean survival time over last 100 episodes was 156.52 ticks.\n",
            "[Episode 1200] - Mean survival time over last 100 episodes was 143.89 ticks.\n",
            "[Episode 1300] - Mean survival time over last 100 episodes was 120.15 ticks.\n",
            "[Episode 1400] - Mean survival time over last 100 episodes was 129.3 ticks.\n",
            "Did not solve after 1499 episodes 😞\n",
            "*************Solving using SARSA*************\n",
            "[Episode 0] - Mean survival time over last 100 episodes was 13.0 ticks.\n",
            "[Episode 100] - Mean survival time over last 100 episodes was 12.33 ticks.\n",
            "[Episode 200] - Mean survival time over last 100 episodes was 19.65 ticks.\n",
            "[Episode 300] - Mean survival time over last 100 episodes was 31.79 ticks.\n",
            "[Episode 400] - Mean survival time over last 100 episodes was 93.71 ticks.\n",
            "[Episode 500] - Mean survival time over last 100 episodes was 137.93 ticks.\n",
            "[Episode 600] - Mean survival time over last 100 episodes was 151.06 ticks.\n",
            "[Episode 700] - Mean survival time over last 100 episodes was 157.33 ticks.\n",
            "[Episode 800] - Mean survival time over last 100 episodes was 152.64 ticks.\n",
            "[Episode 900] - Mean survival time over last 100 episodes was 158.88 ticks.\n",
            "[Episode 1000] - Mean survival time over last 100 episodes was 146.79 ticks.\n",
            "[Episode 1100] - Mean survival time over last 100 episodes was 175.56 ticks.\n",
            "[Episode 1200] - Mean survival time over last 100 episodes was 172.12 ticks.\n",
            "[Episode 1300] - Mean survival time over last 100 episodes was 163.1 ticks.\n",
            "[Episode 1400] - Mean survival time over last 100 episodes was 181.37 ticks.\n",
            "Did not solve after 1499 episodes 😞\n",
            "*************Gamma selected = 1*************\n",
            "\n",
            "*************Solving using DQN***************\n",
            "[Episode 0] - Mean survival time over last 100 episodes was 29.0 ticks.\n",
            "[Episode 100] - Mean survival time over last 100 episodes was 32.18 ticks.\n",
            "[Episode 200] - Mean survival time over last 100 episodes was 67.18 ticks.\n",
            "[Episode 300] - Mean survival time over last 100 episodes was 110.49 ticks.\n",
            "[Episode 400] - Mean survival time over last 100 episodes was 36.25 ticks.\n",
            "[Episode 500] - Mean survival time over last 100 episodes was 119.67 ticks.\n",
            "[Episode 600] - Mean survival time over last 100 episodes was 114.42 ticks.\n",
            "[Episode 700] - Mean survival time over last 100 episodes was 103.47 ticks.\n",
            "[Episode 800] - Mean survival time over last 100 episodes was 94.11 ticks.\n",
            "[Episode 900] - Mean survival time over last 100 episodes was 123.71 ticks.\n",
            "[Episode 1000] - Mean survival time over last 100 episodes was 138.53 ticks.\n",
            "[Episode 1100] - Mean survival time over last 100 episodes was 126.17 ticks.\n",
            "[Episode 1200] - Mean survival time over last 100 episodes was 194.65 ticks.\n",
            "Ran 1201 episodes. Solved after 1101 trials ✔\n",
            "*************Solving using SARSA*************\n",
            "[Episode 0] - Mean survival time over last 100 episodes was 38.0 ticks.\n",
            "[Episode 100] - Mean survival time over last 100 episodes was 22.92 ticks.\n",
            "[Episode 200] - Mean survival time over last 100 episodes was 110.62 ticks.\n",
            "[Episode 300] - Mean survival time over last 100 episodes was 163.33 ticks.\n",
            "[Episode 400] - Mean survival time over last 100 episodes was 174.99 ticks.\n",
            "[Episode 500] - Mean survival time over last 100 episodes was 160.89 ticks.\n",
            "[Episode 600] - Mean survival time over last 100 episodes was 192.76 ticks.\n",
            "Ran 603 episodes. Solved after 503 trials ✔\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XDpp-2E9PCqB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Answer:**\n",
        "\n",
        "$\\gamma$ is a very important parameter for both of our reinforcement learning networks. Here, I selected 5 different values for $\\gamma$ ranges from 0 to 1 to test on both a deep Q-learning network and a SARSA network. Based on the results, we can see that both networks could solve the problem only when $\\gamma$ is exactly equal to 1. We know that $\\gamma = 1$ means that the agent cares about long-term rewards. A relatively small $\\gamma$ value (< 0.5) produces stable poor performance since the agent learns very slowly and could not find a solution in the given time. If we increase the value, the stability of output becomes worse, as we can see from the outputs have a wider range. Also, in general, SARSA is more stable than DQN and could run faster."
      ]
    },
    {
      "metadata": {
        "id": "mzKOS7QU7B-_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### (d) Evaluate the impact of $\\epsilon$ on the learning over the feasible range (0-1). What values seem reasonable?"
      ]
    },
    {
      "metadata": {
        "id": "QAB-1_R5dYdi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2692
        },
        "outputId": "210fd185-c228-4922-c054-dc2c2d1e8361"
      },
      "cell_type": "code",
      "source": [
        "for e in [0, 0.25, 0.5, 0.75, 1]:\n",
        "    print(\"*************Epsilon selected = {}*************\\n\".format(e))\n",
        "    print(\"*************Solving using DQN***************\")\n",
        "    agent1 = DQNCartPoleSolver(n_episodes=1500, epsilon=e, epsilon_min=0.0)\n",
        "    agent1.run()\n",
        "    print(\"*************Solving using SARSA*************\")\n",
        "    agent2 = SARSACartPoleSolver(n_episodes=1500, epsilon=e, epsilon_min=0.0)\n",
        "    agent2.run()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "*************Epsilon selected = 0*************\n",
            "\n",
            "*************Solving using DQN***************\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
            "  result = entry_point.load(False)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Episode 0] - Mean survival time over last 100 episodes was 39.0 ticks.\n",
            "[Episode 100] - Mean survival time over last 100 episodes was 64.27 ticks.\n",
            "[Episode 200] - Mean survival time over last 100 episodes was 30.2 ticks.\n",
            "[Episode 300] - Mean survival time over last 100 episodes was 131.58 ticks.\n",
            "[Episode 400] - Mean survival time over last 100 episodes was 116.85 ticks.\n",
            "[Episode 500] - Mean survival time over last 100 episodes was 129.87 ticks.\n",
            "[Episode 600] - Mean survival time over last 100 episodes was 155.44 ticks.\n",
            "[Episode 700] - Mean survival time over last 100 episodes was 157.77 ticks.\n",
            "[Episode 800] - Mean survival time over last 100 episodes was 106.87 ticks.\n",
            "Ran 897 episodes. Solved after 797 trials ✔\n",
            "*************Solving using SARSA*************\n",
            "[Episode 0] - Mean survival time over last 100 episodes was 9.0 ticks.\n",
            "[Episode 100] - Mean survival time over last 100 episodes was 9.39 ticks.\n",
            "[Episode 200] - Mean survival time over last 100 episodes was 9.41 ticks.\n",
            "[Episode 300] - Mean survival time over last 100 episodes was 9.33 ticks.\n",
            "[Episode 400] - Mean survival time over last 100 episodes was 9.42 ticks.\n",
            "[Episode 500] - Mean survival time over last 100 episodes was 9.34 ticks.\n",
            "[Episode 600] - Mean survival time over last 100 episodes was 9.31 ticks.\n",
            "[Episode 700] - Mean survival time over last 100 episodes was 9.18 ticks.\n",
            "[Episode 800] - Mean survival time over last 100 episodes was 9.37 ticks.\n",
            "[Episode 900] - Mean survival time over last 100 episodes was 9.47 ticks.\n",
            "[Episode 1000] - Mean survival time over last 100 episodes was 9.33 ticks.\n",
            "[Episode 1100] - Mean survival time over last 100 episodes was 9.27 ticks.\n",
            "[Episode 1200] - Mean survival time over last 100 episodes was 9.35 ticks.\n",
            "[Episode 1300] - Mean survival time over last 100 episodes was 9.42 ticks.\n",
            "[Episode 1400] - Mean survival time over last 100 episodes was 9.44 ticks.\n",
            "Did not solve after 1499 episodes 😞\n",
            "*************Epsilon selected = 0.25*************\n",
            "\n",
            "*************Solving using DQN***************\n",
            "[Episode 0] - Mean survival time over last 100 episodes was 10.0 ticks.\n",
            "[Episode 100] - Mean survival time over last 100 episodes was 14.13 ticks.\n",
            "[Episode 200] - Mean survival time over last 100 episodes was 31.9 ticks.\n",
            "[Episode 300] - Mean survival time over last 100 episodes was 39.36 ticks.\n",
            "[Episode 400] - Mean survival time over last 100 episodes was 129.98 ticks.\n",
            "[Episode 500] - Mean survival time over last 100 episodes was 147.2 ticks.\n",
            "[Episode 600] - Mean survival time over last 100 episodes was 136.36 ticks.\n",
            "[Episode 700] - Mean survival time over last 100 episodes was 138.52 ticks.\n",
            "[Episode 800] - Mean survival time over last 100 episodes was 150.7 ticks.\n",
            "[Episode 900] - Mean survival time over last 100 episodes was 151.47 ticks.\n",
            "[Episode 1000] - Mean survival time over last 100 episodes was 159.0 ticks.\n",
            "[Episode 1100] - Mean survival time over last 100 episodes was 156.97 ticks.\n",
            "[Episode 1200] - Mean survival time over last 100 episodes was 145.12 ticks.\n",
            "[Episode 1300] - Mean survival time over last 100 episodes was 185.51 ticks.\n",
            "Ran 1328 episodes. Solved after 1228 trials ✔\n",
            "*************Solving using SARSA*************\n",
            "[Episode 0] - Mean survival time over last 100 episodes was 107.0 ticks.\n",
            "[Episode 100] - Mean survival time over last 100 episodes was 93.78 ticks.\n",
            "[Episode 200] - Mean survival time over last 100 episodes was 54.16 ticks.\n",
            "[Episode 300] - Mean survival time over last 100 episodes was 57.44 ticks.\n",
            "[Episode 400] - Mean survival time over last 100 episodes was 72.81 ticks.\n",
            "[Episode 500] - Mean survival time over last 100 episodes was 157.52 ticks.\n",
            "[Episode 600] - Mean survival time over last 100 episodes was 152.51 ticks.\n",
            "[Episode 700] - Mean survival time over last 100 episodes was 76.58 ticks.\n",
            "[Episode 800] - Mean survival time over last 100 episodes was 74.98 ticks.\n",
            "[Episode 900] - Mean survival time over last 100 episodes was 103.33 ticks.\n",
            "[Episode 1000] - Mean survival time over last 100 episodes was 116.43 ticks.\n",
            "[Episode 1100] - Mean survival time over last 100 episodes was 151.34 ticks.\n",
            "[Episode 1200] - Mean survival time over last 100 episodes was 174.29 ticks.\n",
            "[Episode 1300] - Mean survival time over last 100 episodes was 186.0 ticks.\n",
            "[Episode 1400] - Mean survival time over last 100 episodes was 160.13 ticks.\n",
            "Did not solve after 1499 episodes 😞\n",
            "*************Epsilon selected = 0.5*************\n",
            "\n",
            "*************Solving using DQN***************\n",
            "[Episode 0] - Mean survival time over last 100 episodes was 29.0 ticks.\n",
            "[Episode 100] - Mean survival time over last 100 episodes was 16.4 ticks.\n",
            "[Episode 200] - Mean survival time over last 100 episodes was 26.48 ticks.\n",
            "[Episode 300] - Mean survival time over last 100 episodes was 50.95 ticks.\n",
            "[Episode 400] - Mean survival time over last 100 episodes was 101.52 ticks.\n",
            "[Episode 500] - Mean survival time over last 100 episodes was 96.69 ticks.\n",
            "[Episode 600] - Mean survival time over last 100 episodes was 127.67 ticks.\n",
            "[Episode 700] - Mean survival time over last 100 episodes was 125.01 ticks.\n",
            "[Episode 800] - Mean survival time over last 100 episodes was 189.39 ticks.\n",
            "[Episode 900] - Mean survival time over last 100 episodes was 172.25 ticks.\n",
            "[Episode 1000] - Mean survival time over last 100 episodes was 148.68 ticks.\n",
            "[Episode 1100] - Mean survival time over last 100 episodes was 129.51 ticks.\n",
            "[Episode 1200] - Mean survival time over last 100 episodes was 140.67 ticks.\n",
            "Ran 1260 episodes. Solved after 1160 trials ✔\n",
            "*************Solving using SARSA*************\n",
            "[Episode 0] - Mean survival time over last 100 episodes was 10.0 ticks.\n",
            "[Episode 100] - Mean survival time over last 100 episodes was 16.68 ticks.\n",
            "[Episode 200] - Mean survival time over last 100 episodes was 45.78 ticks.\n",
            "[Episode 300] - Mean survival time over last 100 episodes was 183.69 ticks.\n",
            "[Episode 400] - Mean survival time over last 100 episodes was 161.83 ticks.\n",
            "[Episode 500] - Mean survival time over last 100 episodes was 115.89 ticks.\n",
            "[Episode 600] - Mean survival time over last 100 episodes was 142.66 ticks.\n",
            "[Episode 700] - Mean survival time over last 100 episodes was 181.82 ticks.\n",
            "Ran 747 episodes. Solved after 647 trials ✔\n",
            "*************Epsilon selected = 0.75*************\n",
            "\n",
            "*************Solving using DQN***************\n",
            "[Episode 0] - Mean survival time over last 100 episodes was 23.0 ticks.\n",
            "[Episode 100] - Mean survival time over last 100 episodes was 15.57 ticks.\n",
            "[Episode 200] - Mean survival time over last 100 episodes was 19.92 ticks.\n",
            "[Episode 300] - Mean survival time over last 100 episodes was 36.63 ticks.\n",
            "[Episode 400] - Mean survival time over last 100 episodes was 86.28 ticks.\n",
            "[Episode 500] - Mean survival time over last 100 episodes was 110.5 ticks.\n",
            "[Episode 600] - Mean survival time over last 100 episodes was 98.57 ticks.\n",
            "[Episode 700] - Mean survival time over last 100 episodes was 94.82 ticks.\n",
            "[Episode 800] - Mean survival time over last 100 episodes was 104.98 ticks.\n",
            "[Episode 900] - Mean survival time over last 100 episodes was 124.51 ticks.\n",
            "[Episode 1000] - Mean survival time over last 100 episodes was 140.24 ticks.\n",
            "[Episode 1100] - Mean survival time over last 100 episodes was 157.2 ticks.\n",
            "[Episode 1200] - Mean survival time over last 100 episodes was 169.22 ticks.\n",
            "[Episode 1300] - Mean survival time over last 100 episodes was 148.28 ticks.\n",
            "[Episode 1400] - Mean survival time over last 100 episodes was 140.24 ticks.\n",
            "Did not solve after 1499 episodes 😞\n",
            "*************Solving using SARSA*************\n",
            "[Episode 0] - Mean survival time over last 100 episodes was 19.0 ticks.\n",
            "[Episode 100] - Mean survival time over last 100 episodes was 12.25 ticks.\n",
            "[Episode 200] - Mean survival time over last 100 episodes was 17.92 ticks.\n",
            "[Episode 300] - Mean survival time over last 100 episodes was 63.62 ticks.\n",
            "[Episode 400] - Mean survival time over last 100 episodes was 142.32 ticks.\n",
            "[Episode 500] - Mean survival time over last 100 episodes was 191.68 ticks.\n",
            "Ran 508 episodes. Solved after 408 trials ✔\n",
            "*************Epsilon selected = 1*************\n",
            "\n",
            "*************Solving using DQN***************\n",
            "[Episode 0] - Mean survival time over last 100 episodes was 31.0 ticks.\n",
            "[Episode 100] - Mean survival time over last 100 episodes was 21.66 ticks.\n",
            "[Episode 200] - Mean survival time over last 100 episodes was 41.24 ticks.\n",
            "[Episode 300] - Mean survival time over last 100 episodes was 104.01 ticks.\n",
            "[Episode 400] - Mean survival time over last 100 episodes was 114.62 ticks.\n",
            "[Episode 500] - Mean survival time over last 100 episodes was 106.34 ticks.\n",
            "[Episode 600] - Mean survival time over last 100 episodes was 128.81 ticks.\n",
            "[Episode 700] - Mean survival time over last 100 episodes was 146.3 ticks.\n",
            "[Episode 800] - Mean survival time over last 100 episodes was 134.96 ticks.\n",
            "[Episode 900] - Mean survival time over last 100 episodes was 192.29 ticks.\n",
            "[Episode 1000] - Mean survival time over last 100 episodes was 183.27 ticks.\n",
            "Ran 1081 episodes. Solved after 981 trials ✔\n",
            "*************Solving using SARSA*************\n",
            "[Episode 0] - Mean survival time over last 100 episodes was 31.0 ticks.\n",
            "[Episode 100] - Mean survival time over last 100 episodes was 40.67 ticks.\n",
            "[Episode 200] - Mean survival time over last 100 episodes was 44.97 ticks.\n",
            "[Episode 300] - Mean survival time over last 100 episodes was 85.07 ticks.\n",
            "[Episode 400] - Mean survival time over last 100 episodes was 148.12 ticks.\n",
            "[Episode 500] - Mean survival time over last 100 episodes was 151.31 ticks.\n",
            "[Episode 600] - Mean survival time over last 100 episodes was 123.91 ticks.\n",
            "[Episode 700] - Mean survival time over last 100 episodes was 185.92 ticks.\n",
            "Ran 721 episodes. Solved after 621 trials ✔\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jktBCHa4ShkI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Answer:**\n",
        "\n",
        "$\\epsilon$ controls the model to either explore or exploit. Based on the test results, unlike $\\gamma$, the agents could solve the problem with different initial values of $\\epsilon$ ($\\epsilon$ has a exponential decay term), so it is not as critical as $\\gamma$ for our RL models to find a solution in given time. More specifically, I assume that an initial $\\epsilon$ value that is larger than 0.5 seems reasonable since both agents perform well in this case, and also run faster."
      ]
    },
    {
      "metadata": {
        "id": "eDmBq6ZPNHAv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### (e) Pick one other small agent from the OpenAI gym (https://gym.openai.com/envs/#classic_control) and apply the same techniques "
      ]
    },
    {
      "metadata": {
        "id": "8OXKvTwuf8XC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "outputId": "75a90897-abdc-4279-e517-bd89523c68a2"
      },
      "cell_type": "code",
      "source": [
        "env = gym.make(\"MountainCar-v0\")\n"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
            "  result = entry_point.load(False)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "YGjVY8Aff_x8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "374fba6d-bec9-43ac-b68b-f48c06a22572"
      },
      "cell_type": "code",
      "source": [
        "env.observation_space"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Box(2,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "metadata": {
        "id": "x_9USWhtgAUM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ebcefbc8-b99d-404e-bdb2-e5ded71b9903"
      },
      "cell_type": "code",
      "source": [
        "env.action_space"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discrete(3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "metadata": {
        "id": "SUMAptjL7AAM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 565
        },
        "outputId": "c9e0a4cd-f22c-44da-c785-c319e09304dc"
      },
      "cell_type": "code",
      "source": [
        "# Code based on https://gitlab.oit.duke.edu/dec18/intro_to_deep_learning/blob/master/lectures/22_Reinforcement_Learning.ipynb\n",
        "# Initialization rules based on https://github.com/openai/gym/wiki/MountainCar-v0\n",
        "# Some network architecture based on https://medium.com/@ts1829/solving-mountain-car-with-q-learning-b77bf71b1de2\n",
        "# USING SARSA METHOD\n",
        "\n",
        "class SARSAMountainCarSolver():\n",
        "    def __init__(self, n_episodes=1000, n_win_ticks=195, max_env_steps=None, gamma=0.99, epsilon=0.9, epsilon_min=0.01, epsilon_log_decay=0.995, alpha=0.01, alpha_decay=0.01, batch_size=64, monitor=False, quiet=False):\n",
        "        self.memory = deque(maxlen=10000)\n",
        "        self.env = gym.make('MountainCar-v0')\n",
        "        if monitor: self.env = gym.wrappers.Monitor(self.env, '../data/mountaincar-1', force=True)\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_min = epsilon_min\n",
        "        self.epsilon_decay = epsilon_log_decay\n",
        "        self.alpha = alpha\n",
        "        self.alpha_decay = alpha_decay\n",
        "        self.n_episodes = n_episodes\n",
        "        self.n_win_ticks = n_win_ticks\n",
        "        self.batch_size = batch_size\n",
        "        self.quiet = quiet\n",
        "        if max_env_steps is not None: self.env._max_episode_steps = max_env_steps\n",
        "        self.init_lr = 0.001\n",
        "        # Init model\n",
        "        self.state_ = tf.placeholder(tf.float32, shape=[None, 2])\n",
        "        h = tf.layers.dense(self.state_, units=24, activation=tf.nn.tanh, kernel_initializer=tf.truncated_normal_initializer)\n",
        "        h = tf.layers.dense(h, units=48, activation=tf.nn.tanh)\n",
        "        self.Q = tf.layers.dense(h, units=3)\n",
        "        \n",
        "        self.Q_ = tf.placeholder(tf.float32, shape=[None, 3])\n",
        "        loss = tf.losses.mean_squared_error(self.Q_, self.Q)\n",
        "        \n",
        "        self.lr = tf.placeholder(tf.float32, shape=[])\n",
        "        self.train_step = tf.train.GradientDescentOptimizer(learning_rate=self.lr).minimize(loss)\n",
        "        \n",
        "        self.sess = tf.Session()\n",
        "        self.sess.run(tf.global_variables_initializer())\n",
        "    # Modification: Add a_t+1 term (next action)\n",
        "    def remember(self, state, action, reward, next_state, next_action, done):\n",
        "        self.memory.append((state, action, reward, next_state, next_action, done))\n",
        "\n",
        "    def choose_action(self, state, epsilon):\n",
        "        return self.env.action_space.sample() if (np.random.random() <= epsilon) else np.argmax(self.sess.run(self.Q, feed_dict={self.state_: state, self.lr: self.init_lr}))\n",
        "\n",
        "    def get_epsilon(self, t):\n",
        "        return self.epsilon\n",
        "\n",
        "    def preprocess_state(self, state):\n",
        "        return np.reshape(state, [1, 2])\n",
        "\n",
        "    def replay(self, batch_size):\n",
        "        x_batch, y_batch = [], []\n",
        "        minibatch = random.sample(self.memory, min(len(self.memory), batch_size))\n",
        "        for state, action, reward, next_state, next_action, done in minibatch: # Modification: Add a_t+1 term (next action)\n",
        "            y_target = self.sess.run(self.Q, feed_dict={self.state_: state, self.lr:self.init_lr})\n",
        "            # Modification: Q^old(s_t+1, a_t+1)\n",
        "            y_target[0][action] = reward if done else reward + self.gamma * self.sess.run(self.Q, feed_dict={self.state_: next_state, self.lr:self.init_lr})[0][next_action]\n",
        "            x_batch.append(state[0])\n",
        "            y_batch.append(y_target[0])\n",
        "        \n",
        "        self.sess.run(self.train_step, feed_dict={self.state_: np.array(x_batch), self.Q_: np.array(y_batch), self.lr:self.init_lr})\n",
        "\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "    def run(self):\n",
        "        scores = deque(maxlen=100)\n",
        "        steps = 300\n",
        "        for e in range(self.n_episodes):\n",
        "            state = self.preprocess_state(self.env.reset())\n",
        "            # Modification: Add initial conditions a0\n",
        "            action = self.choose_action(state, self.get_epsilon(e))\n",
        "            max_position = -100\n",
        "            done = False\n",
        "            i = 0\n",
        "            #while not done:\n",
        "            for s in range(steps):\n",
        "                next_state, reward, done, _ = self.env.step(action) # Step forward and receive next state and reward\n",
        "                reward = next_state[0]-0.5  # Adjust reward based on car position\n",
        "                if next_state[0] > max_position: # Keep track of max position\n",
        "                    max_position = next_state[0] \n",
        "                if next_state[0] >= 0.5: # Adjust reward for task completion\n",
        "                    reward += 1\n",
        "                if done:\n",
        "                    if next_state[0] >= 0.5: # On successful epsisodes, adjust the following parameters\n",
        "                        self.epsilon *= self.epsilon_log_decay # Adjust epsilon\n",
        "                        self.init_lr *= 0.9 # Adjust learning rate\n",
        "                        \n",
        "                next_state = self.preprocess_state(next_state)\n",
        "                # Modification: Choose next action based on next state\n",
        "                next_action = self.choose_action(next_state, self.get_epsilon(e))\n",
        "                self.remember(state, action, reward, next_state, next_action, done)\n",
        "                \n",
        "                state = next_state\n",
        "                action = next_action # Update action as well\n",
        "                i += 1\n",
        "            # Record history\n",
        "            scores.append(max_position)\n",
        "            max_score = np.max(scores)\n",
        "            if max_score >= 0.5 and e >= 100:\n",
        "                if not self.quiet: print('Ran {} episodes. Solved after {} trials ✔'.format(e, e - 100))\n",
        "                return e - 100\n",
        "            if e % 10 == 0 and not self.quiet:\n",
        "                print('[Episode {}] - best score {}'.format(e,max_score))\n",
        "\n",
        "            self.replay(self.batch_size)\n",
        "        \n",
        "        if not self.quiet: print('Did not solve after {} episodes 😞'.format(e))\n",
        "        return e\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    agent = SARSAMountainCarSolver(n_episodes=1500)\n",
        "    agent.run()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
            "  result = entry_point.load(False)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
            "[Episode 0] - best score -0.24105732457323645\n",
            "[Episode 10] - best score -0.16222821278553412\n",
            "[Episode 20] - best score -0.16222821278553412\n",
            "[Episode 30] - best score -0.16222821278553412\n",
            "[Episode 40] - best score -0.16222821278553412\n",
            "[Episode 50] - best score -0.1274514999041625\n",
            "[Episode 60] - best score -0.1274514999041625\n",
            "[Episode 70] - best score -0.1274514999041625\n",
            "[Episode 80] - best score -0.1274514999041625\n",
            "[Episode 90] - best score -0.1274514999041625\n",
            "[Episode 100] - best score -0.09157336651680505\n",
            "[Episode 110] - best score 0.09608384058276344\n",
            "[Episode 120] - best score 0.10661531639204574\n",
            "[Episode 130] - best score 0.10661531639204574\n",
            "[Episode 140] - best score 0.10661531639204574\n",
            "[Episode 150] - best score 0.22521521469648118\n",
            "[Episode 160] - best score 0.32939201220800196\n",
            "[Episode 170] - best score 0.32939201220800196\n",
            "[Episode 180] - best score 0.34080127717802156\n",
            "[Episode 190] - best score 0.34080127717802156\n",
            "[Episode 200] - best score 0.34080127717802156\n",
            "[Episode 210] - best score 0.34080127717802156\n",
            "[Episode 220] - best score 0.34080127717802156\n",
            "[Episode 230] - best score 0.34080127717802156\n",
            "[Episode 240] - best score 0.34080127717802156\n",
            "Ran 242 episodes. Solved after 142 trials ✔\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PvjC5nySNPgo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Problem 5: Bookkeeping (5 points)"
      ]
    },
    {
      "metadata": {
        "id": "VokGighaNUye",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### (a) How many hours did this assignment take you? (There is NO correct answer here, this is just an information gathering exercise)"
      ]
    },
    {
      "metadata": {
        "id": "RawcfTVANXrx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "About 30 hours in total. This HW is long."
      ]
    },
    {
      "metadata": {
        "id": "KqB7CNuRNYMa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### (b) Verify that you adhered to the Duke Community Standard in this assignment\n",
        "(https://studentaffairs.duke.edu/conduct/about-us/duke-community-standard)."
      ]
    },
    {
      "metadata": {
        "id": "6DmYhZ22Nfot",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "I adhered to the Duke Community Standard in the completion of this\n",
        "assignment\n",
        "\n",
        "Yifan Li"
      ]
    }
  ]
}